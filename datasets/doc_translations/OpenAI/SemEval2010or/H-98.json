{
    "id": "H-98",
    "original_text": "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept. Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios. For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time. However, the quality of the probability estimates is crucial. We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different. Finally, we analyze the experimental performance of these models over the outputs of two text classifiers. The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1. INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking. For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time. This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time. Furthermore, the costs can be changed without retraining the model. Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23]. Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17]. Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3]. However, in all of these tasks, the quality of the probability estimates is crucial. Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data. Since many text classification tasks often have very little training data, we focus on parametric methods. However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable. While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes. We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models. Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items. We first review related work on improving probability estimates and score modeling in information retrieval. Then, we discuss in further detail the need for asymmetric models. After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores. We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods. Finally, we summarize our contributions and discuss future directions. 2. RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval. Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning. Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines. They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here. They also survey the long history of modeling the relevance scores of search engines. Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data. Focus on improving probability estimates has been growing lately. Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes. In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM. While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results. Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue. In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions. There is a variety of other work that this paper extends. Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM. His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better). Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs. Recalibrating poorly calibrated classifiers is not a new problem. Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3. PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1. A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic). We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively). There are two general types of parametric approaches. The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey. The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)). The second type of approach breaks the problem down as shown in the grey box of Figure 1. An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters. We can visualize this as depicted in Figure 2. Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+). A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen. Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2). Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished. This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3). As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier. Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant. Furthermore, no examples fall between θ− and θ+. The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity. Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved. This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query. Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes. Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples). Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric. A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian. An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right. We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians. A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically. We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters. To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr). While these distributions are composed of halves, the resulting function is a single continuous distribution. These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters. We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive). In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way. Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4). To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval. Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions. In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ. Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ). Now, we fix θ and compute the maximum likelihood for that choice of θ. Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ. The complete derivation is omitted because of space but is available in [2]. We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ. Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ. The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales). By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform. Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half). Dr = 0 is handled similarly. Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable. Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time. In addition, if the scores are sorted, then we can perform the whole process quite efficiently. Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately. Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left. Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ). The MLEs can be worked out similar to the above. We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp. Dr2 = 0), we assign σl = 0 (resp. σr = 0). Again, the same computational complexity analysis applies to estimating these parameters. 4. EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e. P(c) = |c|+1 N+2 where N is the number of documents. For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above. All of the methods below are fit using maximum likelihood estimates. For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d). The log-odds are defined to be log P (+|d) P (−|d) . The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e. P(+|d) = P(−|d) = 0.5). Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19]. Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors. In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate. Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates. This method is denoted in the tables below as Gauss. Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above. Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested. This method is denoted as A. Gauss. Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form. The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14]. We denote this method as Laplace below. Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above. As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs. This method is denoted as A. Laplace below. Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)). Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels. As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier. When this is desired, these methods may be more appropriate. The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it. Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier. Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning. The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d). Instead of using this below, we will use the logodds ratio. This does not affect the model as it simply shifts all of the scores by a constant determined by the priors. We refer to this method as LogReg below. Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM. This model differs from the LogReg model only in how the parameters are estimated. The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled. The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label). Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness. We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP. MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified. We used the same train/test split of 50078/10024 documents as that reported in [9]. The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories. The class proportions in the training set vary from 1.15% to 22.29%. In the testing set, they range from 1.14% to 21.54%. The classes are general subjects such as Health & Fitness and Travel & Vacation. Human indexers assigned the documents to zero or more categories. For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents. Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987. For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents). The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects. There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22]. The class proportions in the training set vary from 1.88% to 29.96%. In the testing set, they range from 1.7% to 32.95%. For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents. TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990. We used the same train/test split of 142791/66992 documents that was used in [18]. As described in [17] (see also [15]), the categories are defined by keywords in a keyword field. The title and body fields are used in the experiments below. There are twenty categories in total. The class proportions in the training set vary from 0.06% to 2.03%. In the testing set, they range from 0.03% to 4.32%. For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation. A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted. After accounting for the variance, that evaluation also supported the claims made here. SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm. The features were represented as continuous values. We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22]. The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero. Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21]. We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. We use the log-odds estimated by the classifier as s(d). The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates. For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise. The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 . When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero. When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one. Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized. We report only the sum of these measures and omit the averages for space. Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus. In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities. This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5. Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates. It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods. We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures. Only pairs that the methods disagree on are used in the sign test. This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed. We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes. In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data. We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result. Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a. Table 1b gives results for producing probabilistic outputs for SVMs. Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right). The best entry for a corpus is in bold. Entries that are statistically significantly better than all other entries are underlined. A † denotes the method is significantly better than all other methods except for na¨ıve Bayes. A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left). The reason for this distinction in significance tests is described in the text. We start with general observations that result from examining the performance of these methods over the various corpora. The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods. There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model. With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error. However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant. In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters. Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly. Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ). Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily. We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on. Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties). In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins. For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise. No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total. The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17]. This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here). The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong. There are several interesting points about the performance of the asymmetric distributions as well. First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount. This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails). While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate. Figure 3 is actually a result of fitting the two distributions to real data. As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class. Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters. Also shown is the fit of the asymmetric Laplace distribution to the training score distribution. The positive class (i.e. Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily. There are enough such cases overall that it seems clearly inferior to the top three methods. However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential). As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace. Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes. Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well. Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor. Finally, we can make a few observations about the usefulness of the various performance metrics. First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate. Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice. Secondly, squared error has a weakness in the other direction. That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates. For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing). This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome. For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced. These observations are straightforward from the definitions but are underscored by the evaluation. 5. FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes). From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails. Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace. This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace. Finally, extending these methods to the outputs of other discriminative classifiers is an open area. We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11]. By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6. SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier. In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function. We have given an efficient way to estimate the parameters of these distributions. While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes. In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression. Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates. Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox. Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work. Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7. REFERENCES [1] P. N. Bennett. Assessing the calibration of naive bayes posterior estimates. Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett. Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods. Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan. A continuous speech recognition system embedding mlp into hmm. In NIPS 89, 1989. [4] G. Brier. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg. The comparison and evaluation of forecasters. Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg. Comparing probability forecasters: Basic binary concepts and multivariate extensions. In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques. Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani. Beyond independence: Conditions for the optimality of the simple bayesian classifier. In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork. Pattern Classification. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen. Hierarchical classification of web content. In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inductive learning algorithms and representations for text categorization. In CIKM 98, 1998. [11] Y. Freund and R. Schapire. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277-296, 1999. [12] I. Good. Rational decisions. Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski. The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance. Birkh¨auser, 2001. [15] D. D. Lewis. A sequential algorithm for training text classifiers: Corrigendum and additional data. SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis. Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale. A sequential algorithm for training text classifiers. In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka. Training algorithms for linear text classifiers. In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown. On the reconciliation of probability assessments. Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng. Modeling score distributions for combining the outputs of search engines. In SIGIR 01, 2001. [21] A. McCallum and K. Nigam. A comparison of event models for naive bayes text classification. In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers. MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost. Active learning for class probability estimation and ranking. In IJCAI 01, 2001. [24] R. L. Winkler. Scoring rules and the evaluation of probability assessors. Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In ICML 01, 2001. [27] B. Zadrozny and C. Elkan. Reducing multiclass to binary by coupling probability estimates. In KDD 02, 2002.",
    "original_translation": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002.",
    "original_sentences": [
        "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
        "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
        "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
        "However, the quality of the probability estimates is crucial.",
        "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
        "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
        "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
        "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
        "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
        "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
        "Furthermore, the costs can be changed without retraining the model.",
        "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
        "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
        "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
        "However, in all of these tasks, the quality of the probability estimates is crucial.",
        "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
        "Since many text classification tasks often have very little training data, we focus on parametric methods.",
        "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
        "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
        "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
        "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
        "We first review related work on improving probability estimates and score modeling in information retrieval.",
        "Then, we discuss in further detail the need for asymmetric models.",
        "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
        "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
        "Finally, we summarize our contributions and discuss future directions. 2.",
        "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
        "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
        "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
        "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
        "They also survey the long history of modeling the relevance scores of search engines.",
        "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
        "Focus on improving probability estimates has been growing lately.",
        "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
        "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
        "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
        "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
        "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
        "There is a variety of other work that this paper extends.",
        "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
        "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
        "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
        "Recalibrating poorly calibrated classifiers is not a new problem.",
        "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
        "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
        "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
        "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
        "There are two general types of parametric approaches.",
        "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
        "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
        "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
        "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
        "We can visualize this as depicted in Figure 2.",
        "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
        "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
        "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
        "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
        "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
        "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
        "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
        "Furthermore, no examples fall between θ− and θ+.",
        "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
        "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
        "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
        "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
        "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
        "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
        "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
        "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
        "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
        "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
        "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
        "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
        "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
        "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
        "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
        "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
        "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
        "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
        "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
        "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
        "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
        "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
        "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
        "The complete derivation is omitted because of space but is available in [2].",
        "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
        "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
        "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
        "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
        "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
        "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
        "Dr = 0 is handled similarly.",
        "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
        "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
        "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
        "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
        "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
        "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
        "The MLEs can be worked out similar to the above.",
        "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
        "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
        "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
        "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
        "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
        "P(c) = |c|+1 N+2 where N is the number of documents.",
        "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
        "All of the methods below are fit using maximum likelihood estimates.",
        "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
        "The log-odds are defined to be log P (+|d) P (−|d) .",
        "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
        "P(+|d) = P(−|d) = 0.5).",
        "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
        "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
        "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
        "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
        "This method is denoted in the tables below as Gauss.",
        "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
        "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
        "This method is denoted as A. Gauss.",
        "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
        "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
        "We denote this method as Laplace below.",
        "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
        "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
        "This method is denoted as A. Laplace below.",
        "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
        "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
        "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
        "When this is desired, these methods may be more appropriate.",
        "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
        "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
        "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
        "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
        "Instead of using this below, we will use the logodds ratio.",
        "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
        "We refer to this method as LogReg below.",
        "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
        "This model differs from the LogReg model only in how the parameters are estimated.",
        "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
        "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
        "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
        "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
        "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
        "We used the same train/test split of 50078/10024 documents as that reported in [9].",
        "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
        "The class proportions in the training set vary from 1.15% to 22.29%.",
        "In the testing set, they range from 1.14% to 21.54%.",
        "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
        "Human indexers assigned the documents to zero or more categories.",
        "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
        "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
        "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
        "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
        "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
        "The class proportions in the training set vary from 1.88% to 29.96%.",
        "In the testing set, they range from 1.7% to 32.95%.",
        "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
        "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
        "We used the same train/test split of 142791/66992 documents that was used in [18].",
        "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
        "The title and body fields are used in the experiments below.",
        "There are twenty categories in total.",
        "The class proportions in the training set vary from 0.06% to 2.03%.",
        "In the testing set, they range from 0.03% to 4.32%.",
        "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
        "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
        "After accounting for the variance, that evaluation also supported the claims made here.",
        "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
        "The features were represented as continuous values.",
        "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
        "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
        "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
        "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
        "We use the log-odds estimated by the classifier as s(d).",
        "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
        "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
        "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
        "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
        "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
        "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
        "We report only the sum of these measures and omit the averages for space.",
        "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
        "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
        "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
        "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
        "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
        "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
        "Only pairs that the methods disagree on are used in the sign test.",
        "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
        "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
        "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
        "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
        "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
        "Table 1b gives results for producing probabilistic outputs for SVMs.",
        "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
        "The best entry for a corpus is in bold.",
        "Entries that are statistically significantly better than all other entries are underlined.",
        "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
        "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
        "The reason for this distinction in significance tests is described in the text.",
        "We start with general observations that result from examining the performance of these methods over the various corpora.",
        "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
        "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
        "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
        "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
        "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
        "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
        "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
        "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
        "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
        "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
        "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
        "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
        "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
        "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
        "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
        "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
        "There are several interesting points about the performance of the asymmetric distributions as well.",
        "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
        "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
        "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
        "Figure 3 is actually a result of fitting the two distributions to real data.",
        "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
        "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
        "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
        "The positive class (i.e.",
        "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
        "There are enough such cases overall that it seems clearly inferior to the top three methods.",
        "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
        "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
        "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
        "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
        "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
        "Finally, we can make a few observations about the usefulness of the various performance metrics.",
        "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
        "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
        "Secondly, squared error has a weakness in the other direction.",
        "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
        "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
        "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
        "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
        "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
        "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
        "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
        "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
        "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
        "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
        "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
        "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
        "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
        "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
        "We have given an efficient way to estimate the parameters of these distributions.",
        "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
        "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
        "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
        "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
        "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
        "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
        "REFERENCES [1] P. N. Bennett.",
        "Assessing the calibration of naive bayes posterior estimates.",
        "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
        "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
        "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
        "A continuous speech recognition system embedding mlp into hmm.",
        "In NIPS 89, 1989. [4] G. Brier.",
        "Verification of forecasts expressed in terms of probability.",
        "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
        "The comparison and evaluation of forecasters.",
        "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
        "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
        "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
        "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
        "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
        "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
        "Pattern Classification.",
        "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
        "Hierarchical classification of web content.",
        "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
        "Inductive learning algorithms and representations for text categorization.",
        "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
        "Large margin classification using the perceptron algorithm.",
        "Machine Learning, 37(3):277-296, 1999. [12] I.",
        "Good.",
        "Rational decisions.",
        "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
        "Text categorization with support vector machines: Learning with many relevant features.",
        "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
        "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
        "Birkh¨auser, 2001. [15] D. D. Lewis.",
        "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
        "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
        "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
        "A sequential algorithm for training text classifiers.",
        "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
        "Training algorithms for linear text classifiers.",
        "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
        "On the reconciliation of probability assessments.",
        "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
        "Modeling score distributions for combining the outputs of search engines.",
        "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
        "A comparison of event models for naive bayes text classification.",
        "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
        "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
        "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
        "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
        "Active learning for class probability estimation and ranking.",
        "In IJCAI 01, 2001. [24] R. L. Winkler.",
        "Scoring rules and the evaluation of probability assessors.",
        "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
        "A re-examination of text categorization methods.",
        "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
        "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
        "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
        "Reducing multiclass to binary by coupling probability estimates.",
        "In KDD 02, 2002."
    ],
    "translated_text_sentences": [
        "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación.",
        "Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios.",
        "Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción.",
        "Sin embargo, la calidad de las estimaciones de probabilidad es crucial.",
        "Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente.",
        "Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto.",
        "El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1.",
        "Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking.",
        "Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción.",
        "Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción.",
        "Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo.",
        "Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23].",
        "El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17].",
        "Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3].",
        "Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial.",
        "Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento.",
        "Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos.",
        "Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable.",
        "Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos.",
        "Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos.",
        "Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes.",
        "Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información.",
        "Luego, discutimos con más detalle la necesidad de modelos asimétricos.",
        "Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos.",
        "Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos.",
        "Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras.",
        "TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información.",
        "Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo.",
        "Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda.",
        "Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí.",
        "También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda.",
        "Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento.",
        "El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente.",
        "Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo.",
        "En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal.",
        "Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia.",
        "Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema.",
        "Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas.",
        "Hay una variedad de otros trabajos a los que este documento se extiende.",
        "Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM.",
        "Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor).",
        "Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM.",
        "Reajustar clasificadores mal calibrados no es un problema nuevo.",
        "Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3.",
        "DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1.",
        "Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema).",
        "Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente).",
        "Hay dos tipos generales de enfoques paramétricos.",
        "El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris.",
        "Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)).",
        "El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1.",
        "Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros.",
        "Podemos visualizar esto tal como se muestra en la Figura 2.",
        "Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+).",
        "Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada.",
        "Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2).",
        "De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles.",
        "Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3).",
        "Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador.",
        "Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes.",
        "Además, no hay ejemplos que caigan entre θ- y θ+.",
        "La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad.",
        "Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda.",
        "Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta.",
        "La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos.",
        "Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles).",
        "Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas.",
        "Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana.",
        "Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha.",
        "Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas.",
        "Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica.",
        "Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo.",
        "Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr).",
        "Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua.",
        "Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros.",
        "Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente).",
        "Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera.",
        "Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4).",
        "Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información.",
        "Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente.",
        "Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ.",
        "Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ).",
        "Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ.",
        "Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ.",
        "La derivación completa se omite debido al espacio pero está disponible en [2].",
        "Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
        "Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ.",
        "Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ.",
        "La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes).",
        "Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme.",
        "De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad).",
        "Dr = 0 se maneja de manera similar.",
        "Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable.",
        "Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante.",
        "Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente.",
        "Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente.",
        "Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo.",
        "Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ).",
        "Los EMV pueden ser calculados de manera similar a lo anterior.",
        "Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
        "La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp.",
        "Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0).",
        "Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4.",
        "ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir,",
        "P(c) = |c|+1 N+2 donde N es el número de documentos.",
        "Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba.",
        "Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud.",
        "Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d).",
        "Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d).",
        "El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir,",
        "P(+|d) = P(−|d) = 0.5.",
        "Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19].",
        "Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori.",
        "En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos.",
        "Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud.",
        "Este método está designado en las tablas a continuación como Gauss.",
        "Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente.",
        "Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos.",
        "Este método se denota como A. Gauss.",
        "Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica.",
        "Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14].",
        "Denominamos a este método como Laplace a continuación.",
        "Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente.",
        "Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ.",
        "Este método se denota como A. Laplace abajo.",
        "Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)).",
        "Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase.",
        "A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador.",
        "Cuando se desee, estos métodos pueden ser más apropiados.",
        "Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan.",
        "Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador.",
        "Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo.",
        "El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d).",
        "En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades.",
        "Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori.",
        "Nos referimos a este método como LogReg a continuación.",
        "Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM.",
        "Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros.",
        "Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente.",
        "El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta).",
        "Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud.",
        "Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP.",
        "El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente.",
        "Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9].",
        "La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior.",
        "Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%.",
        "En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%.",
        "Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones.",
        "Los indexadores humanos asignaron los documentos a cero o más categorías.",
        "Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento.",
        "El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987.",
        "Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados).",
        "Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas.",
        "De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22].",
        "Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%.",
        "En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%.",
        "Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento.",
        "El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990.",
        "Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18].",
        "Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave.",
        "Los campos de título y cuerpo se utilizan en los experimentos a continuación.",
        "Hay veinte categorías en total.",
        "Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%.",
        "En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%.",
        "Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación.",
        "Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters.",
        "Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí.",
        "Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts.",
        "Las características fueron representadas como valores continuos.",
        "Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22].",
        "El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero.",
        "El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21].",
        "Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente.",
        "Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d).",
        "El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad.",
        "Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario.",
        "El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2.",
        "Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero.",
        "Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno.",
        "Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas.",
        "Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio.",
        "Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus.",
        "Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades.",
        "Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5.",
        "Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad.",
        "Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos.",
        "Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas.",
        "Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos.",
        "Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial.",
        "Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases.",
        "Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento.",
        "Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado.",
        "Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a.",
        "La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM.",
        "Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha).",
        "La mejor entrada para un corpus está en negrita.",
        "Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas.",
        "Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo.",
        "Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda).",
        "La razón de esta distinción en las pruebas de significancia está descrita en el texto.",
        "Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus.",
        "El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos.",
        "Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg.",
        "Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático.",
        "Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo.",
        "Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters.",
        "La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior.",
        "La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)).",
        "Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista.",
        "Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo.",
        "Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas).",
        "Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias.",
        "Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise.",
        "Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor.",
        "La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17].",
        "Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí).",
        "Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal.",
        "Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también.",
        "Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida.",
        "Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas).",
        "Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar.",
        "La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales.",
        "Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase.",
        "Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters.",
        "También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento.",
        "La clase positiva (es decir,",
        "La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico.",
        "Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales.",
        "Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial).",
        "Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar.",
        "Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos.",
        "Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también.",
        "Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor.",
        "Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento.",
        "Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta.",
        "Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica.",
        "En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección.",
        "Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles.",
        "Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado).",
        "Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado.",
        "Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas.",
        "Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5.",
        "TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores).",
        "A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas.",
        "Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace.",
        "Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica.",
        "Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta.",
        "Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11].",
        "Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6.",
        "RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado.",
        "Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación.",
        "Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones.",
        "Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos.",
        "En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística.",
        "Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad.",
        "Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox.",
        "También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo.",
        "Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7.",
        "REFERENCIAS [1] P. N. Bennett.",
        "Evaluando la calibración de las estimaciones posteriores de Naive Bayes.",
        "Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett.",
        "Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar.",
        "Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan.",
        "Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm.",
        "En NIPS 89, 1989. [4] G. Brier.",
        "Verificación de pronósticos expresados en términos de probabilidad.",
        "Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg.",
        "La comparación y evaluación de pronosticadores.",
        "Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg.",
        "Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas.",
        "En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión.",
        "Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani.",
        "Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple.",
        "En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork.",
        "Clasificación de patrones.",
        "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen.",
        "Clasificación jerárquica de contenido web.",
        "En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami.",
        "Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto.",
        "En CIKM 98, 1998. [11] Y. Freund y R. Schapire.",
        "Clasificación de márgen amplio utilizando el algoritmo del perceptrón.",
        "Aprendizaje automático, 37(3):277-296, 1999. [12] I.",
        "Bien.",
        "Decisiones racionales.",
        "Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims.",
        "Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes.",
        "En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski.",
        "La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas.",
        "Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis.",
        "Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales.",
        "SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis.",
        "Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale.",
        "Un algoritmo secuencial para entrenar clasificadores de texto.",
        "En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka.",
        "Entrenando algoritmos para clasificadores de texto lineales.",
        "En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown.",
        "Sobre la conciliación de evaluaciones de probabilidad.",
        "Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng.",
        "Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda.",
        "En SIGIR 01, 2001. [21] A. McCallum y K. Nigam.",
        "Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes.",
        "En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt.",
        "Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados.",
        "En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios.",
        "MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost.",
        "Aprendizaje activo para la estimación de probabilidades y clasificación en clase.",
        "En IJCAI 01, 2001. [24] R. L. Winkler.",
        "Reglas de puntuación y la evaluación de los evaluadores de probabilidad.",
        "Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu.",
        "Una reevaluación de los métodos de categorización de texto.",
        "En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan.",
        "Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos.",
        "En ICML 01, 2001. [27] B. Zadrozny y C. Elkan.",
        "Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad.",
        "En KDD 02, 2002."
    ],
    "error_count": 5,
    "keys": {
        "bayesian risk model": {
            "translated_key": "modelo de riesgo bayesiano",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a <br>bayesian risk model</br> to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a <br>bayesian risk model</br> [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "For example, rather than choosing one set decision threshold, they can be used in a <br>bayesian risk model</br> to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "For example, rather than choosing one set decision threshold, they can be used in a <br>bayesian risk model</br> [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un <br>modelo de riesgo bayesiano</br> para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción.",
                "Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un <br>modelo de riesgo bayesiano</br> [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un <br>modelo de riesgo bayesiano</br> para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un <br>modelo de riesgo bayesiano</br> [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "empirical score distribution": {
            "translated_key": "distribución empírica de puntuaciones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the <br>empirical score distribution</br> for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the <br>empirical score distribution</br> for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different."
            ],
            "translated_annotated_samples": [
                "Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la <br>distribución empírica de puntuaciones</br> para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la <br>distribución empírica de puntuaciones</br> para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "probability estimate": {
            "translated_key": "estimaciones de probabilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give <br>probability estimate</br>s are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the <br>probability estimate</br>s is crucial.",
                "We review a variety of standard approaches to converting scores (and poor <br>probability estimate</br>s) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give <br>probability estimate</br>s are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, <br>probability estimate</br>s are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the <br>probability estimate</br>s is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving <br>probability estimate</br>s and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor <br>probability estimate</br>s or produce high quality <br>probability estimate</br>s from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain <br>probability estimate</br>s in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the <br>probability estimate</br>s are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce <br>probability estimate</br>s from relevance scores returned from search engines and demonstrated how the resulting <br>probability estimate</br>s could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving <br>probability estimate</br>s has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce <br>probability estimate</br>s of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of <br>probability estimate</br>s these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor <br>probability estimate</br>s output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce <br>probability estimate</br>s from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the <br>probability estimate</br>s.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the <br>probability estimate</br>s have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the <br>probability estimate</br>s.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its <br>probability estimate</br>s, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful <br>probability estimate</br>s.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing <br>probability estimate</br>s from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality <br>probability estimate</br>s.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated <br>probability estimate</br>s from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling <br>probability estimate</br>s.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give <br>probability estimate</br>s are more readily applicable in a variety of scenarios.",
                "However, the quality of the <br>probability estimate</br>s is crucial.",
                "We review a variety of standard approaches to converting scores (and poor <br>probability estimate</br>s) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "INTRODUCTION Text classifiers that give <br>probability estimate</br>s are more flexible in practice than those that give only a simple classification or even a ranking.",
                "Additionally, <br>probability estimate</br>s are often used as the basis of deciding which documents label to request next during active learning [17, 23]."
            ],
            "translated_annotated_samples": [
                "Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan <br>estimaciones de probabilidad</br> son más fácilmente aplicables en una variedad de escenarios.",
                "Sin embargo, la calidad de las <br>estimaciones de probabilidad</br> es crucial.",
                "Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas <br>estimaciones de probabilidad</br>) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente.",
                "Los clasificadores de texto que proporcionan <br>estimaciones de probabilidad</br> son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking.",
                "Además, las <br>estimaciones de probabilidad</br> se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan <br>estimaciones de probabilidad</br> son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las <br>estimaciones de probabilidad</br> es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas <br>estimaciones de probabilidad</br>) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan <br>estimaciones de probabilidad</br> son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las <br>estimaciones de probabilidad</br> se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "text classifier": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve <br>text classifier</br> Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the <br>text classifier</br> have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target <br>text classifier</br> outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target <br>text classifier</br> outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A <br>text classifier</br> produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "Using Asymmetric Distributions to Improve <br>text classifier</br> Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Additionally, these models can be interpreted as assuming the scores produced by the <br>text classifier</br> have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target <br>text classifier</br> outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target <br>text classifier</br> outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A <br>text classifier</br> produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic)."
            ],
            "translated_annotated_samples": [
                "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del <br>clasificador de texto</br> Paul N. Bennett Departamento de Ciencias de la Computación.",
                "Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el <br>clasificador de texto</br> tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes.",
                "Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de <br>clasificadores de texto</br> que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento.",
                "DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de <br>clasificadores de texto</br>, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1.",
                "Un <br>clasificador de texto</br> produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema)."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del <br>clasificador de texto</br> Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el <br>clasificador de texto</br> tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de <br>clasificadores de texto</br> que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de <br>clasificadores de texto</br>, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un <br>clasificador de texto</br> produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). ",
            "candidates": [],
            "error": [
                [
                    "clasificador de texto",
                    "clasificador de texto",
                    "clasificadores de texto",
                    "clasificadores de texto",
                    "clasificador de texto"
                ]
            ]
        },
        "parametric model": {
            "translated_key": "modelos paramétricos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric <br>parametric model</br>s that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric <br>parametric model</br>s suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "We introduce several asymmetric <br>parametric model</br>s that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric <br>parametric model</br>s suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1."
            ],
            "translated_annotated_samples": [
                "Introducimos varios <br>modelos paramétricos</br> asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos.",
                "DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios <br>modelos paramétricos</br> asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many <br>information retrieval</br> tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in <br>information retrieval</br>.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of <br>information retrieval</br>.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or <br>information retrieval</br>.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "Effective active learning can be key in many <br>information retrieval</br> tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "We first review related work on improving probability estimates and score modeling in <br>information retrieval</br>.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of <br>information retrieval</br>.",
                "To our knowledge, neither family of distributions has been previously used in machine learning or <br>information retrieval</br>."
            ],
            "translated_annotated_samples": [
                "El aprendizaje activo efectivo puede ser clave en muchas tareas de <br>recuperación de información</br> donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17].",
                "Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la <br>recuperación de información</br>.",
                "TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de <br>recuperación de información</br>.",
                "Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o <br>recuperación de información</br>."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de <br>recuperación de información</br> donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la <br>recuperación de información</br>. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de <br>recuperación de información</br>. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o <br>recuperación de información</br>. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "logistic regression framework": {
            "translated_key": "marco de regresión logística",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a <br>logistic regression framework</br> that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "Platt [22] uses a <br>logistic regression framework</br> that models noisy class labels to produce probabilities from the raw output of an SVM."
            ],
            "translated_annotated_samples": [
                "Platt [22] utiliza un <br>marco de regresión logística</br> que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un <br>marco de regresión logística</br> que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "posterior function": {
            "translated_key": "función posterior",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the <br>posterior function</br> directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "The first of these tries to fit the <br>posterior function</br> directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey."
            ],
            "translated_annotated_samples": [
                "El primero de estos intenta ajustar directamente la <br>función posterior</br>, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la <br>función posterior</br>, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "asymmetric laplace distribution": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An <br>asymmetric laplace distribution</br> can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the <br>asymmetric laplace distribution</br> to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the <br>asymmetric laplace distribution</br> appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "An <br>asymmetric laplace distribution</br> can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "Also shown is the fit of the <br>asymmetric laplace distribution</br> to the training score distribution.",
                "In striking contrast, the <br>asymmetric laplace distribution</br> appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression."
            ],
            "translated_annotated_samples": [
                "Una <br>distribución asimétrica de Laplace</br> se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha.",
                "También se muestra el ajuste de la <br>distribución Laplace asimétrica</br> a la distribución de puntuaciones de entrenamiento.",
                "En marcado contraste, la <br>distribución asimétrica de Laplace</br> parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una <br>distribución asimétrica de Laplace</br> se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la <br>distribución Laplace asimétrica</br> a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la <br>distribución asimétrica de Laplace</br> parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    "distribución asimétrica de Laplace",
                    "distribución Laplace asimétrica",
                    "distribución asimétrica de Laplace"
                ]
            ]
        },
        "search engine retrieval": {
            "translated_key": "recuperación de motores de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to <br>search engine retrieval</br> where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "This is in contrast to <br>search engine retrieval</br> where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query."
            ],
            "translated_annotated_samples": [
                "Esto contrasta con la <br>recuperación de motores de búsqueda</br>, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la <br>recuperación de motores de búsqueda</br>, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "symmetric distribution": {
            "translated_key": "distribución simétrica",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common <br>symmetric distribution</br>, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a <br>symmetric distribution</br>, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "A natural first candidate for an asymmetric distribution is to generalize a common <br>symmetric distribution</br>, e.g. the Laplace or the Gaussian.",
                "Furthermore, this family of distributions can still fit a <br>symmetric distribution</br>, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4)."
            ],
            "translated_annotated_samples": [
                "Un primer candidato natural para una distribución asimétrica es generalizar una <br>distribución simétrica</br> común, por ejemplo, la Laplace o la Gaussiana.",
                "Además, esta familia de distribuciones aún puede ajustarse a una <br>distribución simétrica</br>, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4)."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una <br>distribución simétrica</br> común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una <br>distribución simétrica</br>, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "asymmetric gaussian": {
            "translated_key": "Gaussiana asimétrica",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an <br>asymmetric gaussian</br> in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this <br>asymmetric gaussian</br>, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 <br>asymmetric gaussian</br> MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An <br>asymmetric gaussian</br> is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the <br>asymmetric gaussian</br>, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the <br>asymmetric gaussian</br> tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the <br>asymmetric gaussian</br> appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "We can create an <br>asymmetric gaussian</br> in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this <br>asymmetric gaussian</br>, we use the notation Γ(X | θ, σl, σr).",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 <br>asymmetric gaussian</br> MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "Asymmetric Gaussians An <br>asymmetric gaussian</br> is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the <br>asymmetric gaussian</br>, intervals between adjacent scores are divided by 10 in testing candidate θs."
            ],
            "translated_annotated_samples": [
                "Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo.",
                "Para referirnos a esta <br>Gaussiana asimétrica</br>, usamos la notación Γ(X | θ, σl, σr).",
                "Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ).",
                "Gaussianas asimétricas Se ajusta una <br>Gaussiana asimétrica</br> a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente.",
                "Al igual que con la <br>Gaussiana asimétrica</br>, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta <br>Gaussiana asimétrica</br>, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una <br>Gaussiana asimétrica</br> a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la <br>Gaussiana asimétrica</br>, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "maximum likelihood estimate": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding <br>maximum likelihood estimate</br>s (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using <br>maximum likelihood estimate</br>s.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual <br>maximum likelihood estimate</br>s.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding <br>maximum likelihood estimate</br>s (MLE) of the parameters for the above asymmetric distributions.",
                "All of the methods below are fit using <br>maximum likelihood estimate</br>s.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual <br>maximum likelihood estimate</br>s."
            ],
            "translated_annotated_samples": [
                "Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar <br>estimaciones de máxima verosimilitud</br> (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente.",
                "Todos los métodos a continuación se ajustan utilizando <br>estimaciones de máxima verosimilitud</br>.",
                "Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las <br>estimaciones habituales de máxima verosimilitud</br>."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar <br>estimaciones de máxima verosimilitud</br> (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando <br>estimaciones de máxima verosimilitud</br>. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las <br>estimaciones habituales de máxima verosimilitud</br>. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    "estimaciones de máxima verosimilitud",
                    "estimaciones de máxima verosimilitud",
                    "estimaciones habituales de máxima verosimilitud"
                ]
            ]
        },
        "class-conditional densities": {
            "translated_key": "densidades condicionales de clase",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the <br>class-conditional densities</br> (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the <br>class-conditional densities</br>; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the <br>class-conditional densities</br>, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the <br>class-conditional densities</br>, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the <br>class-conditional densities</br> using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the <br>class-conditional densities</br> using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the <br>class-conditional densities</br>, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the <br>class-conditional densities</br> are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "An estimator for each of the <br>class-conditional densities</br> (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the <br>class-conditional densities</br>; they differ only in the criterion used to estimate the parameters.",
                "For methods that fit the <br>class-conditional densities</br>, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "Gaussians A Gaussian is fit to each of the <br>class-conditional densities</br>, using the usual maximum likelihood estimates.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the <br>class-conditional densities</br> using the maximum likelihood estimation procedure described above.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the <br>class-conditional densities</br> using the maximum likelihood estimation procedure described above."
            ],
            "translated_annotated_samples": [
                "Se produce un estimador para cada una de las <br>densidades condicionales de clase</br> (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las <br>densidades condicionales de clase</br>; difieren solo en el criterio utilizado para estimar los parámetros.",
                "Para los métodos que se ajustan a las <br>densidades condicionales de clase</br>, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba.",
                "Se ajusta una distribución gaussiana a cada una de las <br>densidades condicionales de clase</br>, utilizando las estimaciones habituales de máxima verosimilitud.",
                "Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las <br>densidades condicionales de clase</br> utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente.",
                "Se ajusta una distribución Laplace asimétrica a cada una de las <br>densidades condicionales de clase</br> utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las <br>densidades condicionales de clase</br> (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las <br>densidades condicionales de clase</br>; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las <br>densidades condicionales de clase</br>, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las <br>densidades condicionales de clase</br>, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las <br>densidades condicionales de clase</br> utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las <br>densidades condicionales de clase</br> utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "decision threshold": {
            "translated_key": "umbral de decisión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set <br>decision threshold</br>, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set <br>decision threshold</br>, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal <br>decision threshold</br> (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal <br>decision threshold</br> (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal <br>decision threshold</br> is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the <br>decision threshold</br> P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "For example, rather than choosing one set <br>decision threshold</br>, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "For example, rather than choosing one set <br>decision threshold</br>, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "The normal <br>decision threshold</br> (minimizing error) in terms of log-odds is at zero (i.e.",
                "The normal <br>decision threshold</br> (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "The normal <br>decision threshold</br> is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, en lugar de elegir un <br>umbral de decisión</br> fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción.",
                "Por ejemplo, en lugar de elegir un <br>umbral de decisión</br> fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción.",
                "El <br>umbral de decisión</br> normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir,",
                "El <br>umbral de decisión</br> normal (suponiendo que buscamos minimizar errores) para este clasificador es cero.",
                "El <br>umbral de decisión</br> normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un <br>umbral de decisión</br> fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un <br>umbral de decisión</br> fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El <br>umbral de decisión</br> normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El <br>umbral de decisión</br> normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El <br>umbral de decisión</br> normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "text classification": {
            "translated_key": "clasificación de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many <br>text classification</br> tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several <br>text classification</br> corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes <br>text classification</br>.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "Since many <br>text classification</br> tasks often have very little training data, we focus on parametric methods.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several <br>text classification</br> corpora to demonstrate the strengths and weaknesses of the various methods.",
                "A comparison of event models for naive bayes <br>text classification</br>."
            ],
            "translated_annotated_samples": [
                "Dado que muchas tareas de <br>clasificación de texto</br> a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos.",
                "Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de <br>clasificación de texto</br> para demostrar las fortalezas y debilidades de los diferentes métodos.",
                "Una comparación de modelos de eventos para la <br>clasificación de texto</br> con Naive Bayes."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de <br>clasificación de texto</br> a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de <br>clasificación de texto</br> para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en aprendizaje activo. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la <br>clasificación de texto</br> con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "cost-sensitive learn": {
            "translated_key": "aprendizaje sensible al costo",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "active learn": {
            "translated_key": "aprendizaje activo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during <br>active learn</br>ing [17, 23].",
                "Effective <br>active learn</br>ing can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in <br>active learn</br>ing.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in <br>active learn</br>ing.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during <br>active learn</br>ing [17, 23].",
                "Effective <br>active learn</br>ing can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in <br>active learn</br>ing.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in <br>active learn</br>ing."
            ],
            "translated_annotated_samples": [
                "Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el <br>aprendizaje activo</br> [17, 23].",
                "El <br>aprendizaje activo</br> efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17].",
                "Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el <br>aprendizaje activo</br>.",
                "Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en <br>aprendizaje activo</br>."
            ],
            "translated_text": "Utilizando distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu RESUMEN Los clasificadores de texto que proporcionan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano para emitir una decisión en tiempo de ejecución que minimice una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntuaciones (y malas estimaciones de probabilidad) de clasificadores de texto en estimaciones de alta calidad e introducimos nuevos modelos motivados por la intuición de que la distribución empírica de puntuaciones para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes a menudo es significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos mientras aumenta la flexibilidad), computacionalmente eficiente y preferible empíricamente. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.2 [Reconocimiento de Patrones]: Metodología de Diseño Términos Generales Algoritmos, Experimentación, Confiabilidad. 1. Los clasificadores de texto que proporcionan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo ofrecen una clasificación simple o incluso un ranking. Por ejemplo, en lugar de elegir un umbral de decisión fijo, se pueden utilizar en un modelo de riesgo bayesiano [8] para emitir una decisión en tiempo de ejecución que minimice el costo esperado de una función de costo especificada por el usuario, elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para tareas de filtrado donde los costos preespecificados de relevante/no relevante no están disponibles durante el entrenamiento, pero se especifican en el momento de la predicción. Además, los costos pueden cambiarse sin necesidad de volver a entrenar el modelo. Además, las estimaciones de probabilidad se utilizan frecuentemente como base para decidir qué etiqueta de documentos solicitar a continuación durante el <br>aprendizaje activo</br> [17, 23]. El <br>aprendizaje activo</br> efectivo puede ser clave en muchas tareas de recuperación de información donde obtener datos etiquetados puede ser costoso, reduciendo significativamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también están dispuestos a tomar otros tipos de decisiones sensibles al costo [26] y a combinar decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones de que los datos se ajustan al modelo para equilibrar la flexibilidad con la capacidad de estimar con precisión los parámetros del modelo con poca cantidad de datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos enfocamos en métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que consideramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes al tema tengan diferentes varianzas, típicamente imponen la restricción innecesaria de que los documentos estén distribuidos simétricamente alrededor de sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostramos cómo podemos ajustar eficientemente los modelos. Además, estos modelos pueden interpretarse como asumiendo que las puntuaciones producidas por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos trabajos relacionados sobre la mejora de estimaciones de probabilidad y modelado de puntuaciones en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Bayes ingenuo y SVMs, demostramos cómo pueden ser utilizados eficientemente para recalibrar estimaciones de probabilidad pobres o producir estimaciones de probabilidad de alta calidad a partir de puntajes brutos. Luego revisamos experimentos utilizando métodos previamente propuestos y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diferentes métodos. Finalmente, resumimos nuestras contribuciones y discutimos las direcciones futuras. TRABAJO RELACIONADO Se han empleado modelos paramétricos para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo aunque la calidad de las estimaciones de probabilidad no se evalúa directamente; simplemente se realiza como un paso intermedio en el <br>aprendizaje activo</br>. Manmatha et al. [20] introdujeron modelos apropiados para producir estimaciones de probabilidad a partir de puntuaciones de relevancia devueltas por motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían ser posteriormente empleadas para combinar las salidas de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También investigan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en enfoque a estos intentos previos de modelar las puntuaciones de los motores de búsqueda, pero nos enfocamos en las salidas de clasificadores de texto que hemos encontrado que demuestran un tipo diferente de comportamiento en la distribución de puntuaciones debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny & Elkan [26] proporcionan una medida correctiva para árboles de decisión (llamada recorte) y un método no paramétrico para recalibrar el Bayes ingenuo. En un trabajo más reciente [27], investigan el uso de un método semiparamétrico que utiliza un ajuste monótono de piezas constantes a los datos y aplican el método al Bayes ingenuo y a una SVM lineal. Aunque compararon sus métodos con otros métodos paramétricos basados en simetría, no lograron proporcionar resultados de pruebas de significancia. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semiparamétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de las puntuaciones generadas por el clasificador (el número de valores distintos generados), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos a los que este documento se extiende. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida cruda de un SVM. Su trabajo demostró que este método de post-procesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM entrenadas directamente para producir probabilidades (métodos de núcleo de verosimilitud regularizados), sino que también tiende a producir núcleos más dispersos (que generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas al aplicar el método de Platts para la recalibración del Naïve Bayes, pero encontró que había más áreas problemáticas que cuando se aplicaba a las SVM. Reajustar clasificadores mal calibrados no es un problema nuevo. Lindley et al. [19] propusieron por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] proporcionaron la formalización estándar aceptada actualmente para el problema de evaluar la calibración iniciado por otros [4, 24]. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE Nuestro trabajo difiere de enfoques anteriores principalmente en tres puntos: (1) Proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles; (2) Analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y otros métodos producen y proporcionamos pruebas de significancia para estos resultados; (3) Nos enfocamos en las salidas de clasificadores de texto, mientras que la mayoría de la literatura anterior se centró en las salidas de motores de búsqueda. 3.1 Definición del Problema El problema general con el que nos preocupamos se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y proporciona una puntuación s(d) que indica la fuerza de su decisión de que el documento pertenece a la clase positiva (relacionada con el tema). Suponemos en todo momento que solo hay dos clases: la clase positiva y la clase negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta ajustar directamente la función posterior, es decir, hay una regla de Bayes p(s|+) p(s|−) P(+) P(−) Clasificador P(+| s(d)) Predecir clase, c(d)={+,−} confianza s(d) de que c(d)=+ Documento, d y dar la Figura 1 sin normalizar: Nos preocupa cómo realizar el recuadro resaltado en gris. Los componentes internos son para un tipo de enfoque. estimador de función que realiza un mapeo directo de la puntuación s a la probabilidad P(+|s(d)). El segundo tipo de enfoque descompone el problema tal como se muestra en el recuadro gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, p(s|+) y p(s|−)), luego se utiliza la regla de Bayes y las probabilidades a priori de clase para obtener la estimación de P(+|s(d)). 3.2 Motivación para Distribuciones Asimétricas La mayoría de los enfoques paramétricos anteriores a este problema corresponden, ya sea directa o indirectamente (cuando se ajusta solo el posterior), a ajustar gaussianas a las densidades condicionales de clase; difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto tal como se muestra en la Figura 2. Dado que un aumento en s generalmente indica una mayor probabilidad de pertenecer a la clase positiva, entonces la distribución más a la derecha generalmente corresponde a p(s|+). Sin embargo, el uso de gaussianas estándar no aprovecha una característica básica comúnmente observada. Es decir, si tenemos un puntaje de salida en bruto que se puede utilizar para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) suele ser muy diferente al que se encuentra fuera de los modos (etiquetas A y C en la Figura 2). De manera intuitiva, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente son fácilmente distinguibles. Esto sugiere que quizás queramos desacoplar la escala de los segmentos externo e interno de la distribución (como se muestra en la curva denominada A-Gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una elección más apropiada para aplicarla a la puntuación de salida en bruto de un clasificador. Idealmente (es decir, clasificación perfecta) existirán puntuaciones θ− y θ+ tales que todos los ejemplos con puntuación mayor que θ+ son relevantes y todos los ejemplos con puntuaciones menores que θ− son irrelevantes. Además, no hay ejemplos que caigan entre θ- y θ+. La distancia | θ− − θ+ | corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntajes es principalmente un factor de la cantidad de datos de entrenamiento y la separación consiguiente en las clases lograda. Esto contrasta con la recuperación de motores de búsqueda, donde la distribución de puntajes es más un factor de la distribución del lenguaje en los documentos, la función de similitud, y la longitud y tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad uno y cero y muchos métodos funcionarán para propósitos típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones tanto para por qué podrías encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, por ejemplo, la Laplace o la Gaussiana. Una distribución asimétrica de Laplace se puede lograr colocando dos exponenciales alrededor de la moda de la siguiente manera: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) donde θ, β y γ son los parámetros del modelo. θ es la moda de la distribución, β es la escala inversa de la exponencial a la izquierda de la moda, y γ es la escala inversa de la exponencial a la derecha. Utilizaremos la notación Λ(X | θ, β, γ) para referirnos a esta distribución. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Clase={+,-}) Puntuación de Confianza No Normalizada s Gaussiana A-Gaussiana Figura 3: Gaussianas vs. Gaussianas Asimétricas. Una limitación de las distribuciones simétricas: las líneas verticales muestran los modos estimados de forma no paramétrica. Podemos crear una Gaussiana asimétrica de la misma manera: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a esta Gaussiana asimétrica, usamos la notación Γ(X | θ, σl, σr). Si bien estas distribuciones están compuestas por mitades, la función resultante es una única distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con mucha mayor flexibilidad a cambio de solo ajustar seis parámetros. Podríamos intentar en su lugar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos la misma cantidad de parámetros (y a menudo pueden ser más costosas computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes realmente se comportan de esta manera. Además, esta familia de distribuciones aún puede ajustarse a una distribución simétrica, y finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna de las dos familias de distribuciones ha sido utilizada previamente en aprendizaje automático o recuperación de información. Ambos se denominan generalizaciones de una Laplace Asimétrica en [14], pero nos referimos a ellos como se describe arriba para reflejar la forma en que los derivamos para esta tarea. 3.3 Estimación de los parámetros de las distribuciones asimétricas. Esta sección desarrolla el método para encontrar estimaciones de máxima verosimilitud (MLE) de los parámetros para las distribuciones asimétricas mencionadas anteriormente. Para encontrar los EMV, tenemos dos opciones: (1) utilizar estimación numérica para estimar los tres parámetros a la vez, (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dados nuestra elección de θ, luego considerar valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método. 3.3.1 Estimaciones MLE de Laplace asimétricas Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Λ(X | θ, β, γ), la verosimilitud es N i Λ(X | θ, β, γ). Ahora, fijamos θ y calculamos la máxima verosimilitud para esa elección de θ. Entonces, simplemente podemos considerar todas las opciones de θ y elegir aquella con la máxima verosimilitud entre todas las opciones de θ. La derivación completa se omite debido al espacio pero está disponible en [2]. Definimos los siguientes valores: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ. Ten en cuenta que Dl y Dr son la suma de las diferencias absolutas entre las x pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los EMV para β y γ para un θ fijo son: βEMV = N Dl + √ DrDl γEMV = N Dr + √ DrDl. Estas estimaciones no son del todo inesperadas ya que obtendríamos Nl Dl si estimáramos β de forma independiente de γ. La elegancia de las fórmulas radica en que las estimaciones tienden a ser simétricas solo en la medida en que los datos lo dicten (es decir, cuanto más cercanos sean Dl y Dr a ser iguales, más cercanas serán las escalas inversas resultantes). Por argumentos de continuidad, cuando N = 0, asignamos β = γ = 0 donde 0 es una constante pequeña que actúa para dispersar la distribución a una uniforme. De manera similar, cuando N = 0 y Dl = 0, asignamos β = inf donde inf es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda la masa en θ para esa mitad). Dr = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] dependiendo solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado Nl, Sl, Nr, Sr, podemos calcular el posterior y los MLEs en tiempo constante. Además, si los puntajes están ordenados, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, recorremos los puntajes una vez y establecemos Nl, Sl, Nr, Sr apropiadamente. Luego aumentamos θ y simplemente pasamos por encima de las puntuaciones que se han desplazado del lado derecho de la distribución al lado izquierdo. Suponiendo que el número de candidatos θ es O(n), este proceso es O(n), y el proceso general está dominado por la clasificación de las puntuaciones, O(n log n) (o tiempo lineal esperado). 3.3.2 MLEs Gaussianos Asimétricos Para D = {x1, x2, . . . , xN } donde los xi son i.i.d. y X ∼ Γ(X | θ, σl, σr), la verosimilitud es N i Γ(X | θ, β, γ). Los EMV pueden ser calculados de manera similar a lo anterior. Suponemos las mismas definiciones que arriba (la derivación completa omitida por espacio está disponible en [2]), y además, dejemos: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr. La solución analítica para los MLEs para un θ fijo es: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) Por argumentos de continuidad, cuando N = 0, asignamos σr = σl = inf , y cuando N = 0 y Dl2 = 0 (resp. Cuando Dr2 = 0, asignamos σl = 0 (o σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica para estimar estos parámetros. 4. ANÁLISIS EXPERIMENTAL 4.1 Métodos Para cada uno de los métodos que utilizan una clase previa, utilizamos una estimación suavizada de uno adicional, es decir, P(c) = |c|+1 N+2 donde N es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, p(s|+) y p(s|−), las densidades resultantes se invierten utilizando la regla de Bayes como se describe arriba. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las malas estimaciones de probabilidad generadas por el clasificador), es habitual utilizar el logaritmo de las probabilidades de los estimados del clasificador como s(d). Los logaritmos de las probabilidades son definidos como log P (+|d) P (−|d). El umbral de decisión normal (minimizando el error) en términos de logaritmos de probabilidades está en cero (es decir, P(+|d) = P(−|d) = 0.5. Dado que escala las salidas a un espacio [−∞, ∞], las logaritmos de probabilidades hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis & Gale [17] ofrecen un punto de vista más motivador que ajustar los logaritmos de las probabilidades es un efecto amortiguador para la suposición inexacta de independencia y una corrección de sesgo para estimaciones inexactas de las probabilidades a priori. En general, ajustar los logaritmos de las probabilidades puede servir para potenciar o disminuir la señal del clasificador original según lo dicten los datos. Se ajusta una distribución gaussiana a cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima verosimilitud. Este método está designado en las tablas a continuación como Gauss. Gaussianas asimétricas Se ajusta una Gaussiana asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 al probar candidatos θ, es decir, se prueban 8 puntos entre las puntuaciones reales que ocurren en el conjunto de datos. Este método se denota como A. Gauss. Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene un beneficio de la forma asimétrica. Se utilizaron los estimadores MLE habituales para estimar la ubicación y la escala de una distribución Laplace simétrica clásica, tal como se describe en [14]. Denominamos a este método como Laplace a continuación. Se ajusta una distribución Laplace asimétrica a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con la Gaussiana asimétrica, los intervalos entre puntuaciones adyacentes se dividen por 10 al probar candidatos de θ. Este método se denota como A. Laplace abajo. Regresión Logística Este método es el primero de los dos métodos que evaluamos que ajustan directamente el posterior, P(+|s(d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros; difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una ventaja adicional de estos métodos es que preservan por completo la clasificación dada por el clasificador. Cuando se desee, estos métodos pueden ser más apropiados. Los métodos anteriores en su mayoría conservarán los rankings, pero pueden desviarse si los datos lo dictan. Por lo tanto, pueden modelar mejor el comportamiento de los datos a costa de alejarse de una restricción de monotonía en la salida del clasificador. Lewis & Gale [17] utilizan regresión logística para recalibrar el clasificador Bayesiano ingenuo para su posterior uso en <br>aprendizaje activo</br>. El modelo que utilizan es: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . En lugar de utilizar directamente las probabilidades generadas por el clasificador, utilizan el logaritmo de la razón de verosimilitud de las probabilidades, log P (d|+) P (d|−) , como la puntuación s(d). En lugar de usar esto de abajo, utilizaremos la razón de logaritmos de probabilidades. Esto no afecta al modelo, ya que simplemente desplaza todas las puntuaciones por una constante determinada por las probabilidades a priori. Nos referimos a este método como LogReg a continuación. Regresión Logística con Etiquetas de Clase Ruidosas. Platt [22] propone un marco que extiende el modelo de regresión logística mencionado anteriormente para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida cruda de un SVM. Este modelo difiere del modelo LogReg solo en cómo se estiman los parámetros. Los parámetros siguen siendo ajustados utilizando la estimación de máxima verosimilitud, pero se utiliza un modelo de etiquetas de clase ruidosas además, para permitir la posibilidad de que la clase haya sido etiquetada incorrectamente. El ruido se modela asumiendo que hay una probabilidad finita de etiquetar incorrectamente un ejemplo positivo y de etiquetar incorrectamente un ejemplo negativo; estas dos estimaciones de ruido se determinan por el número de ejemplos positivos y el número de ejemplos negativos (usando la regla de Bayes para inferir la probabilidad de etiqueta incorrecta). Aunque no se esperaría que el rendimiento de este modelo difiera mucho del de LogReg, lo evaluamos para asegurar su completitud. Nos referimos a este método como LR+Ruido. 4.2 Datos Examinamos varios corpus, incluyendo el Directorio Web de MSN, Reuters y TREC-AP. El Directorio Web de MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que han sido clasificadas jerárquicamente. Utilizamos la misma división de documentos de entrenamiento/prueba de 50078/10024 que se reportó en [9]. La jerarquía web de MSN es una jerarquía de siete niveles; utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el conjunto de entrenamiento varían del 1.15% al 22.29%. En el conjunto de pruebas, van desde el 1.14% hasta el 21.54%. Las clases son materias generales como Salud y Fitness y Viajes y Vacaciones. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con mayor información mutua para cada clase; aproximadamente 195 mil palabras aparecen en al menos tres documentos de entrenamiento. El corpus Reuters 21578 contiene artículos de noticias de Reuters del año 1987. Para este conjunto de datos, utilizamos la división estándar de entrenamiento/prueba de ModApte de 9603/3299 documentos (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, acq para adquisiciones, earn para ganancias, etc.) que los etiquetadores humanos aplicaron al documento; un documento puede tener varios temas. De hecho, hay 135 clases en este dominio (solo 90 de las cuales aparecen en el conjunto de entrenamiento y prueba); sin embargo, solo examinamos las diez clases más frecuentes, ya que los números pequeños de ejemplos de prueba dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza. Limitar a las diez clases más grandes nos permite comparar nuestros resultados con resultados previamente publicados [10, 13, 21, 22]. Las proporciones de clase en el conjunto de entrenamiento varían del 1.88% al 29.96%. En el conjunto de pruebas, van desde el 1.7% hasta el 32.95%. Para los experimentos a continuación, utilizamos solo las 300 palabras principales con mayor información mutua para cada clase; aproximadamente 15 000 palabras aparecen en al menos tres documentos de entrenamiento. El corpus TREC-AP es una colección de noticias de AP de 1988 a 1990. Utilizamos la misma división de documentos de entrenamiento/prueba de 142791/66992 que se utilizó en [18]. Como se describe en [17] (ver también [15]), las categorías están definidas por palabras clave en un campo de palabras clave. Los campos de título y cuerpo se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de clase en el conjunto de entrenamiento varían del 0.06% al 2.03%. En el conjunto de pruebas, van desde el 0.03% hasta el 4.32%. Para los experimentos descritos a continuación, utilizamos solo las 1000 palabras principales con la información mutua más alta para cada clase; aproximadamente 123 mil palabras aparecen en al menos 3 documentos de entrenamiento. 4.3 Clasificadores Seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal, que es un clasificador discriminativo que normalmente no produce valores de probabilidad, y un clasificador de Bayes ingenuo cuyas salidas de probabilidad suelen ser deficientes [1, 7] pero pueden mejorarse [1, 26, 27]. También se realizó una comparación separada solo entre LogReg, LR+Noise y A. Laplace en las 90 categorías de Reuters. Después de tener en cuenta la varianza, esa evaluación también respaldó las afirmaciones hechas aquí. Para SVM lineales, utilizamos la herramienta Smox que se basa en el algoritmo de Optimización Secuencial Mínima de Platts. Las características fueron representadas como valores continuos. Utilizamos la puntuación de salida en bruto del SVM como s(d) ya que se ha demostrado que es apropiada anteriormente [22]. El umbral de decisión normal (suponiendo que buscamos minimizar errores) para este clasificador es cero. El modelo de clasificador de Bayes ingenuo es un modelo multinomial [21]. Suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de palabras) y una estimación m de Laplace, respectivamente. Utilizamos los logaritmos de las probabilidades estimadas por el clasificador como s(d). El umbral de decisión normal está en cero. 4.4 Medidas de rendimiento Utilizamos la pérdida logarítmica [12] y el error cuadrático [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase c(d) ∈ {+, −} (es decir, los datos tienen etiquetas conocidas y no probabilidades), la pérdida logarítmica se define como δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) donde δ(a, b) . = 1 si a = b y 0 en caso contrario. El error cuadrático es δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida logarítmica es cero y el error cuadrático es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida logarítmica es −∞ y el error cuadrático es uno. Por lo tanto, ambas medidas evalúan qué tan cerca está una estimación de predecir correctamente la clase de los elementos, pero varían en la severidad con la que se penalizan las predicciones incorrectas. Informamos solo la suma de estas medidas y omitimos los promedios por cuestiones de espacio. Sus promedios, pérdida logarítmica promedio y error cuadrático medio (MSE) se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo han mejorado las estimaciones de probabilidad con respecto al umbral de decisión P(+|d) = 0.5. Por lo tanto, el error solo indica cómo se desempeñarían los métodos si un falso positivo fuera penalizado de la misma manera que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de signo de micro emparejado estándar [25] para determinar la significancia estadística en la diferencia de todas las medidas. Solo se utilizan los pares en los que los métodos no están de acuerdo en la prueba de signos. Este test compara pares de puntuaciones de dos sistemas con la hipótesis nula de que el número de elementos en los que discrepan sigue una distribución binomial. Utilizamos un nivel de significancia de p = 0.01. 4.5 Metodología Experimental Dado que las categorías consideradas en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando n clasificadores binarios, donde n es el número de clases. Para generar las puntuaciones que cada método utiliza para ajustar sus estimaciones de probabilidad, utilizamos validación cruzada de cinco pliegues en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar validación cruzada de dejar uno fuera para el clasificador de Bayes ingenuo, esto puede no ser deseable ya que la distribución de puntajes puede verse sesgada como resultado. Por supuesto, al igual que con cualquier aplicación de validación cruzada n-fold, también es posible sesgar los resultados al mantener n demasiado bajo y subestimar el rendimiento del clasificador final. 4.6 Resultados y Discusión Los resultados para recalibrar el Bayes ingenuo se muestran en la Tabla 1a. La Tabla 1b muestra los resultados para la producción de salidas probabilísticas para las SVM. Error de pérdida logarítmica2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Ruido -36468.18 6534.61 8563 Bayes ingenuo -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Ruido -3374.15 604.80 785 Bayes ingenuo -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Ruido -48251.51 7544.84 8801 Bayes ingenuo -1903487.10 41770.21 43661 Error de pérdida logarítmica2 Errores MSN Web Gauss -54463.32 9090.57 10555 A.Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A.Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Ruido -30294.01 5209.80 6551 SVM Lineal N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A.Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A.Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Ruido -2567.68 408.82 516 SVM Lineal N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A.Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A.Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Ruido -48214.96 5919.25 6794 SVM Lineal N/A N/A 6718 Tabla 1: (a) Resultados para Bayes ingenuo (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. Un † indica que el método es significativamente mejor que todos los demás métodos, excepto por el método de Bayes ingenuo. Un ‡ indica que la entrada es significativamente mejor que todos los demás métodos excepto por A. Gauss (y Bayes ingenuo para la tabla de la izquierda). La razón de esta distinción en las pruebas de significancia está descrita en el texto. Comenzamos con observaciones generales que resultan de examinar el rendimiento de estos métodos en los diversos corpus. El primero es que A. Laplace, LR+Noise y LogReg claramente superan a los otros métodos. Por lo general, hay poca diferencia entre el rendimiento de LR+Noise y LogReg (tanto como se muestra aquí como en una base de decisión por decisión), pero esto no es sorprendente ya que LR+Noise simplemente agrega etiquetas de clase ruidosas al modelo LogReg. Con respecto a las tres medidas diferentes, LR+Noise y LogReg tienden a tener un rendimiento ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas en relación con la pérdida logarítmica y el error cuadrático. Sin embargo, A. Laplace siempre produce la menor cantidad de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor idea del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por el método más competitivo en comparación con el comportamiento real de los datos (estimado de forma no paramétrica mediante agrupación) para la clase Earn en Reuters. La Figura 4 muestra las densidades condicionales de clase, por lo que solo se muestra A. Laplace ya que LogReg ajusta directamente el posterior. La Figura 5 muestra las estimaciones de los logaritmos de las probabilidades, es decir, log P (Ganar|s(d)) P (¬Ganar|s(d)). Visualizar los logaritmos de las probabilidades a posteriori (en lugar de las probabilidades a posteriori) generalmente permite detectar errores en la estimación de manera más fácil a simple vista. Podemos desglosar las cosas como lo hace la prueba de signos y simplemente observar las victorias y derrotas en los elementos en los que los métodos no están de acuerdo. Vistos de esta manera, solo dos métodos (naïve Bayes y A. Gauss) tienen más victorias en pares que A. Laplace; esos dos a veces tienen más victorias en pares en pérdida logarítmica y error cuadrático, aunque nunca ganan en total (es decir, son arrastrados por penalizaciones severas). Además, esta comparación de victorias por pares significa que para aquellos casos en los que LogReg y LR+Noise tienen puntajes mejores que A. Laplace, no se consideraría significativo por la prueba de signos en ningún nivel, ya que no tienen más victorias. Por ejemplo, de las 130,000 decisiones binarias sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101,000 victorias en pares frente a LogReg y LR+Noise. Ningún método tiene más victorias en pares que A. Laplace para la comparación de errores, ni ningún método logra un total mejor. La observación básica hecha sobre el método de Bayes ingenuo en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y uno [1, 17]. Esto significa que si tiende a ser correcto la mayor parte del tiempo, producirá resultados que no parecen significativos en una prueba de signos que ignora el tamaño de la diferencia (como la que se muestra aquí). Las sumas del error cuadrático y la pérdida logarítmica confirman la observación previa de que cuando está mal, está realmente mal. Hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas también. Primero, A. Gauss tiene un rendimiento deficiente porque (similar al Bayes ingenuo) hay algunos ejemplos donde se le penaliza en gran medida. Este comportamiento resulta de una tendencia general a comportarse como la imagen mostrada en la Figura 3 (nota el cruce en las colas). Si bien la distribución gaussiana asimétrica tiende a colocar el modo de manera mucho más precisa que una gaussiana simétrica, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa en las colas exteriores, sin ajustarse lo suficientemente alrededor del modo para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos, A. Gauss puede desempeñarse bastante competitivamente, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Clase={+,-}) s(d) = Bayes ingenuo logaritmo de probabilidades Entrenamiento Prueba A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Clase={+,-}) s(d) = SVM lineal puntuación bruta Entrenamiento Prueba A.Laplace Figura 4: La distribución empírica de las puntuaciones del clasificador para documentos en el conjunto de entrenamiento y el conjunto de prueba para la clase Earn en Reuters. También se muestra el ajuste de la distribución Laplace asimétrica a la distribución de puntuaciones de entrenamiento. La clase positiva (es decir, La clase positiva (es decir, Earn) es la distribución a la derecha en cada gráfico, y la clase negativa (es decir, ¬Earn) es la de la izquierda en cada gráfico. Hay suficientes casos de este tipo en general que parece claramente inferior a los tres métodos principales. Sin embargo, la distribución asimétrica de Laplace pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piensa en el pico agudo de una exponencial). Como resultado, la mayor parte de la masa se mantiene centrada alrededor del modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que la Laplace estándar. Dado que el Laplace estándar también corresponde a un ajuste por tramos en el espacio de logaritmos de probabilidades, esto resalta que parte del poder de los métodos asimétricos radica en su sensibilidad para colocar los puntos de inflexión en los modos reales, en lugar de la suposición simétrica de que las medias corresponden a los modos. Además, los métodos asimétricos tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea también. Incluso en casos donde la distribución de prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace sigue proporcionando una solución que se ajusta mejor que LogReg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diferentes métricas de rendimiento. Primero, la pérdida logarítmica solo otorga una cantidad finita de crédito a medida que mejora el grado de corrección de algo (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente por una estimación incorrecta. Por lo tanto, es posible que un valor atípico sesgue los totales, pero clasificar erróneamente este ejemplo puede no importar para ninguna otra función de utilidad real utilizada en la práctica. En segundo lugar, el error cuadrático tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando está produciendo lo que generalmente consideramos estimaciones de probabilidad poco útiles. Por ejemplo, considera un método que solo estima probabilidades como cero o uno (a lo que tiende el Bayes ingenuo pero no alcanza completamente si se utiliza suavizado). Este método podría ganar según el error cuadrático, pero con solo un error nunca superaría en pérdida logarítmica a cualquier método que asigne alguna probabilidad no nula a cada resultado. Por estas razones, recomendamos que ninguno de estos se utilice de forma aislada, ya que cada uno proporciona perspectivas ligeramente diferentes sobre la calidad de las estimaciones producidas. Estas observaciones son directas a partir de las definiciones, pero están subrayadas por la evaluación. 5. TRABAJO FUTURO Una extensión prometedora al trabajo presentado aquí es una distribución híbrida de una Gaussiana (en las pendientes exteriores) y exponenciales (en las pendientes interiores). A partir de la evidencia empírica presentada en [22], la expectativa es que dicha distribución pueda permitir más énfasis de la masa de probabilidad alrededor de los modos (como en el caso de la exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite ajustar directamente el logaritmo de las probabilidades a posteriori con una línea, podríamos ajustar directamente el logaritmo de las probabilidades a posteriori con una línea de tres piezas (un spline) en lugar de hacer lo mismo indirectamente ajustando la distribución asimétrica de Laplace. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría pero no la suposición de que las densidades condicionales de clase provienen de una distribución Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la adecuación de estos métodos para la salida de un perceptrón votado [11]. Por analogía con las probabilidades logarítmicas, la puntuación operativa que parece prometedora es la suma de los votos de los perceptrones con peso logarítmico y los votos de los perceptrones con peso. - 0.6. RESUMEN Y CONCLUSIONES Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones crudas de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos proporcionado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que otorgan los parámetros asimétricos añadidos, la Gaussiana asimétrica parece tener un énfasis excesivo lejos de los modos. En marcado contraste, la distribución asimétrica de Laplace parece ser preferible sobre varios dominios de texto grandes y una variedad de medidas de rendimiento en comparación con los principales métodos paramétricos competidores, aunque a veces se logra un rendimiento comparable con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código del test de signos, a Anton Likhodedov por el código de regresión logística y a John Platt por el soporte del código para la herramienta de clasificación SVM lineal Smox. También agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este artículo. 7. REFERENCIAS [1] P. N. Bennett. Evaluando la calibración de las estimaciones posteriores de Naive Bayes. Informe técnico CMU-CS-00-155, Carnegie Mellon, Escuela de Ciencias de la Computación, 2000. [2] P. N. Bennett. Utilizando distribuciones asimétricas para mejorar las probabilidades del clasificador: Una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Ciencias de la Computación, 2002. [3] H. Bourlard y N. Morgan. Un sistema de reconocimiento continuo del habla que incorpora mlp en hmm. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revista Mensual del Clima, 78:1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de pronosticadores. Estadístico, 32:12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: Conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencia Bayesiana y Técnicas de Decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: Condiciones para la optimalidad del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Real Sociedad Estadística, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas de vectores de soporte: Aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La Distribución de Laplace y sus Generalizaciones: Una Revisión con Aplicaciones a Comunicaciones, Economía, Ingeniería y Finanzas. Birkhäuser, 2001. [15] D. D. Lewis. \n\nBirkhäuser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para entrenar clasificadores de texto: Corrección y datos adicionales. SIGIR Forum, 29(2):13-19, Otoño 1995. [16] D. D. Lewis. Reuters-21578, distribución 1.0. http://www.daviddlewis.com/resources/testcollections/reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para entrenar clasificadores de texto. En SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Entrenando algoritmos para clasificadores de texto lineales. En SIGIR 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la conciliación de evaluaciones de probabilidad. Revista de la Real Sociedad Estadística, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelando las distribuciones de puntuaciones para combinar las salidas de los motores de búsqueda. En SIGIR 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En AAAI 98, Taller sobre Aprendizaje para la Categorización de Textos, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de verosimilitud regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, Avances en Clasificadores de Márgenes Amplios. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación de probabilidades y clasificación en clase. En IJCAI 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de los evaluadores de probabilidad. Revista de la Asociación Estadística Americana, 1969. [25] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de texto. En SIGIR 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas a partir de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducir la clasificación multiclase a binaria mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "classifier combination": {
            "translated_key": "combinación de clasificadores",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates Paul N. Bennett Computer Science Dept.",
                "Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu ABSTRACT Text classifiers that give probability estimates are more readily applicable in a variety of scenarios.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a userspecified cost function dynamically chosen at prediction time.",
                "However, the quality of the probability estimates is crucial.",
                "We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different.",
                "Finally, we analyze the experimental performance of these models over the outputs of two text classifiers.",
                "The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 [Pattern Recognition]: Design Methodology General Terms Algorithms, Experimentation, Reliability. 1.",
                "INTRODUCTION Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking.",
                "For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user-specified cost function dynamically chosen at prediction time.",
                "This can be used to minimize a linear utility cost function for filtering tasks where pre-specified costs of relevant/irrelevant are not available during training but are specified at prediction time.",
                "Furthermore, the costs can be changed without retraining the model.",
                "Additionally, probability estimates are often used as the basis of deciding which documents label to request next during active learning [17, 23].",
                "Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly - severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17].",
                "Finally, they are also amenable to making other types of cost-sensitive decisions [26] and for combining decisions [3].",
                "However, in all of these tasks, the quality of the probability estimates is crucial.",
                "Parametric models generally use assumptions that the data conform to the model to trade-off flexibility with the ability to estimate the model parameters accurately with little training data.",
                "Since many text classification tasks often have very little training data, we focus on parametric methods.",
                "However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable.",
                "While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes.",
                "We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models.",
                "Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior - one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items.",
                "We first review related work on improving probability estimates and score modeling in information retrieval.",
                "Then, we discuss in further detail the need for asymmetric models.",
                "After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores.",
                "We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods.",
                "Finally, we summarize our contributions and discuss future directions. 2.",
                "RELATED WORK Parametric models have been employed to obtain probability estimates in several areas of information retrieval.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning.",
                "Manmatha et. al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines.",
                "They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two-sided asymmetric distributions for a single class as described here.",
                "They also survey the long history of modeling the relevance scores of search engines.",
                "Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data.",
                "Focus on improving probability estimates has been growing lately.",
                "Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non-parametric method for recalibrating na¨ıve Bayes.",
                "In more recent work [27], they investigate using a semi-parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM.",
                "While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results.",
                "Our work provides asymmetric parametric methods which complement the non-parametric and semi-parametric methods they propose when data scarcity is an issue.",
                "In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions.",
                "There is a variety of other work that this paper extends.",
                "Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM.",
                "His work showed that this post-processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better).",
                "Finally, Bennett [1] obtained moderate gains by applying Platts method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs.",
                "Recalibrating poorly calibrated classifiers is not a new problem.",
                "Lindley et. al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. 3.",
                "PROBLEM DEFINITION & APPROACH Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines. 3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.",
                "A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).",
                "We assume throughout there are only two classes: the positive and the negative (or irrelevant) class (+ and - respectively).",
                "There are two general types of parametric approaches.",
                "The first of these tries to fit the posterior function directly, i.e. there is one p(s|+) p(s|−) Bayes RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey.",
                "The internals are for one type of approach. function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).",
                "The second type of approach breaks the problem down as shown in the grey box of Figure 1.",
                "An estimator for each of the class-conditional densities (i.e. p(s|+) and p(s|−)) is produced, then Bayes rule and the class priors are used to obtain the estimate for P(+|s(d)). 3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.",
                "We can visualize this as depicted in Figure 2.",
                "Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).",
                "A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.",
                "Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).",
                "Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.",
                "This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).",
                "As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.",
                "Ideally (i.e. perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.",
                "Furthermore, no examples fall between θ− and θ+.",
                "The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.",
                "Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.",
                "This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.",
                "Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.",
                "Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples).",
                "Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.",
                "A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g. the Laplace or the Gaussian.",
                "An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters. θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.",
                "We will use the notation Λ(X | θ, β, γ) to refer to this distribution. 0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.",
                "A Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.",
                "We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.",
                "To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).",
                "While these distributions are composed of halves, the resulting function is a single continuous distribution.",
                "These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.",
                "We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).",
                "In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.",
                "Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).",
                "To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval.",
                "Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task. 3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates (MLE) of the parameters for the above asymmetric distributions.",
                "In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.",
                "Because of the simplicity of analysis in the latter alternative, we choose this method. 3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).",
                "Now, we fix θ and compute the maximum likelihood for that choice of θ.",
                "Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.",
                "The complete derivation is omitted because of space but is available in [2].",
                "We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.",
                "Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ.",
                "Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl . (3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.",
                "The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e. the closer Dl and Dr are to being equal, the closer the resulting inverse scales).",
                "By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.",
                "Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e. almost all mass at θ for that half).",
                "Dr = 0 is handled similarly.",
                "Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable.",
                "Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.",
                "In addition, if the scores are sorted, then we can perform the whole process quite efficiently.",
                "Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.",
                "Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.",
                "Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time). 3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).",
                "The MLEs can be worked out similar to the above.",
                "We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.",
                "The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N . (5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.",
                "Dr2 = 0), we assign σl = 0 (resp. σr = 0).",
                "Again, the same computational complexity analysis applies to estimating these parameters. 4.",
                "EXPERIMENTAL ANALYSIS 4.1 Methods For each of the methods that use a class prior, we use a smoothed add-one estimate, i.e.",
                "P(c) = |c|+1 N+2 where N is the number of documents.",
                "For methods that fit the class-conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes rule as described above.",
                "All of the methods below are fit using maximum likelihood estimates.",
                "For recalibrating a classifier (i.e. correcting poor probability estimates output by the classifier), it is usual to use the log-odds of the classifiers estimate as s(d).",
                "The log-odds are defined to be log P (+|d) P (−|d) .",
                "The normal decision threshold (minimizing error) in terms of log-odds is at zero (i.e.",
                "P(+|d) = P(−|d) = 0.5).",
                "Since it scales the outputs to a space [−∞, ∞], the log-odds make normal (and similar distributions) applicable [19].",
                "Lewis & Gale [17] give a more motivating viewpoint that fitting the log-odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors.",
                "In general, fitting the log-odds can serve to boost or dampen the signal from the original classifier as the data dictate.",
                "Gaussians A Gaussian is fit to each of the class-conditional densities, using the usual maximum likelihood estimates.",
                "This method is denoted in the tables below as Gauss.",
                "Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e. 8 points between actual scores occurring in the data set are tested.",
                "This method is denoted as A. Gauss.",
                "Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form.",
                "The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14].",
                "We denote this method as Laplace below.",
                "Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class-conditional densities using the maximum likelihood estimation procedure described above.",
                "As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs.",
                "This method is denoted as A. Laplace below.",
                "Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)).",
                "Both methods restrict the set of families to a two-parameter sigmoid family; they differ primarily in their model of class labels.",
                "As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier.",
                "When this is desired, these methods may be more appropriate.",
                "The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it.",
                "Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier.",
                "Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning.",
                "The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) . (6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d).",
                "Instead of using this below, we will use the logodds ratio.",
                "This does not affect the model as it simply shifts all of the scores by a constant determined by the priors.",
                "We refer to this method as LogReg below.",
                "Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM.",
                "This model differs from the LogReg model only in how the parameters are estimated.",
                "The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.",
                "The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes rule to infer the probability of incorrect label).",
                "Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness.",
                "We refer to this method below as LR+Noise. 4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC-AP.",
                "MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified.",
                "We used the same train/test split of 50078/10024 documents as that reported in [9].",
                "The MSN Web hierarchy is a seven-level hierarchy; we used all 13 of the top-level categories.",
                "The class proportions in the training set vary from 1.15% to 22.29%.",
                "In the testing set, they range from 1.14% to 21.54%.",
                "The classes are general subjects such as Health & Fitness and Travel & Vacation.",
                "Human indexers assigned the documents to zero or more categories.",
                "For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents.",
                "Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987.",
                "For this data set, we used the ModApte standard train/ test split of 9603/3299 documents (8676 unused documents).",
                "The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects.",
                "There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22].",
                "The class proportions in the training set vary from 1.88% to 29.96%.",
                "In the testing set, they range from 1.7% to 32.95%.",
                "For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents.",
                "TREC-AP The TREC-AP corpus is a collection of AP news stories from 1988 to 1990.",
                "We used the same train/test split of 142791/66992 documents that was used in [18].",
                "As described in [17] (see also [15]), the categories are defined by keywords in a keyword field.",
                "The title and body fields are used in the experiments below.",
                "There are twenty categories in total.",
                "The class proportions in the training set vary from 0.06% to 2.03%.",
                "In the testing set, they range from 0.03% to 4.32%.",
                "For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents. 4.3 Classifiers We selected two classifiers for evaluation.",
                "A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27]. 1 A separate comparison of only LogReg, LR+Noise, and A. Laplace over all 90 categories of Reuters was also conducted.",
                "After accounting for the variance, that evaluation also supported the claims made here.",
                "SVM For linear SVMs, we use the Smox toolkit which is based on Platts Sequential Minimal Optimization algorithm.",
                "The features were represented as continuous values.",
                "We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22].",
                "The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero.",
                "Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21].",
                "We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.",
                "We use the log-odds estimated by the classifier as s(d).",
                "The normal decision threshold is at zero. 4.4 Performance Measures We use log-loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates.",
                "For a document d with class c(d) ∈ {+, −} (i.e. the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) . = 1 if a = b and 0 otherwise.",
                "The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .",
                "When the class of a document is correctly predicted with a probability of one, log-loss is zero and squared error is zero.",
                "When the class of a document is incorrectly predicted with a probability of one, log-loss is −∞ and squared error is one.",
                "Thus, both measures assess how close an estimate comes to correctly predicting the items class but vary in how harshly incorrect predictions are penalized.",
                "We report only the sum of these measures and omit the averages for space.",
                "Their averages, average log-loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus.",
                "In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.",
                "This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5.",
                "Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates.",
                "It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods.",
                "We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures.",
                "Only pairs that the methods disagree on are used in the sign test.",
                "This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed.",
                "We use a significance level of p = 0.01. 4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes.",
                "In order to generate the scores that each method uses to fit its probability estimates, we use five-fold cross-validation on the training data.",
                "We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result.",
                "Of course, as with any application of n-fold cross-validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier. 4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a.",
                "Table 1b gives results for producing probabilistic outputs for SVMs.",
                "Log-loss Error2 Errors MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84† 8350 LogReg -36470.99 6525.47 8540 LR+Noise -36468.18 6534.61 8563 na¨ıve Bayes -1098900.83 17117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.Gauss -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95‡ 554.37‡ 726 LogReg -3375.63 603.20 786 LR+Noise -3374.15 604.80 785 na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 A.Gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 A.Laplace -48711.55 7251.87‡ 8642 LogReg -48250.81 7540.60 8797 LR+Noise -48251.51 7544.84 8801 na¨ıve Bayes -1903487.10 41770.21 43661 Log-loss Error2 Errors MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 LogReg -30209.36 5158.74 6480 LR+Noise -30294.01 5209.80 6551 Linear SVM N/A N/A 6602 Reuters Gauss -3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 LogReg -2575.85 407.48 509 LR+Noise -2567.68 408.82 516 Linear SVM N/A N/A 516 TREC-AP Gauss -54620.94 6525.71 7321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25‡ 6572‡ LogReg -48285.56 5914.04 6791 LR+Noise -48214.96 5919.25 6794 Linear SVM N/A N/A 6718 Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right).",
                "The best entry for a corpus is in bold.",
                "Entries that are statistically significantly better than all other entries are underlined.",
                "A † denotes the method is significantly better than all other methods except for na¨ıve Bayes.",
                "A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table on the left).",
                "The reason for this distinction in significance tests is described in the text.",
                "We start with general observations that result from examining the performance of these methods over the various corpora.",
                "The first is that A. Laplace, LR+Noise, and LogReg, quite clearly outperform the other methods.",
                "There is usually little difference between the performance of LR+Noise and LogReg (both as shown here and on a decision by decision basis), but this is unsurprising since LR+Noise just adds noisy class labels to the LogReg model.",
                "With respect to the three different measures, LR+Noise and LogReg tend to perform slightly better (but never significantly) than A. Laplace at some tasks with respect to log-loss and squared error.",
                "However, A. Laplace always produces the least number of errors for all of the tasks, though at times the degree of improvement is not significant.",
                "In order to give the reader a better sense of the behavior of these methods, Figures 4-5 show the fits produced by the most competitive of these methods versus the actual data behavior (as estimated nonparametrically by binning) for class Earn in Reuters.",
                "Figure 4 shows the class-conditional densities, and thus only A. Laplace is shown since LogReg fits the posterior directly.",
                "Figure 5 shows the estimations of the log-odds, (i.e. log P (Earn|s(d)) P (¬Earn|s(d)) ).",
                "Viewing the log-odds (rather than the posterior) usually enables errors in estimation to be detected by the eye more easily.",
                "We can break things down as the sign test does and just look at wins and losses on the items that the methods disagree on.",
                "Looked at in this way only two methods (na¨ıve Bayes and A. Gauss) ever have more pairwise wins than A. Laplace; those two sometimes have more pairwise wins on log-loss and squared error even though the total never wins (i.e. they are dragged down by heavy penalties).",
                "In addition, this comparison of pairwise wins means that for those cases where LogReg and LR+Noise have better scores than A. Laplace, it would not be deemed significant by the sign test at any level since they do not have more wins.",
                "For example, of the 130K binary decisions over the MSN Web dataset, A. Laplace had approximately 101K pairwise wins versus LogReg and LR+Noise.",
                "No method ever has more pairwise wins than A. Laplace for the error comparison nor does any method every achieve a better total.",
                "The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].",
                "This means if it tends to be right enough of the time, it will produce results that do not appear significant in a sign test that ignores size of difference (as the one here).",
                "The totals of the squared error and log-loss bear out the previous observation that when its wrong its really wrong.",
                "There are several interesting points about the performance of the asymmetric distributions as well.",
                "First, A. Gauss performs poorly because (similar to na¨ıve Bayes) there are some examples where it is penalized a large amount.",
                "This behavior results from a general tendency to perform like the picture shown in Figure 3 (note the crossover at the tails).",
                "While the asymmetric Gaussian tends to place the mode much more accurately than a symmetric Gaussian, its asymmetric flexibility combined with its distance function causes it to distribute too much mass to the outside tails while failing to fit around the mode accurately enough to compensate.",
                "Figure 3 is actually a result of fitting the two distributions to real data.",
                "As a result, at the tails there can be a large discrepancy between the likelihood of belonging to each class.",
                "Thus when there are no outliers A. Gauss can perform quite competitively, but when there is an 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 p(s(d)|Class={+,-}) s(d) = naive Bayes log-odds Train Test A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 p(s(d)|Class={+,-}) s(d) = linear SVM raw score Train Test A.Laplace Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.",
                "Also shown is the fit of the asymmetric Laplace distribution to the training score distribution.",
                "The positive class (i.e.",
                "Earn) is the distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph. -6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = naive Bayes log-odds Train Test A.Laplace LogReg -5 0 5 10 15 -4 -2 0 2 4 6 LogOdds=logP(+|s(d))-logP(-|s(d)) s(d) = linear SVM raw score Train Test A.Laplace LogReg Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters. outlier A. Gauss is penalized quite heavily.",
                "There are enough such cases overall that it seems clearly inferior to the top three methods.",
                "However, the asymmetric Laplace places much more emphasis around the mode (Figure 4) because of the different distance function (think of the sharp peak of an exponential).",
                "As a result most of the mass stays centered around the mode, while the asymmetric parameters still allow more flexibility than the standard Laplace.",
                "Since the standard Laplace also corresponds to a piecewise fit in the log-odds space, this highlights that part of the power of the asymmetric methods is their sensitivity in placing the knots at the actual modes - rather than the symmetric assumption that the means correspond to the modes.",
                "Additionally, the asymmetric methods have greater flexibility in fitting the slopes of the line segments as well.",
                "Even in cases where the test distribution differs from the training distribution (Figure 4), A. Laplace still yields a solution that gives a better fit than LogReg (Figure 5), the next best competitor.",
                "Finally, we can make a few observations about the usefulness of the various performance metrics.",
                "First, log-loss only awards a finite amount of credit as the degree to which something is correct improves (i.e. there are diminishing returns as it approaches zero), but it can infinitely penalize for a wrong estimate.",
                "Thus, it is possible for one outlier to skew the totals, but misclassifying this example may not matter for any but a handful of actual utility functions used in practice.",
                "Secondly, squared error has a weakness in the other direction.",
                "That is, its penalty and reward are bounded in [0, 1], but if the number of errors is small enough, it is possible for a method to appear better when it is producing what we generally consider unhelpful probability estimates.",
                "For example, consider a method that only estimates probabilities as zero or one (which na¨ıve Bayes tends to but doesnt quite reach if you use smoothing).",
                "This method could win according to squared error, but with just one error it would never perform better on log-loss than any method that assigns some non-zero probability to each outcome.",
                "For these reasons, we recommend that neither of these are used in isolation as they each give slightly different insights to the quality of the estimates produced.",
                "These observations are straightforward from the definitions but are underscored by the evaluation. 5.",
                "FUTURE WORK A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes).",
                "From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails.",
                "Just as logistic regression allows the log-odds of the posterior distribution to be fit directly with a line, we could directly fit the log-odds of the posterior with a three-piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace.",
                "This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class-conditional densities are from an asymmetric Laplace.",
                "Finally, extending these methods to the outputs of other discriminative classifiers is an open area.",
                "We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11].",
                "By analogy to the log-odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . 6.",
                "SUMMARY AND CONCLUSIONS We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier.",
                "In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function.",
                "We have given an efficient way to estimate the parameters of these distributions.",
                "While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes.",
                "In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression.",
                "Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates.",
                "Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox.",
                "Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work.",
                "Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper. 7.",
                "REFERENCES [1] P. N. Bennett.",
                "Assessing the calibration of naive bayes posterior estimates.",
                "Technical Report CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett.",
                "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods.",
                "Technical Report CMU-CS-02-126, Carnegie Mellon, School of Computer Science, 2002. [3] H. Bourlard and N. Morgan.",
                "A continuous speech recognition system embedding mlp into hmm.",
                "In NIPS 89, 1989. [4] G. Brier.",
                "Verification of forecasts expressed in terms of probability.",
                "Monthly Weather Review, 78:1-3, 1950. [5] M. H. DeGroot and S. E. Fienberg.",
                "The comparison and evaluation of forecasters.",
                "Statistician, 32:12-22, 1983. [6] M. H. DeGroot and S. E. Fienberg.",
                "Comparing probability forecasters: Basic binary concepts and multivariate extensions.",
                "In P. Goel and A. Zellner, editors, Bayesian Inference and Decision Techniques.",
                "Elsevier Science Publishers B.V., 1986. [7] P. Domingos and M. Pazzani.",
                "Beyond independence: Conditions for the optimality of the simple bayesian classifier.",
                "In ICML 96, 1996. [8] R. Duda, P. Hart, and D. Stork.",
                "Pattern Classification.",
                "John Wiley & Sons, Inc., 2001. [9] S. T. Dumais and H. Chen.",
                "Hierarchical classification of web content.",
                "In SIGIR 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.",
                "Inductive learning algorithms and representations for text categorization.",
                "In CIKM 98, 1998. [11] Y. Freund and R. Schapire.",
                "Large margin classification using the perceptron algorithm.",
                "Machine Learning, 37(3):277-296, 1999. [12] I.",
                "Good.",
                "Rational decisions.",
                "Journal of the Royal Statistical Society, Series B, 1952. [13] T. Joachims.",
                "Text categorization with support vector machines: Learning with many relevant features.",
                "In ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski, and K. Podgorski.",
                "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance.",
                "Birkh¨auser, 2001. [15] D. D. Lewis.",
                "A sequential algorithm for training text classifiers: Corrigendum and additional data.",
                "SIGIR Forum, 29(2):13-19, Fall 1995. [16] D. D. Lewis.",
                "Reuters-21578, distribution 1.0. http://www.daviddlewis.com/resources/ testcollections/reuters21578, January 1997. [17] D. D. Lewis and W. A. Gale.",
                "A sequential algorithm for training text classifiers.",
                "In SIGIR 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.",
                "Training algorithms for linear text classifiers.",
                "In SIGIR 96, 1996. [19] D. Lindley, A. Tversky, and R. Brown.",
                "On the reconciliation of probability assessments.",
                "Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath, and F. Feng.",
                "Modeling score distributions for combining the outputs of search engines.",
                "In SIGIR 01, 2001. [21] A. McCallum and K. Nigam.",
                "A comparison of event models for naive bayes text classification.",
                "In AAAI 98, Workshop on Learning for Text Categorization, 1998. [22] J. C. Platt.",
                "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.",
                "In A. J. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers.",
                "MIT Press, 1999. [23] M. Saar-Tsechansky and F. Provost.",
                "Active learning for class probability estimation and ranking.",
                "In IJCAI 01, 2001. [24] R. L. Winkler.",
                "Scoring rules and the evaluation of probability assessors.",
                "Journal of the American Statistical Association, 1969. [25] Y. Yang and X. Liu.",
                "A re-examination of text categorization methods.",
                "In SIGIR 99, 1999. [26] B. Zadrozny and C. Elkan.",
                "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers.",
                "In ICML 01, 2001. [27] B. Zadrozny and C. Elkan.",
                "Reducing multiclass to binary by coupling probability estimates.",
                "In KDD 02, 2002."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}