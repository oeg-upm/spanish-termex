{
    "id": "H-50",
    "original_text": "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.). In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking. In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another. We show that the proposed method deals well with the Information Retrieval distinctive features. Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators. Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models. General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1. INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request. The result lists produced by these approaches depend on the exact definition of the relevance concept. Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking. Such approaches give rise to metasearch engines in the Web context. We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores. This corresponds indeed to the reality, where only ordinal information is available. Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16]. Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods. For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant. Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents. Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances. These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22]. Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1]. The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23]. Most current rank aggregation methods consider each input ranking as a permutation over the same set of items. They also give rigid interpretation to the exact ranking of the items. Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections. The remaining of the paper is organized as follows. We first review current rank aggregation methods in Section 2. Then we outline the specificities of the data fusion problem in the IR context (Section 3). In Section 4, we present a new aggregation method which is proven to best fit the IR context. Experimental results are presented in Section 5 and conclusions are provided in a final section. 2. RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked. These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way. Let D = {d1, d2, . . . , dnd } be a set of nd documents. A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n). Thus, di j di means di is ranked better than di in j. When Dj = D, j is said to be a full list. Otherwise, it is a partial list. If di belongs to Dj, rj i denotes the rank or position of di in j. We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|. Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n). Restricting PR to the rankings containing document di defines PRi. We also call the number of rankings which contain document di the rank hits of di [19]. The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di. Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents. When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28]. For instance, Callan et al. [6] used the inference networks model [30] to combine rankings. Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ. The first three operators correspond to the sum, min and max operators, respectively. CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits. It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others. Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21]. Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |. It extends to several lists as follows. Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j). Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i . This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance. During the training process, probabilities of relevance are calculated. For subsequent queries, documents are ranked based on these probabilities. For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j). For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k . Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores. Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest. Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|. The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank. Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance. Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists. It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event. In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di. The consensus ranking corresponds to the stationary distribution of MC4. 3. SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized. For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value. Indeed, in the IR context, the complete order provided by an input method may hide ties. In this case, we call such rankings semi orders. This was outlined in [13] as the problem of aggregation with ties. It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance. Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists. This was outlined in [14] as the problem of having to merge top-k results from various input lists. For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines. Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1. Partial lists can have various lengths, which can favour long lists. We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking. H1 all: We consider all the documents from each input ranking. 2. Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking. Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1). H2 all: We consider all the documents which are ranked in at least one input ranking. Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents. We also call a candidate document which is missing in one or more rankings, a missing document. 3. Some candidate documents are missing documents in some input rankings. Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available. We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position. H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4. When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking. Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking. H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones. In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4. OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information. This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths. Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results. For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents. Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking. Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings. Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties. Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information. Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion. Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle). Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents. The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi . Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined. They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition. A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv. This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n). It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29]. Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist. Therefore, we need specific procedures in order to derive a consensus ranking. We propose the following procedure which finds its roots in [27]. It consists in partitioning the set of documents into r ranked classes. Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed. Documents within the same equivalence class are ranked arbitrarily. Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk . Each class Ch results from a distillation process. It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1. When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1. Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}. The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4). Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively. The following tables give the concordance, discordance and outranking matrices. Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|. Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅. Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds. Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix. The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 . They are respectively 2, 2, 2, -2 and -4. Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}. Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated. At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}. They are respectively 1 and -1. So C3 = E1 = {d4}. The last document d5 is the only document of the last class C3. Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation. In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10]. In this task, there are 75 topics where only a short description of each is given. For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams. The performances of these runs are reported in table 3. Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents. The number of documents retrieved by all these runs ranges from 543 to 5769. Their average (median) number is 3340 (3386). It is worth noting that we found similar distributions of the documents among the rankings as in [11]. For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10. Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms. In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs. In the tables of the following section, statistically significant differences are marked with an asterisk. Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings. We set our basic run mcm with the following parameters. We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%). Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking. They consequently may vary from one ranking to another. In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0). Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi . Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0). To test the run mcm, we had chosen the following assumptions. We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new. In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second. Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters. This was validated by preliminary experiments. Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered. Afterwards, we study the impact of tuning parameters. Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses. In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm. Moreover, S@1 moves from 41.33% to 34.67% (-16.11%). This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change. We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings. Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance. Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar. Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones. From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25. Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances. This result was predictable since in both cases we have more detailed information on the relative importance of documents. Tables 5 and 6 confirm this evidence. Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase. It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance. Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k . Values between brackets are rank hits. For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful. This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded. Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models. Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered. We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%. Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%. We can thus conclude that the input rankings are semi orders rather than complete orders. Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold. We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant. Performance drops significantly for very low and very high values of the concordance threshold. In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively. Therefore, the outranking relation becomes either too weak or too strong respectively. Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures. In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily. Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold. For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered. Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3]. Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking. Therefore, when they are considered, performance decreases. Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies. We also examined the performance of one majoritarian method which is the Markov chain method (MC4). For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters. The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents. For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no. The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000. The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking. The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents. The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered. Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track. This set of runs aims to show whether relative performance of the various methods is task-dependent. The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row. This aims to show whether relative performance of the various methods changes from year to year. Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach. Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average). This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance. For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%. This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150. This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all. This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ. It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6. CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused. We noticed that the input rankings can hide ties, so they should not be considered as complete orders. Only robust information should be used from each input ranking. Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings. They should be adapted by considering specific working assumptions. We propose a new outranking method for rank aggregation which is well adapted to the IR context. Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document. There is also no need to make specific assumptions on the positions of the missing documents. This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively. Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies. It also out-performs a good performing majoritarian methods which is the Markov chain method. These results are tested against different test collections and queries. From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods. The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list. Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores. Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7. REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur. Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents. In Proceedings TREC2005. NIST Publication, 2005. [2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval. ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In Proceedings ACM-SIGIR94, pages 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. Combining evidence of multiple query representations for information retrieval. IPM, 31(3):431-448, 1995. [5] J. Borda. M´emoire sur les ´elections au scrutin. Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet. Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix. Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress. Ordinal ranking with intensity of preference. Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking. Overview of the TREC-2002 Web Track. In Proceedings TREC2002. NIST Publication, 2002. [10] N. Craswell and D. Hawking. Overview of the TREC-2004 Web Track. In Proceedings of TREC2004. NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the Web. In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin. Combining fuzzy information from multiple systems. JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee. Comparing and aggregating rankings with ties. In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing top k lists. SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proceedings of TREC3. NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta. A study of the overlap among document representations. Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan. Collection selection and results merging with topically organized U.S. patents and TREC data. In Proceedings ACM-CIKM2000, pages 282-289. ACM Press, 2000. [18] A. Le Calv´e and J. Savoy. Database merging strategy based on logistic regression. IPM, 36(3):341-359, 2000. [19] J. H. Lee. Analyses of multiple evidence combination. In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In Proceedings ACM-SIGIR2006, pages 139-146. ACM Press, 2006. [21] J. I. Marden. Analyzing and Modeling Rank Data. Number 64 in Monographs on Statistics and Applied Probability. Chapman & Hall, 1995. [22] M. Montague and J. A. Aslam. Metasearch consistency. In Proceedings ACM-SIGIR2001, pages 386-387. ACM Press, 2001. [23] D. M. Pennock and E. Horvitz. Analysis of the axiomatic foundations of collaborative filtering. In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia. Web metasearch: rank vs. score based rank aggregation methods. In Proceedings ACM-SAC2003, pages 841-846. ACM Press, 2003. [25] W. H. Riker. Liberalism against populism. Waveland Press, 1982. [26] B. Roy. The outranking approach and the foundations of ELECTRE methods. Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard. Ranking of suburban line extension projects on the Paris metro system by a multicriteria method. Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan. Using sampled data and regression to merge search engine results. In Proceedings ACM-SIGIR2002, pages 19-26. ACM Press, 2002. [29] M. Truchon. An extension of the Condorcet criterion and Kemeny orders. Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft. Inference networks for document retrieval. In Proceedings of ACM-SIGIR90, pages 1-24. ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell. Fusion via a linear combination of scores. Information Retrieval, 1(3):151-173, 1999.",
    "original_translation": "Un enfoque de clasificación para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1]. El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23]. La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida al ranking exacto de los elementos. Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones. El resto del documento está organizado de la siguiente manera. Primero revisamos los métodos actuales de agregación de rangos en la Sección 2. Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR. Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2. TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar. Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme. Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos. Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n). Por lo tanto, di j di significa que di está clasificado mejor que di en j. Cuando Dj = D, se dice que j es una lista completa. De lo contrario, es una lista parcial. Si di pertenece a Dj, rj i denota la clasificación o posición de di en j. Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n). Restringir PR a los rankings que contienen el documento di define PRi. También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19]. El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di. Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos. Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28]. Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones. Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ. Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente. CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango. Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás. Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j). Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i . Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican según estas probabilidades. Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j). Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k. Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes. Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares. Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman. Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. 3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas. Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates. En este caso, llamamos a tales clasificaciones semiórdenes. Esto fue descrito en [13] como el problema de la agregación con empates. Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares. Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales. Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada. Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1. Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada. Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2. Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1). Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada. De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos. También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3. Algunos documentos candidatos faltan en algunas clasificaciones de entrada. Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición. H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4. Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso. En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos. En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4. Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal. Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes. Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso. Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates. Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio. Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías). Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos. La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones de prelación. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia. Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv. Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n). También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29]. Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en r clases clasificadas. Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria. Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk. Cada clase Ch resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \\ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1. Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1. Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4). Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente. Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia. Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|. Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅. Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple. Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}. Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \\ C1 = {d4, d5}. Son respectivamente 1 y -1. Entonces C3 = E1 = {d4}. El último documento d5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos. En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10]. En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004. Las actuaciones de estas carreras se informan en la tabla 3. Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediana) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10. Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos. En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del enfoque de clasificación cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del enfoque de clasificación con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada. Configuramos nuestro módulo de ejecución básico con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%). Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada. Por lo tanto, pueden variar de un ranking a otro. Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi. Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0). Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones. Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones. Después, estudiamos el impacto de ajustar los parámetros. Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del enfoque de clasificación por jerarquías bajo diferentes hipótesis de trabajo. En la Tabla 4: Impacto de las suposiciones de trabajo, se muestra que en la ejecución mcm22, en la que los documentos faltantes se colocan todos en la misma última posición de cada clasificación de entrada, se produce una disminución en el rendimiento con respecto a la ejecución mcm. Además, S@1 pasa de 41.33% a 34.67% (-16.11%). Esto muestra que varios documentos relevantes que inicialmente se ubicaron en la primera posición del ranking de consenso en mcm, pierden esta primera posición pero siguen clasificados en los 5 primeros documentos, ya que S@5 no cambió. También concluimos que los documentos que tienen posiciones bastante buenas en algunas clasificaciones de entrada son más propensos a ser relevantes, aunque falten en otras clasificaciones. Por consiguiente, cuando faltan en ciertas clasificaciones, asignarles rangos más bajos a estos documentos es perjudicial para el rendimiento. Además, a partir de la Tabla 4, encontramos que las actuaciones de las ejecuciones mcm y mcm23 son similares. Por lo tanto, el enfoque de clasificación no está sujeto a mantener las posiciones iniciales de los documentos candidatos o a recalcularlas descartando los excluidos. De la misma Tabla 4, el rendimiento del enfoque de clasificación aumenta significativamente para las ejecuciones mcm24 y mcm25. Por lo tanto, ya sea que consideremos todos los documentos presentes en la mitad de las clasificaciones (mcm24) o consideremos todos los documentos clasificados en las primeras 100 posiciones en una o más clasificaciones (mcm25), se incrementan los rendimientos. Este resultado era predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos. Las tablas 5 y 6 confirman esta evidencia. La Tabla 5, donde los valores entre corchetes de la primera columna indican el número de documentos que se retienen de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce a un aumento en el rendimiento. Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento. Tabla 5: Impacto del número de documentos retenidos. Identificador de ejecución MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% La Tabla 6 informa ejecuciones correspondientes a variaciones de H2 k. Los valores entre corchetes son éxitos de rango. Por ejemplo, en la ejecución mcm32, solo se consideraron exitosos los documentos que estaban presentes en 3 o más clasificaciones de entrada. Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos son descartados. Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos de RI. Tabla 6: Rendimiento considerando diferentes éxitos de rango. Identificación de ejecución MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% Para las ejecuciones mcm24 y mcm25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo por consulta aumentó y se situó en alrededor de 5 segundos. 5.2.2 Impacto de la Variación de los Parámetros. La Tabla 7 muestra la variación de rendimiento del enfoque de clasificación por preferencias cuando se consideran diferentes umbrales de preferencia. Encontramos una mejora en el rendimiento hasta valores de umbral de aproximadamente el 5%, luego hay una disminución en el rendimiento que se vuelve significativa para valores de umbral superiores al 10%. Además, S@1 mejora del 41.33% al 46.67% cuando el umbral de preferencia cambia de 0 a 5%. Por lo tanto, podemos concluir que las clasificaciones de entrada son semiordeles en lugar de órdenes completos. La Tabla 8 muestra la evolución de las medidas de rendimiento con respecto al umbral de concordancia. Podemos concluir que para colocar el documento di antes de di en la clasificación de consenso, en la Tabla 7: Impacto de la variación del umbral de preferencia del 0 al 12.5%. Ejecutar Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% al menos la mitad de las clasificaciones de entrada de PRi ∩ PRi deben ser concordantes. El rendimiento disminuye significativamente para valores muy bajos y muy altos del umbral de concordancia. De hecho, para tales valores, la condición de concordancia se cumple o bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente. Por lo tanto, la relación de clasificación se vuelve demasiado débil o demasiado fuerte respectivamente. Tabla 8: Impacto de la variación de cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% En los experimentos, variar el umbral de veto, así como el umbral de discordancia dentro de intervalos razonables, no tiene un impacto significativo en las medidas de rendimiento. De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron un rendimiento similar, aunque hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada veten fácilmente. Además, el ajuste del umbral de discordancia se realizó para valores del 50% y 75% del umbral de veto. Para estas ejecuciones no observamos ninguna variación de rendimiento notable, aunque para umbrales de discordancia bajos (dmax < 20%), el rendimiento disminuyó ligeramente. 5.2.3 Impacto de la Variación del Número de Clasificaciones de Entrada Para estudiar la evolución del rendimiento cuando se consideran diferentes conjuntos de clasificaciones de entrada, realizamos tres ejecuciones adicionales donde se consideran 2, 4 y 6 de los conjuntos de clasificaciones de entrada con mejor rendimiento. Los resultados reportados en la Tabla 9 parecen ser contraintuitivos y tampoco respaldan hallazgos previos en la investigación sobre la agregación de rangos [3]. Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento aportan más ruido que información para establecer la clasificación de consenso. Por lo tanto, cuando se consideran, el rendimiento disminuye. Tabla 9: Rendimiento considerando diferentes conjuntos de clasificaciones de entrada con mejor rendimiento. Identificador de ejecución MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de clasificaciones. En este conjunto de ejecuciones, comparamos el enfoque de clasificación con algunos métodos de agregación de clasificaciones estándar que han demostrado tener un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias CombSUM y CombMNZ. También examinamos el rendimiento de un método mayoritario que es el método de la cadena de Markov (MC4). Para las comparaciones, consideramos una relación de superación específica S∗ = S(5%,50%,50%,30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros. La primera fila de la Tabla 10 muestra el rendimiento de los métodos de agregación de rangos con respecto a un conjunto de suposiciones básicas A1 = (H1 100, H2 5, H4 nuevo): solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos los documentos presentes en 5 o más clasificaciones y actualizamos los rangos de los documentos exitosos. Para los métodos posicionales, colocamos los documentos faltantes en la cola de la clasificación (H3 sí), mientras que para nuestro método, al igual que para MC4, conservamos la hipótesis H3 no. Las tres filas siguientes de la Tabla 10 informan sobre los rendimientos al cambiar un elemento del conjunto de suposiciones básicas: la segunda fila corresponde al conjunto de suposiciones A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a 1000. La tercera fila corresponde al conjunto de supuestos A3 = (H1 100, H2 todos, H4 nuevos), es decir, considerando los documentos presentes en al menos un ranking. La cuarta fila corresponde al conjunto de supuestos A4 = (H1 100, H2 5, H4 init), es decir, manteniendo los rangos originales de los documentos exitosos. La quinta fila de la Tabla 10, etiquetada como A5, muestra el rendimiento cuando se consideran todas las 225 consultas de la pista web de TREC-2004. Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de Página de Inicio y Página Nombrada [10]) de la pista web TREC-2004. Este conjunto de pruebas tiene como objetivo demostrar si el rendimiento relativo de los diferentes métodos depende de la tarea. La última fila de la Tabla 10, etiquetada como A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de suposiciones A1 de la primera fila. Esto tiene como objetivo mostrar si el rendimiento relativo de los diferentes métodos cambia de un año a otro. Los valores entre corchetes de la Tabla 10 son variaciones del rendimiento de cada método de agregación de rangos con respecto al rendimiento del enfoque de superación. Tabla 10: Rendimiento (MAP) de diferentes métodos de agregación de rangos bajo 3 colecciones de pruebas diferentes mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) Del análisis de la tabla 10 se puede establecer lo siguiente: • para todas las ejecuciones, considerar todos los documentos en cada clasificación de entrada (A2) mejora significativamente el rendimiento (MAP aumenta en promedio un 11.62%). Esto es predecible ya que algunos documentos relevantes inicialmente no reportados recibirían mejores posiciones en la clasificación de consenso. • para todas las ejecuciones, considerar documentos incluso aquellos presentes en solo una clasificación de entrada (A3) mejora significativamente el rendimiento. Para mcm, combSUM y combMNZ, la mejora del rendimiento es más importante (el MAP aumenta en promedio un 20.27%) que para la ejecución de Markov (el MAP aumenta un 3.86%). • preservar las posiciones iniciales de los documentos (A4) o volver a calcularlas (A1) no tiene una influencia notable en el rendimiento para ambos métodos posicional y mayoritario. • considerar todas las consultas de la pista web de TREC2004 (A5) así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos. • considerando las consultas TD de la pista web de TREC2002, los rendimientos de todos los métodos de fusión de datos son más bajos que el del mejor ranking de entrada que tiene un valor de MAP de 18.58%. Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con la mejor, lo que añade más ruido a la clasificación de consenso. • los rendimientos de los métodos de fusión de datos mcm y markov son significativamente mejores que el de la mejor clasificación de entrada uogWebCAU150. Esto sigue siendo cierto solo para las ejecuciones combSUM y combMNZ bajo las suposiciones H1 todas o H2 todas. Esto demuestra que los métodos mayoritarios son menos sensibles a suposiciones que los métodos posicionales. El enfoque de superación siempre tiene un rendimiento significativamente mejor que los métodos posicionales combSUM y combMNZ. También tiene un mejor rendimiento que el método de la cadena de Markov, especialmente bajo la suposición H2 donde la diferencia de rendimientos se vuelve significativa. 6. CONCLUSIONES En este artículo, abordamos el problema de agregación de rangos donde se deben fusionar listas de documentos diferentes, pero no disjuntas. Notamos que las clasificaciones de entrada pueden ocultar empates, por lo que no deben considerarse como órdenes completos. Solo se debe utilizar información sólida de cada clasificación de entrada. Los métodos actuales de agregación de rangos, y especialmente los métodos posicionales (por ejemplo, combSUM [15]), no fueron diseñados inicialmente para trabajar con tales clasificaciones. Deben adaptarse teniendo en cuenta supuestos de trabajo específicos. Proponemos un nuevo método de clasificación para la agregación de rangos que está bien adaptado al contexto de la RI. De hecho, clasifica dos documentos con respecto a la intensidad de la diferencia de sus posiciones en cada clasificación de entrada y también considera el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico. Tampoco es necesario hacer suposiciones específicas sobre las posiciones de los documentos faltantes. Esta es una característica importante, ya que la ausencia de un documento en un ranking no necesariamente debe interpretarse de forma negativa. Los resultados experimentales muestran que el método de clasificación supera significativamente a los populares métodos clásicos de fusión de datos posicionales como las estrategias combSUM y combMNZ. También supera en rendimiento a los métodos mayoritarios de buen rendimiento, como el método de la cadena de Markov. Estos resultados se prueban con diferentes colecciones de pruebas y consultas. De los experimentos, también podemos concluir que para mejorar el rendimiento, deberíamos fusionar listas de resultados de modelos de IR con buen desempeño, y que los métodos de fusión de datos mayoritarios funcionan mejor que los métodos posicionales. El método propuesto puede tener un impacto real en el rendimiento de la metabúsqueda web, ya que la mayoría de los motores de búsqueda primarios solo proporcionan clasificaciones, mientras que la mayoría de los enfoques actuales necesitan puntuaciones para fusionar las listas de resultados en una sola lista. El trabajo adicional implica investigar si el enfoque de clasificación por jerarquía funciona bien en varios otros contextos, por ejemplo, utilizando las puntuaciones de los documentos o alguna combinación de los rangos y puntuaciones de los documentos. Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este artículo. 7. REFERENCIAS [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur. Fusión de enfoques intensivos en conocimiento y estadísticos para recuperar y anotar documentos genómicos textuales. En Actas de TREC2005. Publicación del NIST, 2005. [2] R. A. Baeza-Yates y B. A. Ribeiro-Neto. Recuperación de información moderna. ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de múltiples sistemas de recuperación clasificados. En Actas ACM-SIGIR94, páginas 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. \n\nSpringer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, y J. A. Shaw. Combinando evidencia de múltiples representaciones de consulta para la recuperación de información. IPM, 31(3):431-448, 1995. [5] J. Borda. \n\nIPM, 31(3):431-448, 1995. [5] J. Borda. Memoria sobre las elecciones por voto secreto. Historia de la Academia de Ciencias, 1781. [6] J. P. Callan, Z. Lu y W. B. Croft. Buscando colecciones distribuidas con redes de inferencia. En Actas ACM-SIGIR95, páginas 21-28, 1995. [7] M. Condorcet. Ensayo sobre la aplicación del análisis de probabilidad a las decisiones tomadas por mayoría de votos. Imprimerie Royale, París, 1785. [8] W. D. Cook y M. Kress. Clasificación ordinal con intensidad de preferencia. Ciencia de la Gestión, 31(1):26-32, 1985. [9] N. Craswell y D. Hawking. Resumen de la pista web TREC-2002. En Actas de TREC2002. Publicación del NIST, 2002. [10] N. Craswell y D. Hawking. Resumen de la TREC-2004 Web Track. En Actas de TREC2004. Publicación del NIST, 2004. [11] C. Dwork, S. R. Kumar, M. Naor y D. Sivakumar. Métodos de agregación de clasificaciones para la Web. En Actas WWW2001, páginas 613-622, 2001. [12] R. Fagin. Combinando información difusa de múltiples sistemas. JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar y E. Vee. Comparando y agregando clasificaciones con empates. En PODS, páginas 47-58, 2004. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas de los k mejores. SIAM J. en Matemáticas Discretas, 17(1):134-160, 2003. [15] E. A. Zorro y J. A. Shaw. Combinación de múltiples búsquedas. En Actas de TREC3. Publicación del NIST, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes y P. DasGupta. Un estudio de la superposición entre representaciones de documentos. Tecnología de la Información: Investigación y Desarrollo, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell y J. Callan. Selección de colecciones y fusión de resultados con patentes de EE. UU. organizadas por tema y datos de TREC. En las Actas ACM-CIKM2000, páginas 282-289. ACM Press, 2000. [18] A.\nACM Press, 2000. [18] A. Le Calv´e y J. Savoy. Estrategia de fusión de bases de datos basada en regresión logística. IPM, 36(3):341-359, 2000. [19] J. H. Lee. \n\nIPM, 36(3):341-359, 2000. [19] J. H. Lee. Análisis de la combinación de múltiples evidencias. En Actas ACM-SIGIR97, páginas 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier y J. Dunnion. Probfuse: un enfoque probabilístico para la fusión de datos. En las Actas ACM-SIGIR2006, páginas 139-146. ACM Press, 2006. [21] J. I. Marden. \n\nACM Press, 2006. [21] J. I. Marden. Analizando y modelando datos de rango. Número 64 en Monografías sobre Estadística y Probabilidad Aplicada. Chapman & Hall, 1995. [22] M. Montague y J. A. Aslam. Consistencia de la metabúsqueda. En las Actas ACM-SIGIR2001, páginas 386-387. ACM Press, 2001. [23] D. M. Pennock y E. Horvitz. Análisis de los fundamentos axiomáticos del filtrado colaborativo. En el Taller sobre Inteligencia Artificial para el Comercio Electrónico en la 16ª Conferencia Nacional de Inteligencia Artificial, 1999. [24] M. E. Renda y U. Straccia. Búsqueda web metasearch: métodos de agregación de rango basados en rango vs. puntuación. En las Actas de ACM-SAC2003, páginas 841-846. ACM Press, 2003. [25] W. H. Riker. \n\nACM Press, 2003. [25] W. H. Riker. Liberalismo contra populismo. Waveland Press, 1982. [26] B. Roy. \n\nWaveland Press, 1982. [26] B. Roy. El enfoque de jerarquización y los fundamentos de los métodos ELECTRE. Teoría y decisión, 31:49-73, 1991. [27] B. Roy y J. Hugonnard. Clasificación de proyectos de extensión de líneas suburbanas en el sistema de metro de París mediante un método multicriterio. Investigación en Transporte, 16A(4):301-312, 1982. [28] L. Si y J. Callan. Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En las Actas ACM-SIGIR2002, páginas 19-26. ACM Press, 2002. [29] M. Truchon. \n\nACM Press, 2002. [29] M. Truchon. Una extensión del criterio de Condorcet y órdenes de Kemeny. Cuaderno 9813, Centro de Investigación en Economía y Finanzas Aplicadas, octubre de 1998. [30] H. Turtle y W. B. Croft. Redes de inferencia para la recuperación de documentos. En Actas de ACM-SIGIR90, páginas 1-24. ACM Press, 1990. [31] C. C. Vogt y G. W. Cottrell. Fusión a través de una combinación lineal de puntuaciones. Recuperación de información, 1(3):151-173, 1999.",
    "original_sentences": [
        "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
        "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
        "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
        "We show that the proposed method deals well with the Information Retrieval distinctive features.",
        "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
        "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
        "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
        "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
        "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
        "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
        "Such approaches give rise to metasearch engines in the Web context.",
        "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
        "This corresponds indeed to the reality, where only ordinal information is available.",
        "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
        "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
        "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
        "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
        "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
        "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
        "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
        "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
        "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
        "They also give rigid interpretation to the exact ranking of the items.",
        "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
        "The remaining of the paper is organized as follows.",
        "We first review current rank aggregation methods in Section 2.",
        "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
        "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
        "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
        "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
        "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
        "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
        "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
        "Thus, di j di means di is ranked better than di in j.",
        "When Dj = D, j is said to be a full list.",
        "Otherwise, it is a partial list.",
        "If di belongs to Dj, rj i denotes the rank or position of di in j.",
        "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
        "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
        "Restricting PR to the rankings containing document di defines PRi.",
        "We also call the number of rankings which contain document di the rank hits of di [19].",
        "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
        "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
        "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
        "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
        "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
        "The first three operators correspond to the sum, min and max operators, respectively.",
        "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
        "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
        "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
        "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
        "It extends to several lists as follows.",
        "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
        "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
        "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
        "During the training process, probabilities of relevance are calculated.",
        "For subsequent queries, documents are ranked based on these probabilities.",
        "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
        "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
        "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
        "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
        "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
        "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
        "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
        "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
        "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
        "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
        "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
        "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
        "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
        "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
        "In this case, we call such rankings semi orders.",
        "This was outlined in [13] as the problem of aggregation with ties.",
        "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
        "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
        "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
        "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
        "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
        "Partial lists can have various lengths, which can favour long lists.",
        "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
        "H1 all: We consider all the documents from each input ranking. 2.",
        "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
        "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
        "H2 all: We consider all the documents which are ranked in at least one input ranking.",
        "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
        "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
        "Some candidate documents are missing documents in some input rankings.",
        "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
        "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
        "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
        "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
        "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
        "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
        "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
        "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
        "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
        "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
        "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
        "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
        "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
        "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
        "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
        "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
        "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
        "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
        "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
        "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
        "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
        "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
        "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
        "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
        "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
        "Therefore, we need specific procedures in order to derive a consensus ranking.",
        "We propose the following procedure which finds its roots in [27].",
        "It consists in partitioning the set of documents into r ranked classes.",
        "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
        "Documents within the same equivalence class are ranked arbitrarily.",
        "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
        "Each class Ch results from a distillation process.",
        "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
        "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
        "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
        "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
        "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
        "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
        "The following tables give the concordance, discordance and outranking matrices.",
        "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
        "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
        "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
        "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
        "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
        "They are respectively 2, 2, 2, -2 and -4.",
        "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
        "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
        "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
        "They are respectively 1 and -1.",
        "So C3 = E1 = {d4}.",
        "The last document d5 is the only document of the last class C3.",
        "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
        "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
        "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
        "In this task, there are 75 topics where only a short description of each is given.",
        "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
        "The performances of these runs are reported in table 3.",
        "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
        "The number of documents retrieved by all these runs ranges from 543 to 5769.",
        "Their average (median) number is 3340 (3386).",
        "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
        "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
        "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
        "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
        "In the tables of the following section, statistically significant differences are marked with an asterisk.",
        "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
        "We set our basic run mcm with the following parameters.",
        "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
        "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
        "They consequently may vary from one ranking to another.",
        "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
        "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
        "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
        "To test the run mcm, we had chosen the following assumptions.",
        "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
        "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
        "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
        "This was validated by preliminary experiments.",
        "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
        "Afterwards, we study the impact of tuning parameters.",
        "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
        "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
        "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
        "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
        "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
        "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
        "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
        "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
        "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
        "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
        "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
        "Tables 5 and 6 confirm this evidence.",
        "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
        "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
        "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
        "Values between brackets are rank hits.",
        "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
        "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
        "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
        "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
        "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
        "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
        "We can thus conclude that the input rankings are semi orders rather than complete orders.",
        "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
        "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
        "Performance drops significantly for very low and very high values of the concordance threshold.",
        "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
        "Therefore, the outranking relation becomes either too weak or too strong respectively.",
        "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
        "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
        "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
        "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
        "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
        "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
        "Therefore, when they are considered, performance decreases.",
        "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
        "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
        "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
        "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
        "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
        "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
        "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
        "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
        "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
        "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
        "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
        "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
        "This aims to show whether relative performance of the various methods changes from year to year.",
        "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
        "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
        "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
        "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
        "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
        "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
        "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
        "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
        "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
        "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
        "Only robust information should be used from each input ranking.",
        "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
        "They should be adapted by considering specific working assumptions.",
        "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
        "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
        "There is also no need to make specific assumptions on the positions of the missing documents.",
        "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
        "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
        "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
        "These results are tested against different test collections and queries.",
        "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
        "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
        "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
        "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
        "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
        "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
        "In Proceedings TREC2005.",
        "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
        "A. Ribeiro-Neto.",
        "Modern Information Retrieval.",
        "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
        "Automatic combination of multiple ranked retrieval systems.",
        "In Proceedings ACM-SIGIR94, pages 173-181.",
        "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
        "Fox, and J.",
        "A. Shaw.",
        "Combining evidence of multiple query representations for information retrieval.",
        "IPM, 31(3):431-448, 1995. [5] J. Borda.",
        "M´emoire sur les ´elections au scrutin.",
        "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
        "Searching distributed collections with inference networks.",
        "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
        "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
        "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
        "Ordinal ranking with intensity of preference.",
        "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
        "Overview of the TREC-2002 Web Track.",
        "In Proceedings TREC2002.",
        "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
        "Overview of the TREC-2004 Web Track.",
        "In Proceedings of TREC2004.",
        "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
        "Rank aggregation methods for the Web.",
        "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
        "Combining fuzzy information from multiple systems.",
        "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
        "Comparing and aggregating rankings with ties.",
        "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
        "Comparing top k lists.",
        "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
        "Fox and J.",
        "A. Shaw.",
        "Combination of multiple searches.",
        "In Proceedings of TREC3.",
        "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
        "A study of the overlap among document representations.",
        "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
        "Collection selection and results merging with topically organized U.S. patents and TREC data.",
        "In Proceedings ACM-CIKM2000, pages 282-289.",
        "ACM Press, 2000. [18] A.",
        "Le Calv´e and J. Savoy.",
        "Database merging strategy based on logistic regression.",
        "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
        "Analyses of multiple evidence combination.",
        "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
        "Probfuse: a probabilistic approach to data fusion.",
        "In Proceedings ACM-SIGIR2006, pages 139-146.",
        "ACM Press, 2006. [21] J. I. Marden.",
        "Analyzing and Modeling Rank Data.",
        "Number 64 in Monographs on Statistics and Applied Probability.",
        "Chapman & Hall, 1995. [22] M. Montague and J.",
        "A. Aslam.",
        "Metasearch consistency.",
        "In Proceedings ACM-SIGIR2001, pages 386-387.",
        "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
        "Analysis of the axiomatic foundations of collaborative filtering.",
        "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
        "Web metasearch: rank vs. score based rank aggregation methods.",
        "In Proceedings ACM-SAC2003, pages 841-846.",
        "ACM Press, 2003. [25] W. H. Riker.",
        "Liberalism against populism.",
        "Waveland Press, 1982. [26] B. Roy.",
        "The outranking approach and the foundations of ELECTRE methods.",
        "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
        "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
        "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
        "Using sampled data and regression to merge search engine results.",
        "In Proceedings ACM-SIGIR2002, pages 19-26.",
        "ACM Press, 2002. [29] M. Truchon.",
        "An extension of the Condorcet criterion and Kemeny orders.",
        "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
        "Inference networks for document retrieval.",
        "In Proceedings of ACM-SIGIR90, pages 1-24.",
        "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
        "Fusion via a linear combination of scores.",
        "Information Retrieval, 1(3):151-173, 1999."
    ],
    "translated_text_sentences": [
        "Un enfoque de clasificación para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.).",
        "En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking.",
        "En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro.",
        "Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información.",
        "Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ.",
        "Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación.",
        "Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1.",
        "INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario.",
        "Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia.",
        "Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking.",
        "Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet.",
        "Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia.",
        "Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal.",
        "La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16].",
        "Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada.",
        "Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes.",
        "Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes.",
        "Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil.",
        "Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22].",
        "La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1].",
        "El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23].",
        "La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos.",
        "También dan una interpretación rígida al ranking exacto de los elementos.",
        "Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones.",
        "El resto del documento está organizado de la siguiente manera.",
        "Primero revisamos los métodos actuales de agregación de rangos en la Sección 2.",
        "Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3).",
        "En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR.",
        "Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2.",
        "TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar.",
        "Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme.",
        "Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos.",
        "Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n).",
        "Por lo tanto, di j di significa que di está clasificado mejor que di en j.",
        "Cuando Dj = D, se dice que j es una lista completa.",
        "De lo contrario, es una lista parcial.",
        "Si di pertenece a Dj, rj i denota la clasificación o posición de di en j.",
        "Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|.",
        "Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n).",
        "Restringir PR a los rankings que contienen el documento di define PRi.",
        "También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19].",
        "El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di.",
        "Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos.",
        "Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28].",
        "Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones.",
        "Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ.",
        "Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente.",
        "CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango.",
        "Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás.",
        "Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21].",
        "Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|.",
        "Se extiende a varias listas de la siguiente manera.",
        "Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j).",
        "Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i .",
        "Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro.",
        "Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia.",
        "Para consultas posteriores, los documentos se clasifican según estas probabilidades.",
        "Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j).",
        "Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k.",
        "Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes.",
        "Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares.",
        "Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|.",
        "La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar.",
        "Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman.",
        "Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas.",
        "Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición.",
        "En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di.",
        "La clasificación de consenso corresponde a la distribución estacionaria de MC4.3.",
        "3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas.",
        "Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor.",
        "De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates.",
        "En este caso, llamamos a tales clasificaciones semiórdenes.",
        "Esto fue descrito en [13] como el problema de la agregación con empates.",
        "Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares.",
        "Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales.",
        "Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada.",
        "Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda.",
        "La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1.",
        "Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas.",
        "Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada.",
        "Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2.",
        "Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso.",
        "Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1).",
        "Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada.",
        "De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos.",
        "También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3.",
        "Algunos documentos candidatos faltan en algunas clasificaciones de entrada.",
        "Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible.",
        "Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición.",
        "H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4.",
        "Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso.",
        "En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada.",
        "H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos.",
        "En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4.",
        "Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal.",
        "Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes.",
        "Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados.",
        "Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos.",
        "Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso.",
        "Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada.",
        "Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates.",
        "Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta.",
        "Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio.",
        "Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías).",
        "Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos.",
        "La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi.",
        "Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones de prelación.",
        "Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia.",
        "Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv.",
        "Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n).",
        "También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29].",
        "Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos.",
        "Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso.",
        "Proponemos el siguiente procedimiento que encuentra sus raíces en [27].",
        "Consiste en dividir el conjunto de documentos en r clases clasificadas.",
        "Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores.",
        "Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria.",
        "Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk.",
        "Cada clase Ch resulta de un proceso de destilación.",
        "Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \\ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3.",
        "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1.",
        "Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1.",
        "Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}.",
        "La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4).",
        "Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente.",
        "Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia.",
        "Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|.",
        "Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅.",
        "Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple.",
        "Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación.",
        "La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1.",
        "Son respectivamente 2, 2, 2, -2 y -4.",
        "Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}.",
        "Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados.",
        "En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \\ C1 = {d4, d5}.",
        "Son respectivamente 1 y -1.",
        "Entonces C3 = E1 = {d4}.",
        "El último documento d5 es el único documento de la última clase C3.",
        "Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5.",
        "EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos.",
        "En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10].",
        "En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno.",
        "Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004.",
        "Las actuaciones de estas carreras se informan en la tabla 3.",
        "Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos.",
        "El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769.",
        "Su número promedio (mediana) es 3340 (3386).",
        "Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11].",
        "Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10.",
        "Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos.",
        "En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas.",
        "En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco.",
        "Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del enfoque de clasificación cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del enfoque de clasificación con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada.",
        "Configuramos nuestro módulo de ejecución básico con los siguientes parámetros.",
        "Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%).",
        "Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada.",
        "Por lo tanto, pueden variar de un ranking a otro.",
        "Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0).",
        "Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi.",
        "Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0).",
        "Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones.",
        "Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo.",
        "En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo.",
        "Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo.",
        "Esto fue validado por experimentos preliminares.",
        "Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones.",
        "Después, estudiamos el impacto de ajustar los parámetros.",
        "Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del enfoque de clasificación por jerarquías bajo diferentes hipótesis de trabajo.",
        "En la Tabla 4: Impacto de las suposiciones de trabajo, se muestra que en la ejecución mcm22, en la que los documentos faltantes se colocan todos en la misma última posición de cada clasificación de entrada, se produce una disminución en el rendimiento con respecto a la ejecución mcm.",
        "Además, S@1 pasa de 41.33% a 34.67% (-16.11%).",
        "Esto muestra que varios documentos relevantes que inicialmente se ubicaron en la primera posición del ranking de consenso en mcm, pierden esta primera posición pero siguen clasificados en los 5 primeros documentos, ya que S@5 no cambió.",
        "También concluimos que los documentos que tienen posiciones bastante buenas en algunas clasificaciones de entrada son más propensos a ser relevantes, aunque falten en otras clasificaciones.",
        "Por consiguiente, cuando faltan en ciertas clasificaciones, asignarles rangos más bajos a estos documentos es perjudicial para el rendimiento.",
        "Además, a partir de la Tabla 4, encontramos que las actuaciones de las ejecuciones mcm y mcm23 son similares.",
        "Por lo tanto, el enfoque de clasificación no está sujeto a mantener las posiciones iniciales de los documentos candidatos o a recalcularlas descartando los excluidos.",
        "De la misma Tabla 4, el rendimiento del enfoque de clasificación aumenta significativamente para las ejecuciones mcm24 y mcm25.",
        "Por lo tanto, ya sea que consideremos todos los documentos presentes en la mitad de las clasificaciones (mcm24) o consideremos todos los documentos clasificados en las primeras 100 posiciones en una o más clasificaciones (mcm25), se incrementan los rendimientos.",
        "Este resultado era predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos.",
        "Las tablas 5 y 6 confirman esta evidencia.",
        "La Tabla 5, donde los valores entre corchetes de la primera columna indican el número de documentos que se retienen de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce a un aumento en el rendimiento.",
        "Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento.",
        "Tabla 5: Impacto del número de documentos retenidos. Identificador de ejecución MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% La Tabla 6 informa ejecuciones correspondientes a variaciones de H2 k.",
        "Los valores entre corchetes son éxitos de rango.",
        "Por ejemplo, en la ejecución mcm32, solo se consideraron exitosos los documentos que estaban presentes en 3 o más clasificaciones de entrada.",
        "Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos son descartados.",
        "Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos de RI.",
        "Tabla 6: Rendimiento considerando diferentes éxitos de rango. Identificación de ejecución MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% Para las ejecuciones mcm24 y mcm25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo por consulta aumentó y se situó en alrededor de 5 segundos. 5.2.2 Impacto de la Variación de los Parámetros. La Tabla 7 muestra la variación de rendimiento del enfoque de clasificación por preferencias cuando se consideran diferentes umbrales de preferencia.",
        "Encontramos una mejora en el rendimiento hasta valores de umbral de aproximadamente el 5%, luego hay una disminución en el rendimiento que se vuelve significativa para valores de umbral superiores al 10%.",
        "Además, S@1 mejora del 41.33% al 46.67% cuando el umbral de preferencia cambia de 0 a 5%.",
        "Por lo tanto, podemos concluir que las clasificaciones de entrada son semiordeles en lugar de órdenes completos.",
        "La Tabla 8 muestra la evolución de las medidas de rendimiento con respecto al umbral de concordancia.",
        "Podemos concluir que para colocar el documento di antes de di en la clasificación de consenso, en la Tabla 7: Impacto de la variación del umbral de preferencia del 0 al 12.5%. Ejecutar Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% al menos la mitad de las clasificaciones de entrada de PRi ∩ PRi deben ser concordantes.",
        "El rendimiento disminuye significativamente para valores muy bajos y muy altos del umbral de concordancia.",
        "De hecho, para tales valores, la condición de concordancia se cumple o bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente.",
        "Por lo tanto, la relación de clasificación se vuelve demasiado débil o demasiado fuerte respectivamente.",
        "Tabla 8: Impacto de la variación de cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% En los experimentos, variar el umbral de veto, así como el umbral de discordancia dentro de intervalos razonables, no tiene un impacto significativo en las medidas de rendimiento.",
        "De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron un rendimiento similar, aunque hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada veten fácilmente.",
        "Además, el ajuste del umbral de discordancia se realizó para valores del 50% y 75% del umbral de veto.",
        "Para estas ejecuciones no observamos ninguna variación de rendimiento notable, aunque para umbrales de discordancia bajos (dmax < 20%), el rendimiento disminuyó ligeramente. 5.2.3 Impacto de la Variación del Número de Clasificaciones de Entrada Para estudiar la evolución del rendimiento cuando se consideran diferentes conjuntos de clasificaciones de entrada, realizamos tres ejecuciones adicionales donde se consideran 2, 4 y 6 de los conjuntos de clasificaciones de entrada con mejor rendimiento.",
        "Los resultados reportados en la Tabla 9 parecen ser contraintuitivos y tampoco respaldan hallazgos previos en la investigación sobre la agregación de rangos [3].",
        "Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento aportan más ruido que información para establecer la clasificación de consenso.",
        "Por lo tanto, cuando se consideran, el rendimiento disminuye.",
        "Tabla 9: Rendimiento considerando diferentes conjuntos de clasificaciones de entrada con mejor rendimiento. Identificador de ejecución MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de clasificaciones. En este conjunto de ejecuciones, comparamos el enfoque de clasificación con algunos métodos de agregación de clasificaciones estándar que han demostrado tener un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias CombSUM y CombMNZ.",
        "También examinamos el rendimiento de un método mayoritario que es el método de la cadena de Markov (MC4).",
        "Para las comparaciones, consideramos una relación de superación específica S∗ = S(5%,50%,50%,30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros.",
        "La primera fila de la Tabla 10 muestra el rendimiento de los métodos de agregación de rangos con respecto a un conjunto de suposiciones básicas A1 = (H1 100, H2 5, H4 nuevo): solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos los documentos presentes en 5 o más clasificaciones y actualizamos los rangos de los documentos exitosos.",
        "Para los métodos posicionales, colocamos los documentos faltantes en la cola de la clasificación (H3 sí), mientras que para nuestro método, al igual que para MC4, conservamos la hipótesis H3 no.",
        "Las tres filas siguientes de la Tabla 10 informan sobre los rendimientos al cambiar un elemento del conjunto de suposiciones básicas: la segunda fila corresponde al conjunto de suposiciones A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a 1000.",
        "La tercera fila corresponde al conjunto de supuestos A3 = (H1 100, H2 todos, H4 nuevos), es decir, considerando los documentos presentes en al menos un ranking.",
        "La cuarta fila corresponde al conjunto de supuestos A4 = (H1 100, H2 5, H4 init), es decir, manteniendo los rangos originales de los documentos exitosos.",
        "La quinta fila de la Tabla 10, etiquetada como A5, muestra el rendimiento cuando se consideran todas las 225 consultas de la pista web de TREC-2004.",
        "Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de Página de Inicio y Página Nombrada [10]) de la pista web TREC-2004.",
        "Este conjunto de pruebas tiene como objetivo demostrar si el rendimiento relativo de los diferentes métodos depende de la tarea.",
        "La última fila de la Tabla 10, etiquetada como A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de suposiciones A1 de la primera fila.",
        "Esto tiene como objetivo mostrar si el rendimiento relativo de los diferentes métodos cambia de un año a otro.",
        "Los valores entre corchetes de la Tabla 10 son variaciones del rendimiento de cada método de agregación de rangos con respecto al rendimiento del enfoque de superación.",
        "Tabla 10: Rendimiento (MAP) de diferentes métodos de agregación de rangos bajo 3 colecciones de pruebas diferentes mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) Del análisis de la tabla 10 se puede establecer lo siguiente: • para todas las ejecuciones, considerar todos los documentos en cada clasificación de entrada (A2) mejora significativamente el rendimiento (MAP aumenta en promedio un 11.62%).",
        "Esto es predecible ya que algunos documentos relevantes inicialmente no reportados recibirían mejores posiciones en la clasificación de consenso. • para todas las ejecuciones, considerar documentos incluso aquellos presentes en solo una clasificación de entrada (A3) mejora significativamente el rendimiento.",
        "Para mcm, combSUM y combMNZ, la mejora del rendimiento es más importante (el MAP aumenta en promedio un 20.27%) que para la ejecución de Markov (el MAP aumenta un 3.86%). • preservar las posiciones iniciales de los documentos (A4) o volver a calcularlas (A1) no tiene una influencia notable en el rendimiento para ambos métodos posicional y mayoritario. • considerar todas las consultas de la pista web de TREC2004 (A5) así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos. • considerando las consultas TD de la pista web de TREC2002, los rendimientos de todos los métodos de fusión de datos son más bajos que el del mejor ranking de entrada que tiene un valor de MAP de 18.58%.",
        "Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con la mejor, lo que añade más ruido a la clasificación de consenso. • los rendimientos de los métodos de fusión de datos mcm y markov son significativamente mejores que el de la mejor clasificación de entrada uogWebCAU150.",
        "Esto sigue siendo cierto solo para las ejecuciones combSUM y combMNZ bajo las suposiciones H1 todas o H2 todas.",
        "Esto demuestra que los métodos mayoritarios son menos sensibles a suposiciones que los métodos posicionales. El enfoque de superación siempre tiene un rendimiento significativamente mejor que los métodos posicionales combSUM y combMNZ.",
        "También tiene un mejor rendimiento que el método de la cadena de Markov, especialmente bajo la suposición H2 donde la diferencia de rendimientos se vuelve significativa. 6.",
        "CONCLUSIONES En este artículo, abordamos el problema de agregación de rangos donde se deben fusionar listas de documentos diferentes, pero no disjuntas.",
        "Notamos que las clasificaciones de entrada pueden ocultar empates, por lo que no deben considerarse como órdenes completos.",
        "Solo se debe utilizar información sólida de cada clasificación de entrada.",
        "Los métodos actuales de agregación de rangos, y especialmente los métodos posicionales (por ejemplo, combSUM [15]), no fueron diseñados inicialmente para trabajar con tales clasificaciones.",
        "Deben adaptarse teniendo en cuenta supuestos de trabajo específicos.",
        "Proponemos un nuevo método de clasificación para la agregación de rangos que está bien adaptado al contexto de la RI.",
        "De hecho, clasifica dos documentos con respecto a la intensidad de la diferencia de sus posiciones en cada clasificación de entrada y también considera el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico.",
        "Tampoco es necesario hacer suposiciones específicas sobre las posiciones de los documentos faltantes.",
        "Esta es una característica importante, ya que la ausencia de un documento en un ranking no necesariamente debe interpretarse de forma negativa.",
        "Los resultados experimentales muestran que el método de clasificación supera significativamente a los populares métodos clásicos de fusión de datos posicionales como las estrategias combSUM y combMNZ.",
        "También supera en rendimiento a los métodos mayoritarios de buen rendimiento, como el método de la cadena de Markov.",
        "Estos resultados se prueban con diferentes colecciones de pruebas y consultas.",
        "De los experimentos, también podemos concluir que para mejorar el rendimiento, deberíamos fusionar listas de resultados de modelos de IR con buen desempeño, y que los métodos de fusión de datos mayoritarios funcionan mejor que los métodos posicionales.",
        "El método propuesto puede tener un impacto real en el rendimiento de la metabúsqueda web, ya que la mayoría de los motores de búsqueda primarios solo proporcionan clasificaciones, mientras que la mayoría de los enfoques actuales necesitan puntuaciones para fusionar las listas de resultados en una sola lista.",
        "El trabajo adicional implica investigar si el enfoque de clasificación por jerarquía funciona bien en varios otros contextos, por ejemplo, utilizando las puntuaciones de los documentos o alguna combinación de los rangos y puntuaciones de los documentos.",
        "Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este artículo. 7.",
        "REFERENCIAS [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur.",
        "Fusión de enfoques intensivos en conocimiento y estadísticos para recuperar y anotar documentos genómicos textuales.",
        "En Actas de TREC2005.",
        "Publicación del NIST, 2005. [2] R. A. Baeza-Yates y B.",
        "A. Ribeiro-Neto.",
        "Recuperación de información moderna.",
        "ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew.",
        "Combinación automática de múltiples sistemas de recuperación clasificados.",
        "En Actas ACM-SIGIR94, páginas 173-181.",
        "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. \n\nSpringer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
        "Fox, y J.",
        "A. Shaw.",
        "Combinando evidencia de múltiples representaciones de consulta para la recuperación de información.",
        "IPM, 31(3):431-448, 1995. [5] J. Borda. \n\nIPM, 31(3):431-448, 1995. [5] J. Borda.",
        "Memoria sobre las elecciones por voto secreto.",
        "Historia de la Academia de Ciencias, 1781. [6] J. P. Callan, Z. Lu y W. B. Croft.",
        "Buscando colecciones distribuidas con redes de inferencia.",
        "En Actas ACM-SIGIR95, páginas 21-28, 1995. [7] M. Condorcet.",
        "Ensayo sobre la aplicación del análisis de probabilidad a las decisiones tomadas por mayoría de votos.",
        "Imprimerie Royale, París, 1785. [8] W. D. Cook y M. Kress.",
        "Clasificación ordinal con intensidad de preferencia.",
        "Ciencia de la Gestión, 31(1):26-32, 1985. [9] N. Craswell y D. Hawking.",
        "Resumen de la pista web TREC-2002.",
        "En Actas de TREC2002.",
        "Publicación del NIST, 2002. [10] N. Craswell y D. Hawking.",
        "Resumen de la TREC-2004 Web Track.",
        "En Actas de TREC2004.",
        "Publicación del NIST, 2004. [11] C. Dwork, S. R. Kumar, M. Naor y D. Sivakumar.",
        "Métodos de agregación de clasificaciones para la Web.",
        "En Actas WWW2001, páginas 613-622, 2001. [12] R. Fagin.",
        "Combinando información difusa de múltiples sistemas.",
        "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar y E. Vee.",
        "Comparando y agregando clasificaciones con empates.",
        "En PODS, páginas 47-58, 2004. [14] R. Fagin, R. Kumar y D. Sivakumar.",
        "Comparando listas de los k mejores.",
        "SIAM J. en Matemáticas Discretas, 17(1):134-160, 2003. [15] E. A.",
        "Zorro y J.",
        "A. Shaw.",
        "Combinación de múltiples búsquedas.",
        "En Actas de TREC3.",
        "Publicación del NIST, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes y P. DasGupta.",
        "Un estudio de la superposición entre representaciones de documentos.",
        "Tecnología de la Información: Investigación y Desarrollo, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell y J. Callan.",
        "Selección de colecciones y fusión de resultados con patentes de EE. UU. organizadas por tema y datos de TREC.",
        "En las Actas ACM-CIKM2000, páginas 282-289.",
        "ACM Press, 2000. [18] A.\nACM Press, 2000. [18] A.",
        "Le Calv´e y J. Savoy.",
        "Estrategia de fusión de bases de datos basada en regresión logística.",
        "IPM, 36(3):341-359, 2000. [19] J. H. Lee. \n\nIPM, 36(3):341-359, 2000. [19] J. H. Lee.",
        "Análisis de la combinación de múltiples evidencias.",
        "En Actas ACM-SIGIR97, páginas 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier y J. Dunnion.",
        "Probfuse: un enfoque probabilístico para la fusión de datos.",
        "En las Actas ACM-SIGIR2006, páginas 139-146.",
        "ACM Press, 2006. [21] J. I. Marden. \n\nACM Press, 2006. [21] J. I. Marden.",
        "Analizando y modelando datos de rango.",
        "Número 64 en Monografías sobre Estadística y Probabilidad Aplicada.",
        "Chapman & Hall, 1995. [22] M. Montague y J.",
        "A. Aslam.",
        "Consistencia de la metabúsqueda.",
        "En las Actas ACM-SIGIR2001, páginas 386-387.",
        "ACM Press, 2001. [23] D. M. Pennock y E. Horvitz.",
        "Análisis de los fundamentos axiomáticos del filtrado colaborativo.",
        "En el Taller sobre Inteligencia Artificial para el Comercio Electrónico en la 16ª Conferencia Nacional de Inteligencia Artificial, 1999. [24] M. E. Renda y U. Straccia.",
        "Búsqueda web metasearch: métodos de agregación de rango basados en rango vs. puntuación.",
        "En las Actas de ACM-SAC2003, páginas 841-846.",
        "ACM Press, 2003. [25] W. H. Riker. \n\nACM Press, 2003. [25] W. H. Riker.",
        "Liberalismo contra populismo.",
        "Waveland Press, 1982. [26] B. Roy. \n\nWaveland Press, 1982. [26] B. Roy.",
        "El enfoque de jerarquización y los fundamentos de los métodos ELECTRE.",
        "Teoría y decisión, 31:49-73, 1991. [27] B. Roy y J. Hugonnard.",
        "Clasificación de proyectos de extensión de líneas suburbanas en el sistema de metro de París mediante un método multicriterio.",
        "Investigación en Transporte, 16A(4):301-312, 1982. [28] L. Si y J. Callan.",
        "Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda.",
        "En las Actas ACM-SIGIR2002, páginas 19-26.",
        "ACM Press, 2002. [29] M. Truchon. \n\nACM Press, 2002. [29] M. Truchon.",
        "Una extensión del criterio de Condorcet y órdenes de Kemeny.",
        "Cuaderno 9813, Centro de Investigación en Economía y Finanzas Aplicadas, octubre de 1998. [30] H. Turtle y W. B. Croft.",
        "Redes de inferencia para la recuperación de documentos.",
        "En Actas de ACM-SIGIR90, páginas 1-24.",
        "ACM Press, 1990. [31] C. C. Vogt y G. W. Cottrell.",
        "Fusión a través de una combinación lineal de puntuaciones.",
        "Recuperación de información, 1(3):151-173, 1999."
    ],
    "error_count": 9,
    "keys": {
        "rank aggregation": {
            "translated_key": "agregación de rangos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for <br>rank aggregation</br> in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the <br>rank aggregation</br> problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a <br>rank aggregation</br> method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "<br>rank aggregation</br> approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that <br>rank aggregation</br> has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that <br>rank aggregation</br> methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The <br>rank aggregation</br> problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current <br>rank aggregation</br> methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current <br>rank aggregation</br> methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of <br>rank aggregation</br> methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the <br>rank aggregation</br> methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The <br>rank aggregation</br> or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the <br>rank aggregation</br> problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE <br>rank aggregation</br> PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "<br>rank aggregation</br> of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, <br>rank aggregation</br> methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR <br>rank aggregation</br> 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current <br>rank aggregation</br> methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the <br>rank aggregation</br> purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of <br>rank aggregation</br> methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for <br>rank aggregation</br>.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard <br>rank aggregation</br> algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard <br>rank aggregation</br> strategies , and iii) check whether <br>rank aggregation</br> performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding <br>rank aggregation</br> research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different <br>rank aggregation</br> Methods In this set of runs, we compare the outranking approach with some standard <br>rank aggregation</br> methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the <br>rank aggregation</br> methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each <br>rank aggregation</br> method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different <br>rank aggregation</br> methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the <br>rank aggregation</br> problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current <br>rank aggregation</br> methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for <br>rank aggregation</br> which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "<br>rank aggregation</br> methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based <br>rank aggregation</br> methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [
                "An Outranking Approach for <br>rank aggregation</br> in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the <br>rank aggregation</br> problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a <br>rank aggregation</br> method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "<br>rank aggregation</br> approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Several studies argued that <br>rank aggregation</br> has the potential of combining effectively all the various sources of evidence considered in various input methods."
            ],
            "translated_annotated_samples": [
                "Un enfoque de clasificación para la <br>agregación de rangos</br> en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.).",
                "En este artículo, nos enfocamos en el problema de <br>agregación de rangos</br>, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking.",
                "En este contexto, proponemos un método de <br>agregación de rangos</br> dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro.",
                "Los enfoques de <br>agregación de rangos</br>, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking.",
                "Varios estudios argumentaron que la <br>agregación de rangos</br> tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada."
            ],
            "translated_text": "Un enfoque de clasificación para la <br>agregación de rangos</br> en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de <br>agregación de rangos</br>, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de <br>agregación de rangos</br> dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de <br>agregación de rangos</br>, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la <br>agregación de rangos</br> tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in <br>information retrieval</br> Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in <br>information retrieval</br> usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the <br>information retrieval</br> distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current <br>information retrieval</br> (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern <br>information retrieval</br>.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for <br>information retrieval</br>.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "<br>information retrieval</br>, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [
                "An Outranking Approach for Rank Aggregation in <br>information retrieval</br> Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in <br>information retrieval</br> usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "We show that the proposed method deals well with the <br>information retrieval</br> distinctive features.",
                "INTRODUCTION A wide range of current <br>information retrieval</br> (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "Modern <br>information retrieval</br>.",
                "Combining evidence of multiple query representations for <br>information retrieval</br>."
            ],
            "translated_annotated_samples": [
                "Un enfoque de clasificación para la agregación de rangos en la <br>recuperación de información</br>. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.).",
                "Mostramos que el método propuesto se desempeña bien con las características distintivas de la <br>Recuperación de Información</br>.",
                "INTRODUCCIÓN Una amplia gama de enfoques actuales de <br>Recuperación de Información</br> (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario.",
                "Recuperación de información moderna.",
                "Combinando evidencia de múltiples representaciones de consulta para la <br>recuperación de información</br>."
            ],
            "translated_text": "Un enfoque de clasificación para la agregación de rangos en la <br>recuperación de información</br>. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la <br>Recuperación de Información</br>. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de <br>Recuperación de Información</br> (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1]. El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23]. La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida al ranking exacto de los elementos. Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones. El resto del documento está organizado de la siguiente manera. Primero revisamos los métodos actuales de agregación de rangos en la Sección 2. Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR. Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2. TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar. Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme. Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos. Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n). Por lo tanto, di j di significa que di está clasificado mejor que di en j. Cuando Dj = D, se dice que j es una lista completa. De lo contrario, es una lista parcial. Si di pertenece a Dj, rj i denota la clasificación o posición de di en j. Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n). Restringir PR a los rankings que contienen el documento di define PRi. También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19]. El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di. Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos. Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28]. Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones. Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ. Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente. CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango. Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás. Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j). Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i . Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican según estas probabilidades. Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j). Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k. Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes. Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares. Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman. Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. 3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas. Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates. En este caso, llamamos a tales clasificaciones semiórdenes. Esto fue descrito en [13] como el problema de la agregación con empates. Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares. Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales. Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada. Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1. Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada. Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2. Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1). Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada. De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos. También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3. Algunos documentos candidatos faltan en algunas clasificaciones de entrada. Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición. H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4. Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso. En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos. En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4. Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal. Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes. Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso. Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates. Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio. Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías). Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos. La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones de prelación. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia. Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv. Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n). También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29]. Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en r clases clasificadas. Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria. Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk. Cada clase Ch resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \\ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1. Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1. Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4). Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente. Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia. Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|. Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅. Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple. Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}. Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \\ C1 = {d4, d5}. Son respectivamente 1 y -1. Entonces C3 = E1 = {d4}. El último documento d5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos. En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10]. En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004. Las actuaciones de estas carreras se informan en la tabla 3. Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediana) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10. Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos. En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del enfoque de clasificación cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del enfoque de clasificación con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada. Configuramos nuestro módulo de ejecución básico con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%). Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada. Por lo tanto, pueden variar de un ranking a otro. Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi. Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0). Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones. Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones. Después, estudiamos el impacto de ajustar los parámetros. Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del enfoque de clasificación por jerarquías bajo diferentes hipótesis de trabajo. En la Tabla 4: Impacto de las suposiciones de trabajo, se muestra que en la ejecución mcm22, en la que los documentos faltantes se colocan todos en la misma última posición de cada clasificación de entrada, se produce una disminución en el rendimiento con respecto a la ejecución mcm. Además, S@1 pasa de 41.33% a 34.67% (-16.11%). Esto muestra que varios documentos relevantes que inicialmente se ubicaron en la primera posición del ranking de consenso en mcm, pierden esta primera posición pero siguen clasificados en los 5 primeros documentos, ya que S@5 no cambió. También concluimos que los documentos que tienen posiciones bastante buenas en algunas clasificaciones de entrada son más propensos a ser relevantes, aunque falten en otras clasificaciones. Por consiguiente, cuando faltan en ciertas clasificaciones, asignarles rangos más bajos a estos documentos es perjudicial para el rendimiento. Además, a partir de la Tabla 4, encontramos que las actuaciones de las ejecuciones mcm y mcm23 son similares. Por lo tanto, el enfoque de clasificación no está sujeto a mantener las posiciones iniciales de los documentos candidatos o a recalcularlas descartando los excluidos. De la misma Tabla 4, el rendimiento del enfoque de clasificación aumenta significativamente para las ejecuciones mcm24 y mcm25. Por lo tanto, ya sea que consideremos todos los documentos presentes en la mitad de las clasificaciones (mcm24) o consideremos todos los documentos clasificados en las primeras 100 posiciones en una o más clasificaciones (mcm25), se incrementan los rendimientos. Este resultado era predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos. Las tablas 5 y 6 confirman esta evidencia. La Tabla 5, donde los valores entre corchetes de la primera columna indican el número de documentos que se retienen de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce a un aumento en el rendimiento. Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento. Tabla 5: Impacto del número de documentos retenidos. Identificador de ejecución MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% La Tabla 6 informa ejecuciones correspondientes a variaciones de H2 k. Los valores entre corchetes son éxitos de rango. Por ejemplo, en la ejecución mcm32, solo se consideraron exitosos los documentos que estaban presentes en 3 o más clasificaciones de entrada. Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos son descartados. Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos de RI. Tabla 6: Rendimiento considerando diferentes éxitos de rango. Identificación de ejecución MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% Para las ejecuciones mcm24 y mcm25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo por consulta aumentó y se situó en alrededor de 5 segundos. 5.2.2 Impacto de la Variación de los Parámetros. La Tabla 7 muestra la variación de rendimiento del enfoque de clasificación por preferencias cuando se consideran diferentes umbrales de preferencia. Encontramos una mejora en el rendimiento hasta valores de umbral de aproximadamente el 5%, luego hay una disminución en el rendimiento que se vuelve significativa para valores de umbral superiores al 10%. Además, S@1 mejora del 41.33% al 46.67% cuando el umbral de preferencia cambia de 0 a 5%. Por lo tanto, podemos concluir que las clasificaciones de entrada son semiordeles en lugar de órdenes completos. La Tabla 8 muestra la evolución de las medidas de rendimiento con respecto al umbral de concordancia. Podemos concluir que para colocar el documento di antes de di en la clasificación de consenso, en la Tabla 7: Impacto de la variación del umbral de preferencia del 0 al 12.5%. Ejecutar Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% al menos la mitad de las clasificaciones de entrada de PRi ∩ PRi deben ser concordantes. El rendimiento disminuye significativamente para valores muy bajos y muy altos del umbral de concordancia. De hecho, para tales valores, la condición de concordancia se cumple o bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente. Por lo tanto, la relación de clasificación se vuelve demasiado débil o demasiado fuerte respectivamente. Tabla 8: Impacto de la variación de cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% En los experimentos, variar el umbral de veto, así como el umbral de discordancia dentro de intervalos razonables, no tiene un impacto significativo en las medidas de rendimiento. De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron un rendimiento similar, aunque hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada veten fácilmente. Además, el ajuste del umbral de discordancia se realizó para valores del 50% y 75% del umbral de veto. Para estas ejecuciones no observamos ninguna variación de rendimiento notable, aunque para umbrales de discordancia bajos (dmax < 20%), el rendimiento disminuyó ligeramente. 5.2.3 Impacto de la Variación del Número de Clasificaciones de Entrada Para estudiar la evolución del rendimiento cuando se consideran diferentes conjuntos de clasificaciones de entrada, realizamos tres ejecuciones adicionales donde se consideran 2, 4 y 6 de los conjuntos de clasificaciones de entrada con mejor rendimiento. Los resultados reportados en la Tabla 9 parecen ser contraintuitivos y tampoco respaldan hallazgos previos en la investigación sobre la agregación de rangos [3]. Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento aportan más ruido que información para establecer la clasificación de consenso. Por lo tanto, cuando se consideran, el rendimiento disminuye. Tabla 9: Rendimiento considerando diferentes conjuntos de clasificaciones de entrada con mejor rendimiento. Identificador de ejecución MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de clasificaciones. En este conjunto de ejecuciones, comparamos el enfoque de clasificación con algunos métodos de agregación de clasificaciones estándar que han demostrado tener un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias CombSUM y CombMNZ. También examinamos el rendimiento de un método mayoritario que es el método de la cadena de Markov (MC4). Para las comparaciones, consideramos una relación de superación específica S∗ = S(5%,50%,50%,30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros. La primera fila de la Tabla 10 muestra el rendimiento de los métodos de agregación de rangos con respecto a un conjunto de suposiciones básicas A1 = (H1 100, H2 5, H4 nuevo): solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos los documentos presentes en 5 o más clasificaciones y actualizamos los rangos de los documentos exitosos. Para los métodos posicionales, colocamos los documentos faltantes en la cola de la clasificación (H3 sí), mientras que para nuestro método, al igual que para MC4, conservamos la hipótesis H3 no. Las tres filas siguientes de la Tabla 10 informan sobre los rendimientos al cambiar un elemento del conjunto de suposiciones básicas: la segunda fila corresponde al conjunto de suposiciones A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a 1000. La tercera fila corresponde al conjunto de supuestos A3 = (H1 100, H2 todos, H4 nuevos), es decir, considerando los documentos presentes en al menos un ranking. La cuarta fila corresponde al conjunto de supuestos A4 = (H1 100, H2 5, H4 init), es decir, manteniendo los rangos originales de los documentos exitosos. La quinta fila de la Tabla 10, etiquetada como A5, muestra el rendimiento cuando se consideran todas las 225 consultas de la pista web de TREC-2004. Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de Página de Inicio y Página Nombrada [10]) de la pista web TREC-2004. Este conjunto de pruebas tiene como objetivo demostrar si el rendimiento relativo de los diferentes métodos depende de la tarea. La última fila de la Tabla 10, etiquetada como A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de suposiciones A1 de la primera fila. Esto tiene como objetivo mostrar si el rendimiento relativo de los diferentes métodos cambia de un año a otro. Los valores entre corchetes de la Tabla 10 son variaciones del rendimiento de cada método de agregación de rangos con respecto al rendimiento del enfoque de superación. Tabla 10: Rendimiento (MAP) de diferentes métodos de agregación de rangos bajo 3 colecciones de pruebas diferentes mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) Del análisis de la tabla 10 se puede establecer lo siguiente: • para todas las ejecuciones, considerar todos los documentos en cada clasificación de entrada (A2) mejora significativamente el rendimiento (MAP aumenta en promedio un 11.62%). Esto es predecible ya que algunos documentos relevantes inicialmente no reportados recibirían mejores posiciones en la clasificación de consenso. • para todas las ejecuciones, considerar documentos incluso aquellos presentes en solo una clasificación de entrada (A3) mejora significativamente el rendimiento. Para mcm, combSUM y combMNZ, la mejora del rendimiento es más importante (el MAP aumenta en promedio un 20.27%) que para la ejecución de Markov (el MAP aumenta un 3.86%). • preservar las posiciones iniciales de los documentos (A4) o volver a calcularlas (A1) no tiene una influencia notable en el rendimiento para ambos métodos posicional y mayoritario. • considerar todas las consultas de la pista web de TREC2004 (A5) así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos. • considerando las consultas TD de la pista web de TREC2002, los rendimientos de todos los métodos de fusión de datos son más bajos que el del mejor ranking de entrada que tiene un valor de MAP de 18.58%. Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con la mejor, lo que añade más ruido a la clasificación de consenso. • los rendimientos de los métodos de fusión de datos mcm y markov son significativamente mejores que el de la mejor clasificación de entrada uogWebCAU150. Esto sigue siendo cierto solo para las ejecuciones combSUM y combMNZ bajo las suposiciones H1 todas o H2 todas. Esto demuestra que los métodos mayoritarios son menos sensibles a suposiciones que los métodos posicionales. El enfoque de superación siempre tiene un rendimiento significativamente mejor que los métodos posicionales combSUM y combMNZ. También tiene un mejor rendimiento que el método de la cadena de Markov, especialmente bajo la suposición H2 donde la diferencia de rendimientos se vuelve significativa. 6. CONCLUSIONES En este artículo, abordamos el problema de agregación de rangos donde se deben fusionar listas de documentos diferentes, pero no disjuntas. Notamos que las clasificaciones de entrada pueden ocultar empates, por lo que no deben considerarse como órdenes completos. Solo se debe utilizar información sólida de cada clasificación de entrada. Los métodos actuales de agregación de rangos, y especialmente los métodos posicionales (por ejemplo, combSUM [15]), no fueron diseñados inicialmente para trabajar con tales clasificaciones. Deben adaptarse teniendo en cuenta supuestos de trabajo específicos. Proponemos un nuevo método de clasificación para la agregación de rangos que está bien adaptado al contexto de la RI. De hecho, clasifica dos documentos con respecto a la intensidad de la diferencia de sus posiciones en cada clasificación de entrada y también considera el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico. Tampoco es necesario hacer suposiciones específicas sobre las posiciones de los documentos faltantes. Esta es una característica importante, ya que la ausencia de un documento en un ranking no necesariamente debe interpretarse de forma negativa. Los resultados experimentales muestran que el método de clasificación supera significativamente a los populares métodos clásicos de fusión de datos posicionales como las estrategias combSUM y combMNZ. También supera en rendimiento a los métodos mayoritarios de buen rendimiento, como el método de la cadena de Markov. Estos resultados se prueban con diferentes colecciones de pruebas y consultas. De los experimentos, también podemos concluir que para mejorar el rendimiento, deberíamos fusionar listas de resultados de modelos de IR con buen desempeño, y que los métodos de fusión de datos mayoritarios funcionan mejor que los métodos posicionales. El método propuesto puede tener un impacto real en el rendimiento de la metabúsqueda web, ya que la mayoría de los motores de búsqueda primarios solo proporcionan clasificaciones, mientras que la mayoría de los enfoques actuales necesitan puntuaciones para fusionar las listas de resultados en una sola lista. El trabajo adicional implica investigar si el enfoque de clasificación por jerarquía funciona bien en varios otros contextos, por ejemplo, utilizando las puntuaciones de los documentos o alguna combinación de los rangos y puntuaciones de los documentos. Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este artículo. 7. REFERENCIAS [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur. Fusión de enfoques intensivos en conocimiento y estadísticos para recuperar y anotar documentos genómicos textuales. En Actas de TREC2005. Publicación del NIST, 2005. [2] R. A. Baeza-Yates y B. A. Ribeiro-Neto. Recuperación de información moderna. ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de múltiples sistemas de recuperación clasificados. En Actas ACM-SIGIR94, páginas 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. \n\nSpringer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, y J. A. Shaw. Combinando evidencia de múltiples representaciones de consulta para la <br>recuperación de información</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "datum fusion problem": {
            "translated_key": "problema de fusión de datos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "multiple criterion framework": {
            "translated_key": "marco de múltiples criterios",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "decision rule": {
            "translated_key": "reglas de decisión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on <br>decision rule</br>s identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some <br>decision rule</br>s, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on <br>decision rule</br>s identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some <br>decision rule</br>s, several outranking relations can be defined."
            ],
            "translated_annotated_samples": [
                "En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en <br>reglas de decisión</br> que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro.",
                "Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas <br>reglas de decisión</br>, se pueden definir varias relaciones de prelación."
            ],
            "translated_text": "Un enfoque de clasificación para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en <br>reglas de decisión</br> que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1]. El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23]. La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida al ranking exacto de los elementos. Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones. El resto del documento está organizado de la siguiente manera. Primero revisamos los métodos actuales de agregación de rangos en la Sección 2. Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR. Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2. TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar. Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme. Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos. Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n). Por lo tanto, di j di significa que di está clasificado mejor que di en j. Cuando Dj = D, se dice que j es una lista completa. De lo contrario, es una lista parcial. Si di pertenece a Dj, rj i denota la clasificación o posición de di en j. Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n). Restringir PR a los rankings que contienen el documento di define PRi. También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19]. El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di. Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos. Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28]. Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones. Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ. Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente. CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango. Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás. Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j). Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i . Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican según estas probabilidades. Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j). Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k. Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes. Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares. Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman. Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. 3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas. Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates. En este caso, llamamos a tales clasificaciones semiórdenes. Esto fue descrito en [13] como el problema de la agregación con empates. Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares. Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales. Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada. Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1. Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada. Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2. Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1). Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada. De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos. También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3. Algunos documentos candidatos faltan en algunas clasificaciones de entrada. Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición. H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4. Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso. En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos. En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4. Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal. Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes. Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso. Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates. Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio. Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías). Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos. La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas <br>reglas de decisión</br>, se pueden definir varias relaciones de prelación. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia. Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv. Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n). También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29]. Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en r clases clasificadas. Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria. Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk. Cada clase Ch resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \\ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1. Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1. Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4). Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente. Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia. Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|. Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅. Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple. Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}. Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \\ C1 = {d4, d5}. Son respectivamente 1 y -1. Entonces C3 = E1 = {d4}. El último documento d5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos. En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10]. En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004. Las actuaciones de estas carreras se informan en la tabla 3. Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediana) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10. Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos. En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del enfoque de clasificación cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del enfoque de clasificación con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada. Configuramos nuestro módulo de ejecución básico con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%). Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada. Por lo tanto, pueden variar de un ranking a otro. Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi. Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0). Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones. Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones. Después, estudiamos el impacto de ajustar los parámetros. Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del enfoque de clasificación por jerarquías bajo diferentes hipótesis de trabajo. En la Tabla 4: Impacto de las suposiciones de trabajo, se muestra que en la ejecución mcm22, en la que los documentos faltantes se colocan todos en la misma última posición de cada clasificación de entrada, se produce una disminución en el rendimiento con respecto a la ejecución mcm. Además, S@1 pasa de 41.33% a 34.67% (-16.11%). Esto muestra que varios documentos relevantes que inicialmente se ubicaron en la primera posición del ranking de consenso en mcm, pierden esta primera posición pero siguen clasificados en los 5 primeros documentos, ya que S@5 no cambió. También concluimos que los documentos que tienen posiciones bastante buenas en algunas clasificaciones de entrada son más propensos a ser relevantes, aunque falten en otras clasificaciones. Por consiguiente, cuando faltan en ciertas clasificaciones, asignarles rangos más bajos a estos documentos es perjudicial para el rendimiento. Además, a partir de la Tabla 4, encontramos que las actuaciones de las ejecuciones mcm y mcm23 son similares. Por lo tanto, el enfoque de clasificación no está sujeto a mantener las posiciones iniciales de los documentos candidatos o a recalcularlas descartando los excluidos. De la misma Tabla 4, el rendimiento del enfoque de clasificación aumenta significativamente para las ejecuciones mcm24 y mcm25. Por lo tanto, ya sea que consideremos todos los documentos presentes en la mitad de las clasificaciones (mcm24) o consideremos todos los documentos clasificados en las primeras 100 posiciones en una o más clasificaciones (mcm25), se incrementan los rendimientos. Este resultado era predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos. Las tablas 5 y 6 confirman esta evidencia. La Tabla 5, donde los valores entre corchetes de la primera columna indican el número de documentos que se retienen de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce a un aumento en el rendimiento. Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento. Tabla 5: Impacto del número de documentos retenidos. Identificador de ejecución MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% La Tabla 6 informa ejecuciones correspondientes a variaciones de H2 k. Los valores entre corchetes son éxitos de rango. Por ejemplo, en la ejecución mcm32, solo se consideraron exitosos los documentos que estaban presentes en 3 o más clasificaciones de entrada. Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos son descartados. Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos de RI. Tabla 6: Rendimiento considerando diferentes éxitos de rango. Identificación de ejecución MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% Para las ejecuciones mcm24 y mcm25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo por consulta aumentó y se situó en alrededor de 5 segundos. 5.2.2 Impacto de la Variación de los Parámetros. La Tabla 7 muestra la variación de rendimiento del enfoque de clasificación por preferencias cuando se consideran diferentes umbrales de preferencia. Encontramos una mejora en el rendimiento hasta valores de umbral de aproximadamente el 5%, luego hay una disminución en el rendimiento que se vuelve significativa para valores de umbral superiores al 10%. Además, S@1 mejora del 41.33% al 46.67% cuando el umbral de preferencia cambia de 0 a 5%. Por lo tanto, podemos concluir que las clasificaciones de entrada son semiordeles en lugar de órdenes completos. La Tabla 8 muestra la evolución de las medidas de rendimiento con respecto al umbral de concordancia. Podemos concluir que para colocar el documento di antes de di en la clasificación de consenso, en la Tabla 7: Impacto de la variación del umbral de preferencia del 0 al 12.5%. Ejecutar Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% al menos la mitad de las clasificaciones de entrada de PRi ∩ PRi deben ser concordantes. El rendimiento disminuye significativamente para valores muy bajos y muy altos del umbral de concordancia. De hecho, para tales valores, la condición de concordancia se cumple o bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente. Por lo tanto, la relación de clasificación se vuelve demasiado débil o demasiado fuerte respectivamente. Tabla 8: Impacto de la variación de cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% En los experimentos, variar el umbral de veto, así como el umbral de discordancia dentro de intervalos razonables, no tiene un impacto significativo en las medidas de rendimiento. De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron un rendimiento similar, aunque hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada veten fácilmente. Además, el ajuste del umbral de discordancia se realizó para valores del 50% y 75% del umbral de veto. Para estas ejecuciones no observamos ninguna variación de rendimiento notable, aunque para umbrales de discordancia bajos (dmax < 20%), el rendimiento disminuyó ligeramente. 5.2.3 Impacto de la Variación del Número de Clasificaciones de Entrada Para estudiar la evolución del rendimiento cuando se consideran diferentes conjuntos de clasificaciones de entrada, realizamos tres ejecuciones adicionales donde se consideran 2, 4 y 6 de los conjuntos de clasificaciones de entrada con mejor rendimiento. Los resultados reportados en la Tabla 9 parecen ser contraintuitivos y tampoco respaldan hallazgos previos en la investigación sobre la agregación de rangos [3]. Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento aportan más ruido que información para establecer la clasificación de consenso. Por lo tanto, cuando se consideran, el rendimiento disminuye. Tabla 9: Rendimiento considerando diferentes conjuntos de clasificaciones de entrada con mejor rendimiento. Identificador de ejecución MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de clasificaciones. En este conjunto de ejecuciones, comparamos el enfoque de clasificación con algunos métodos de agregación de clasificaciones estándar que han demostrado tener un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias CombSUM y CombMNZ. También examinamos el rendimiento de un método mayoritario que es el método de la cadena de Markov (MC4). Para las comparaciones, consideramos una relación de superación específica S∗ = S(5%,50%,50%,30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros. La primera fila de la Tabla 10 muestra el rendimiento de los métodos de agregación de rangos con respecto a un conjunto de suposiciones básicas A1 = (H1 100, H2 5, H4 nuevo): solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos los documentos presentes en 5 o más clasificaciones y actualizamos los rangos de los documentos exitosos. Para los métodos posicionales, colocamos los documentos faltantes en la cola de la clasificación (H3 sí), mientras que para nuestro método, al igual que para MC4, conservamos la hipótesis H3 no. Las tres filas siguientes de la Tabla 10 informan sobre los rendimientos al cambiar un elemento del conjunto de suposiciones básicas: la segunda fila corresponde al conjunto de suposiciones A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a 1000. La tercera fila corresponde al conjunto de supuestos A3 = (H1 100, H2 todos, H4 nuevos), es decir, considerando los documentos presentes en al menos un ranking. La cuarta fila corresponde al conjunto de supuestos A4 = (H1 100, H2 5, H4 init), es decir, manteniendo los rangos originales de los documentos exitosos. La quinta fila de la Tabla 10, etiquetada como A5, muestra el rendimiento cuando se consideran todas las 225 consultas de la pista web de TREC-2004. Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de Página de Inicio y Página Nombrada [10]) de la pista web TREC-2004. Este conjunto de pruebas tiene como objetivo demostrar si el rendimiento relativo de los diferentes métodos depende de la tarea. La última fila de la Tabla 10, etiquetada como A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de suposiciones A1 de la primera fila. Esto tiene como objetivo mostrar si el rendimiento relativo de los diferentes métodos cambia de un año a otro. Los valores entre corchetes de la Tabla 10 son variaciones del rendimiento de cada método de agregación de rangos con respecto al rendimiento del enfoque de superación. Tabla 10: Rendimiento (MAP) de diferentes métodos de agregación de rangos bajo 3 colecciones de pruebas diferentes mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) Del análisis de la tabla 10 se puede establecer lo siguiente: • para todas las ejecuciones, considerar todos los documentos en cada clasificación de entrada (A2) mejora significativamente el rendimiento (MAP aumenta en promedio un 11.62%). Esto es predecible ya que algunos documentos relevantes inicialmente no reportados recibirían mejores posiciones en la clasificación de consenso. • para todas las ejecuciones, considerar documentos incluso aquellos presentes en solo una clasificación de entrada (A3) mejora significativamente el rendimiento. Para mcm, combSUM y combMNZ, la mejora del rendimiento es más importante (el MAP aumenta en promedio un 20.27%) que para la ejecución de Markov (el MAP aumenta un 3.86%). • preservar las posiciones iniciales de los documentos (A4) o volver a calcularlas (A1) no tiene una influencia notable en el rendimiento para ambos métodos posicional y mayoritario. • considerar todas las consultas de la pista web de TREC2004 (A5) así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos. • considerando las consultas TD de la pista web de TREC2002, los rendimientos de todos los métodos de fusión de datos son más bajos que el del mejor ranking de entrada que tiene un valor de MAP de 18.58%. Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con la mejor, lo que añade más ruido a la clasificación de consenso. • los rendimientos de los métodos de fusión de datos mcm y markov son significativamente mejores que el de la mejor clasificación de entrada uogWebCAU150. Esto sigue siendo cierto solo para las ejecuciones combSUM y combMNZ bajo las suposiciones H1 todas o H2 todas. Esto demuestra que los métodos mayoritarios son menos sensibles a suposiciones que los métodos posicionales. El enfoque de superación siempre tiene un rendimiento significativamente mejor que los métodos posicionales combSUM y combMNZ. También tiene un mejor rendimiento que el método de la cadena de Markov, especialmente bajo la suposición H2 donde la diferencia de rendimientos se vuelve significativa. 6. CONCLUSIONES En este artículo, abordamos el problema de agregación de rangos donde se deben fusionar listas de documentos diferentes, pero no disjuntas. Notamos que las clasificaciones de entrada pueden ocultar empates, por lo que no deben considerarse como órdenes completos. Solo se debe utilizar información sólida de cada clasificación de entrada. Los métodos actuales de agregación de rangos, y especialmente los métodos posicionales (por ejemplo, combSUM [15]), no fueron diseñados inicialmente para trabajar con tales clasificaciones. Deben adaptarse teniendo en cuenta supuestos de trabajo específicos. Proponemos un nuevo método de clasificación para la agregación de rangos que está bien adaptado al contexto de la RI. De hecho, clasifica dos documentos con respecto a la intensidad de la diferencia de sus posiciones en cada clasificación de entrada y también considera el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico. Tampoco es necesario hacer suposiciones específicas sobre las posiciones de los documentos faltantes. Esta es una característica importante, ya que la ausencia de un documento en un ranking no necesariamente debe interpretarse de forma negativa. Los resultados experimentales muestran que el método de clasificación supera significativamente a los populares métodos clásicos de fusión de datos posicionales como las estrategias combSUM y combMNZ. También supera en rendimiento a los métodos mayoritarios de buen rendimiento, como el método de la cadena de Markov. Estos resultados se prueban con diferentes colecciones de pruebas y consultas. De los experimentos, también podemos concluir que para mejorar el rendimiento, deberíamos fusionar listas de resultados de modelos de IR con buen desempeño, y que los métodos de fusión de datos mayoritarios funcionan mejor que los métodos posicionales. El método propuesto puede tener un impacto real en el rendimiento de la metabúsqueda web, ya que la mayoría de los motores de búsqueda primarios solo proporcionan clasificaciones, mientras que la mayoría de los enfoques actuales necesitan puntuaciones para fusionar las listas de resultados en una sola lista. El trabajo adicional implica investigar si el enfoque de clasificación por jerarquía funciona bien en varios otros contextos, por ejemplo, utilizando las puntuaciones de los documentos o alguna combinación de los rangos y puntuaciones de los documentos. Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este artículo. 7. REFERENCIAS [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur. Fusión de enfoques intensivos en conocimiento y estadísticos para recuperar y anotar documentos genómicos textuales. En Actas de TREC2005. Publicación del NIST, 2005. [2] R. A. Baeza-Yates y B. A. Ribeiro-Neto. Recuperación de información moderna. ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de múltiples sistemas de recuperación clasificados. En Actas ACM-SIGIR94, páginas 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. \n\nSpringer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, y J. A. Shaw. Combinando evidencia de múltiples representaciones de consulta para la recuperación de información. IPM, 31(3):431-448, 1995. [5] J. Borda. \n\nIPM, 31(3):431-448, 1995. [5] J. Borda. Memoria sobre las elecciones por voto secreto. Historia de la Academia de Ciencias, 1781. [6] J. P. Callan, Z. Lu y W. B. Croft. Buscando colecciones distribuidas con redes de inferencia. En Actas ACM-SIGIR95, páginas 21-28, 1995. [7] M. Condorcet. Ensayo sobre la aplicación del análisis de probabilidad a las decisiones tomadas por mayoría de votos. Imprimerie Royale, París, 1785. [8] W. D. Cook y M. Kress. Clasificación ordinal con intensidad de preferencia. Ciencia de la Gestión, 31(1):26-32, 1985. [9] N. Craswell y D. Hawking. Resumen de la pista web TREC-2002. En Actas de TREC2002. Publicación del NIST, 2002. [10] N. Craswell y D. Hawking. Resumen de la TREC-2004 Web Track. En Actas de TREC2004. Publicación del NIST, 2004. [11] C. Dwork, S. R. Kumar, M. Naor y D. Sivakumar. Métodos de agregación de clasificaciones para la Web. En Actas WWW2001, páginas 613-622, 2001. [12] R. Fagin. Combinando información difusa de múltiples sistemas. JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar y E. Vee. Comparando y agregando clasificaciones con empates. En PODS, páginas 47-58, 2004. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas de los k mejores. SIAM J. en Matemáticas Discretas, 17(1):134-160, 2003. [15] E. A. Zorro y J. A. Shaw. Combinación de múltiples búsquedas. En Actas de TREC3. Publicación del NIST, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes y P. DasGupta. Un estudio de la superposición entre representaciones de documentos. Tecnología de la Información: Investigación y Desarrollo, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell y J. Callan. Selección de colecciones y fusión de resultados con patentes de EE. UU. organizadas por tema y datos de TREC. En las Actas ACM-CIKM2000, páginas 282-289. ACM Press, 2000. [18] A.\nACM Press, 2000. [18] A. Le Calv´e y J. Savoy. Estrategia de fusión de bases de datos basada en regresión logística. IPM, 36(3):341-359, 2000. [19] J. H. Lee. \n\nIPM, 36(3):341-359, 2000. [19] J. H. Lee. Análisis de la combinación de múltiples evidencias. En Actas ACM-SIGIR97, páginas 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier y J. Dunnion. Probfuse: un enfoque probabilístico para la fusión de datos. En las Actas ACM-SIGIR2006, páginas 139-146. ACM Press, 2006. [21] J. I. Marden. \n\nACM Press, 2006. [21] J. I. Marden. Analizando y modelando datos de rango. Número 64 en Monografías sobre Estadística y Probabilidad Aplicada. Chapman & Hall, 1995. [22] M. Montague y J. A. Aslam. Consistencia de la metabúsqueda. En las Actas ACM-SIGIR2001, páginas 386-387. ACM Press, 2001. [23] D. M. Pennock y E. Horvitz. Análisis de los fundamentos axiomáticos del filtrado colaborativo. En el Taller sobre Inteligencia Artificial para el Comercio Electrónico en la 16ª Conferencia Nacional de Inteligencia Artificial, 1999. [24] M. E. Renda y U. Straccia. Búsqueda web metasearch: métodos de agregación de rango basados en rango vs. puntuación. En las Actas de ACM-SAC2003, páginas 841-846. ACM Press, 2003. [25] W. H. Riker. \n\nACM Press, 2003. [25] W. H. Riker. Liberalismo contra populismo. Waveland Press, 1982. [26] B. Roy. \n\nWaveland Press, 1982. [26] B. Roy. El enfoque de jerarquización y los fundamentos de los métodos ELECTRE. Teoría y decisión, 31:49-73, 1991. [27] B. Roy y J. Hugonnard. Clasificación de proyectos de extensión de líneas suburbanas en el sistema de metro de París mediante un método multicriterio. Investigación en Transporte, 16A(4):301-312, 1982. [28] L. Si y J. Callan. Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En las Actas ACM-SIGIR2002, páginas 19-26. ACM Press, 2002. [29] M. Truchon. \n\nACM Press, 2002. [29] M. Truchon. Una extensión del criterio de Condorcet y órdenes de Kemeny. Cuaderno 9813, Centro de Investigación en Economía y Finanzas Aplicadas, octubre de 1998. [30] H. Turtle y W. B. Croft. Redes de inferencia para la recuperación de documentos. En Actas de ACM-SIGIR90, páginas 1-24. ACM Press, 1990. [31] C. C. Vogt y G. W. Cottrell. Fusión a través de una combinación lineal de puntuaciones. Recuperación de información, 1(3):151-173, 1999. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "outranking approach": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An <br>outranking approach</br> for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "<br>outranking approach</br> FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our <br>outranking approach</br> for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the <br>outranking approach</br> when tuning the parameters and working assumptions, ii) compare performances of the <br>outranking approach</br> vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the <br>outranking approach</br> under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the <br>outranking approach</br> is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the <br>outranking approach</br> increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the <br>outranking approach</br> when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the <br>outranking approach</br> with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the <br>outranking approach</br>.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • <br>outranking approach</br> always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the <br>outranking approach</br> performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The <br>outranking approach</br> and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [
                "An <br>outranking approach</br> for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "<br>outranking approach</br> FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our <br>outranking approach</br> for rank aggregation.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the <br>outranking approach</br> when tuning the parameters and working assumptions, ii) compare performances of the <br>outranking approach</br> vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the <br>outranking approach</br> under different working hypotheses."
            ],
            "translated_annotated_samples": [
                "Un <br>enfoque de clasificación</br> para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.).",
                "Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal.",
                "EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro <br>enfoque de clasificación</br> para la agregación de rangos.",
                "Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del <br>enfoque de clasificación</br> cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del <br>enfoque de clasificación</br> con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada.",
                "Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del <br>enfoque de clasificación por jerarquías</br> bajo diferentes hipótesis de trabajo."
            ],
            "translated_text": "Un <br>enfoque de clasificación</br> para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1]. El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23]. La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida al ranking exacto de los elementos. Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones. El resto del documento está organizado de la siguiente manera. Primero revisamos los métodos actuales de agregación de rangos en la Sección 2. Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR. Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2. TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar. Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme. Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos. Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n). Por lo tanto, di j di significa que di está clasificado mejor que di en j. Cuando Dj = D, se dice que j es una lista completa. De lo contrario, es una lista parcial. Si di pertenece a Dj, rj i denota la clasificación o posición de di en j. Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n). Restringir PR a los rankings que contienen el documento di define PRi. También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19]. El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di. Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos. Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28]. Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones. Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ. Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente. CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango. Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás. Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j). Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i . Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican según estas probabilidades. Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j). Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k. Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes. Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares. Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman. Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. 3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas. Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates. En este caso, llamamos a tales clasificaciones semiórdenes. Esto fue descrito en [13] como el problema de la agregación con empates. Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares. Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales. Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada. Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1. Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada. Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2. Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1). Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada. De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos. También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3. Algunos documentos candidatos faltan en algunas clasificaciones de entrada. Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición. H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4. Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso. En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos. En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4. Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal. Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes. Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso. Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates. Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio. Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías). Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos. La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones de prelación. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia. Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv. Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n). También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29]. Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en r clases clasificadas. Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria. Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk. Cada clase Ch resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \\ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1. Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1. Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4). Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente. Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia. Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|. Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅. Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple. Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}. Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \\ C1 = {d4, d5}. Son respectivamente 1 y -1. Entonces C3 = E1 = {d4}. El último documento d5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro <br>enfoque de clasificación</br> para la agregación de rangos. En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10]. En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004. Las actuaciones de estas carreras se informan en la tabla 3. Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediana) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10. Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos. En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del <br>enfoque de clasificación</br> cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del <br>enfoque de clasificación</br> con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada. Configuramos nuestro módulo de ejecución básico con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%). Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada. Por lo tanto, pueden variar de un ranking a otro. Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi. Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0). Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones. Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones. Después, estudiamos el impacto de ajustar los parámetros. Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del <br>enfoque de clasificación por jerarquías</br> bajo diferentes hipótesis de trabajo. ",
            "candidates": [],
            "error": [
                [
                    "enfoque de clasificación",
                    "enfoque de clasificación",
                    "enfoque de clasificación",
                    "enfoque de clasificación",
                    "enfoque de clasificación por jerarquías"
                ]
            ]
        },
        "metasearch engine": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype <br>metasearch engine</br> that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype <br>metasearch engine</br> that implements a version of our outranking approach for rank aggregation."
            ],
            "translated_annotated_samples": [
                "EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos."
            ],
            "translated_text": "Un enfoque de clasificación para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1]. El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23]. La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida al ranking exacto de los elementos. Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones. El resto del documento está organizado de la siguiente manera. Primero revisamos los métodos actuales de agregación de rangos en la Sección 2. Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR. Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2. TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar. Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme. Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos. Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n). Por lo tanto, di j di significa que di está clasificado mejor que di en j. Cuando Dj = D, se dice que j es una lista completa. De lo contrario, es una lista parcial. Si di pertenece a Dj, rj i denota la clasificación o posición de di en j. Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n). Restringir PR a los rankings que contienen el documento di define PRi. También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19]. El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di. Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos. Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28]. Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones. Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ. Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente. CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango. Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás. Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j). Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i . Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican según estas probabilidades. Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j). Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k. Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes. Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares. Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman. Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. 3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas. Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates. En este caso, llamamos a tales clasificaciones semiórdenes. Esto fue descrito en [13] como el problema de la agregación con empates. Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares. Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales. Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada. Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1. Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada. Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2. Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1). Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada. De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos. También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3. Algunos documentos candidatos faltan en algunas clasificaciones de entrada. Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición. H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4. Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso. En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos. En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4. Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal. Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes. Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso. Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates. Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio. Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías). Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos. La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones de prelación. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia. Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv. Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n). También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29]. Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en r clases clasificadas. Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria. Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk. Cada clase Ch resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \\ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1. Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1. Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4). Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente. Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia. Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|. Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅. Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple. Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}. Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \\ C1 = {d4, d5}. Son respectivamente 1 y -1. Entonces C3 = E1 = {d4}. El último documento d5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos. En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10]. En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004. Las actuaciones de estas carreras se informan en la tabla 3. Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediana) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10. Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos. En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del enfoque de clasificación cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del enfoque de clasificación con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada. Configuramos nuestro módulo de ejecución básico con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%). Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada. Por lo tanto, pueden variar de un ranking a otro. Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi. Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0). Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones. Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones. Después, estudiamos el impacto de ajustar los parámetros. Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del enfoque de clasificación por jerarquías bajo diferentes hipótesis de trabajo. En la Tabla 4: Impacto de las suposiciones de trabajo, se muestra que en la ejecución mcm22, en la que los documentos faltantes se colocan todos en la misma última posición de cada clasificación de entrada, se produce una disminución en el rendimiento con respecto a la ejecución mcm. Además, S@1 pasa de 41.33% a 34.67% (-16.11%). Esto muestra que varios documentos relevantes que inicialmente se ubicaron en la primera posición del ranking de consenso en mcm, pierden esta primera posición pero siguen clasificados en los 5 primeros documentos, ya que S@5 no cambió. También concluimos que los documentos que tienen posiciones bastante buenas en algunas clasificaciones de entrada son más propensos a ser relevantes, aunque falten en otras clasificaciones. Por consiguiente, cuando faltan en ciertas clasificaciones, asignarles rangos más bajos a estos documentos es perjudicial para el rendimiento. Además, a partir de la Tabla 4, encontramos que las actuaciones de las ejecuciones mcm y mcm23 son similares. Por lo tanto, el enfoque de clasificación no está sujeto a mantener las posiciones iniciales de los documentos candidatos o a recalcularlas descartando los excluidos. De la misma Tabla 4, el rendimiento del enfoque de clasificación aumenta significativamente para las ejecuciones mcm24 y mcm25. Por lo tanto, ya sea que consideremos todos los documentos presentes en la mitad de las clasificaciones (mcm24) o consideremos todos los documentos clasificados en las primeras 100 posiciones en una o más clasificaciones (mcm25), se incrementan los rendimientos. Este resultado era predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos. Las tablas 5 y 6 confirman esta evidencia. La Tabla 5, donde los valores entre corchetes de la primera columna indican el número de documentos que se retienen de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce a un aumento en el rendimiento. Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento. Tabla 5: Impacto del número de documentos retenidos. Identificador de ejecución MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% La Tabla 6 informa ejecuciones correspondientes a variaciones de H2 k. Los valores entre corchetes son éxitos de rango. Por ejemplo, en la ejecución mcm32, solo se consideraron exitosos los documentos que estaban presentes en 3 o más clasificaciones de entrada. Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos son descartados. Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos de RI. Tabla 6: Rendimiento considerando diferentes éxitos de rango. Identificación de ejecución MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% Para las ejecuciones mcm24 y mcm25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo por consulta aumentó y se situó en alrededor de 5 segundos. 5.2.2 Impacto de la Variación de los Parámetros. La Tabla 7 muestra la variación de rendimiento del enfoque de clasificación por preferencias cuando se consideran diferentes umbrales de preferencia. Encontramos una mejora en el rendimiento hasta valores de umbral de aproximadamente el 5%, luego hay una disminución en el rendimiento que se vuelve significativa para valores de umbral superiores al 10%. Además, S@1 mejora del 41.33% al 46.67% cuando el umbral de preferencia cambia de 0 a 5%. Por lo tanto, podemos concluir que las clasificaciones de entrada son semiordeles en lugar de órdenes completos. La Tabla 8 muestra la evolución de las medidas de rendimiento con respecto al umbral de concordancia. Podemos concluir que para colocar el documento di antes de di en la clasificación de consenso, en la Tabla 7: Impacto de la variación del umbral de preferencia del 0 al 12.5%. Ejecutar Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% al menos la mitad de las clasificaciones de entrada de PRi ∩ PRi deben ser concordantes. El rendimiento disminuye significativamente para valores muy bajos y muy altos del umbral de concordancia. De hecho, para tales valores, la condición de concordancia se cumple o bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente. Por lo tanto, la relación de clasificación se vuelve demasiado débil o demasiado fuerte respectivamente. Tabla 8: Impacto de la variación de cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% En los experimentos, variar el umbral de veto, así como el umbral de discordancia dentro de intervalos razonables, no tiene un impacto significativo en las medidas de rendimiento. De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron un rendimiento similar, aunque hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada veten fácilmente. Además, el ajuste del umbral de discordancia se realizó para valores del 50% y 75% del umbral de veto. Para estas ejecuciones no observamos ninguna variación de rendimiento notable, aunque para umbrales de discordancia bajos (dmax < 20%), el rendimiento disminuyó ligeramente. 5.2.3 Impacto de la Variación del Número de Clasificaciones de Entrada Para estudiar la evolución del rendimiento cuando se consideran diferentes conjuntos de clasificaciones de entrada, realizamos tres ejecuciones adicionales donde se consideran 2, 4 y 6 de los conjuntos de clasificaciones de entrada con mejor rendimiento. Los resultados reportados en la Tabla 9 parecen ser contraintuitivos y tampoco respaldan hallazgos previos en la investigación sobre la agregación de rangos [3]. Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento aportan más ruido que información para establecer la clasificación de consenso. Por lo tanto, cuando se consideran, el rendimiento disminuye. Tabla 9: Rendimiento considerando diferentes conjuntos de clasificaciones de entrada con mejor rendimiento. Identificador de ejecución MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de clasificaciones. En este conjunto de ejecuciones, comparamos el enfoque de clasificación con algunos métodos de agregación de clasificaciones estándar que han demostrado tener un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias CombSUM y CombMNZ. También examinamos el rendimiento de un método mayoritario que es el método de la cadena de Markov (MC4). Para las comparaciones, consideramos una relación de superación específica S∗ = S(5%,50%,50%,30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros. La primera fila de la Tabla 10 muestra el rendimiento de los métodos de agregación de rangos con respecto a un conjunto de suposiciones básicas A1 = (H1 100, H2 5, H4 nuevo): solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos los documentos presentes en 5 o más clasificaciones y actualizamos los rangos de los documentos exitosos. Para los métodos posicionales, colocamos los documentos faltantes en la cola de la clasificación (H3 sí), mientras que para nuestro método, al igual que para MC4, conservamos la hipótesis H3 no. Las tres filas siguientes de la Tabla 10 informan sobre los rendimientos al cambiar un elemento del conjunto de suposiciones básicas: la segunda fila corresponde al conjunto de suposiciones A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a 1000. La tercera fila corresponde al conjunto de supuestos A3 = (H1 100, H2 todos, H4 nuevos), es decir, considerando los documentos presentes en al menos un ranking. La cuarta fila corresponde al conjunto de supuestos A4 = (H1 100, H2 5, H4 init), es decir, manteniendo los rangos originales de los documentos exitosos. La quinta fila de la Tabla 10, etiquetada como A5, muestra el rendimiento cuando se consideran todas las 225 consultas de la pista web de TREC-2004. Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de Página de Inicio y Página Nombrada [10]) de la pista web TREC-2004. Este conjunto de pruebas tiene como objetivo demostrar si el rendimiento relativo de los diferentes métodos depende de la tarea. La última fila de la Tabla 10, etiquetada como A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de suposiciones A1 de la primera fila. Esto tiene como objetivo mostrar si el rendimiento relativo de los diferentes métodos cambia de un año a otro. Los valores entre corchetes de la Tabla 10 son variaciones del rendimiento de cada método de agregación de rangos con respecto al rendimiento del enfoque de superación. Tabla 10: Rendimiento (MAP) de diferentes métodos de agregación de rangos bajo 3 colecciones de pruebas diferentes mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) Del análisis de la tabla 10 se puede establecer lo siguiente: • para todas las ejecuciones, considerar todos los documentos en cada clasificación de entrada (A2) mejora significativamente el rendimiento (MAP aumenta en promedio un 11.62%). Esto es predecible ya que algunos documentos relevantes inicialmente no reportados recibirían mejores posiciones en la clasificación de consenso. • para todas las ejecuciones, considerar documentos incluso aquellos presentes en solo una clasificación de entrada (A3) mejora significativamente el rendimiento. Para mcm, combSUM y combMNZ, la mejora del rendimiento es más importante (el MAP aumenta en promedio un 20.27%) que para la ejecución de Markov (el MAP aumenta un 3.86%). • preservar las posiciones iniciales de los documentos (A4) o volver a calcularlas (A1) no tiene una influencia notable en el rendimiento para ambos métodos posicional y mayoritario. • considerar todas las consultas de la pista web de TREC2004 (A5) así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos. • considerando las consultas TD de la pista web de TREC2002, los rendimientos de todos los métodos de fusión de datos son más bajos que el del mejor ranking de entrada que tiene un valor de MAP de 18.58%. Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con la mejor, lo que añade más ruido a la clasificación de consenso. • los rendimientos de los métodos de fusión de datos mcm y markov son significativamente mejores que el de la mejor clasificación de entrada uogWebCAU150. Esto sigue siendo cierto solo para las ejecuciones combSUM y combMNZ bajo las suposiciones H1 todas o H2 todas. Esto demuestra que los métodos mayoritarios son menos sensibles a suposiciones que los métodos posicionales. El enfoque de superación siempre tiene un rendimiento significativamente mejor que los métodos posicionales combSUM y combMNZ. También tiene un mejor rendimiento que el método de la cadena de Markov, especialmente bajo la suposición H2 donde la diferencia de rendimientos se vuelve significativa. 6. CONCLUSIONES En este artículo, abordamos el problema de agregación de rangos donde se deben fusionar listas de documentos diferentes, pero no disjuntas. Notamos que las clasificaciones de entrada pueden ocultar empates, por lo que no deben considerarse como órdenes completos. Solo se debe utilizar información sólida de cada clasificación de entrada. Los métodos actuales de agregación de rangos, y especialmente los métodos posicionales (por ejemplo, combSUM [15]), no fueron diseñados inicialmente para trabajar con tales clasificaciones. Deben adaptarse teniendo en cuenta supuestos de trabajo específicos. Proponemos un nuevo método de clasificación para la agregación de rangos que está bien adaptado al contexto de la RI. De hecho, clasifica dos documentos con respecto a la intensidad de la diferencia de sus posiciones en cada clasificación de entrada y también considera el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico. Tampoco es necesario hacer suposiciones específicas sobre las posiciones de los documentos faltantes. Esta es una característica importante, ya que la ausencia de un documento en un ranking no necesariamente debe interpretarse de forma negativa. Los resultados experimentales muestran que el método de clasificación supera significativamente a los populares métodos clásicos de fusión de datos posicionales como las estrategias combSUM y combMNZ. También supera en rendimiento a los métodos mayoritarios de buen rendimiento, como el método de la cadena de Markov. Estos resultados se prueban con diferentes colecciones de pruebas y consultas. De los experimentos, también podemos concluir que para mejorar el rendimiento, deberíamos fusionar listas de resultados de modelos de IR con buen desempeño, y que los métodos de fusión de datos mayoritarios funcionan mejor que los métodos posicionales. El método propuesto puede tener un impacto real en el rendimiento de la metabúsqueda web, ya que la mayoría de los motores de búsqueda primarios solo proporcionan clasificaciones, mientras que la mayoría de los enfoques actuales necesitan puntuaciones para fusionar las listas de resultados en una sola lista. El trabajo adicional implica investigar si el enfoque de clasificación por jerarquía funciona bien en varios otros contextos, por ejemplo, utilizando las puntuaciones de los documentos o alguna combinación de los rangos y puntuaciones de los documentos. Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este artículo. 7. REFERENCIAS [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur. Fusión de enfoques intensivos en conocimiento y estadísticos para recuperar y anotar documentos genómicos textuales. En Actas de TREC2005. Publicación del NIST, 2005. [2] R. A. Baeza-Yates y B. A. Ribeiro-Neto. Recuperación de información moderna. ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de múltiples sistemas de recuperación clasificados. En Actas ACM-SIGIR94, páginas 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. \n\nSpringer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, y J. A. Shaw. Combinando evidencia de múltiples representaciones de consulta para la recuperación de información. IPM, 31(3):431-448, 1995. [5] J. Borda. \n\nIPM, 31(3):431-448, 1995. [5] J. Borda. Memoria sobre las elecciones por voto secreto. Historia de la Academia de Ciencias, 1781. [6] J. P. Callan, Z. Lu y W. B. Croft. Buscando colecciones distribuidas con redes de inferencia. En Actas ACM-SIGIR95, páginas 21-28, 1995. [7] M. Condorcet. Ensayo sobre la aplicación del análisis de probabilidad a las decisiones tomadas por mayoría de votos. Imprimerie Royale, París, 1785. [8] W. D. Cook y M. Kress. Clasificación ordinal con intensidad de preferencia. Ciencia de la Gestión, 31(1):26-32, 1985. [9] N. Craswell y D. Hawking. Resumen de la pista web TREC-2002. En Actas de TREC2002. Publicación del NIST, 2002. [10] N. Craswell y D. Hawking. Resumen de la TREC-2004 Web Track. En Actas de TREC2004. Publicación del NIST, 2004. [11] C. Dwork, S. R. Kumar, M. Naor y D. Sivakumar. Métodos de agregación de clasificaciones para la Web. En Actas WWW2001, páginas 613-622, 2001. [12] R. Fagin. Combinando información difusa de múltiples sistemas. JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar y E. Vee. Comparando y agregando clasificaciones con empates. En PODS, páginas 47-58, 2004. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas de los k mejores. SIAM J. en Matemáticas Discretas, 17(1):134-160, 2003. [15] E. A. Zorro y J. A. Shaw. Combinación de múltiples búsquedas. En Actas de TREC3. Publicación del NIST, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes y P. DasGupta. Un estudio de la superposición entre representaciones de documentos. Tecnología de la Información: Investigación y Desarrollo, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell y J. Callan. Selección de colecciones y fusión de resultados con patentes de EE. UU. organizadas por tema y datos de TREC. En las Actas ACM-CIKM2000, páginas 282-289. ACM Press, 2000. [18] A.\nACM Press, 2000. [18] A. Le Calv´e y J. Savoy. Estrategia de fusión de bases de datos basada en regresión logística. IPM, 36(3):341-359, 2000. [19] J. H. Lee. \n\nIPM, 36(3):341-359, 2000. [19] J. H. Lee. Análisis de la combinación de múltiples evidencias. En Actas ACM-SIGIR97, páginas 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier y J. Dunnion. Probfuse: un enfoque probabilístico para la fusión de datos. En las Actas ACM-SIGIR2006, páginas 139-146. ACM Press, 2006. [21] J. I. Marden. \n\nACM Press, 2006. [21] J. I. Marden. Analizando y modelando datos de rango. Número 64 en Monografías sobre Estadística y Probabilidad Aplicada. Chapman & Hall, 1995. [22] M. Montague y J. A. Aslam. Consistencia de la metabúsqueda. En las Actas ACM-SIGIR2001, páginas 386-387. ACM Press, 2001. [23] D. M. Pennock y E. Horvitz. Análisis de los fundamentos axiomáticos del filtrado colaborativo. En el Taller sobre Inteligencia Artificial para el Comercio Electrónico en la 16ª Conferencia Nacional de Inteligencia Artificial, 1999. [24] M. E. Renda y U. Straccia. Búsqueda web metasearch: métodos de agregación de rango basados en rango vs. puntuación. En las Actas de ACM-SAC2003, páginas 841-846. ACM Press, 2003. [25] W. H. Riker. \n\nACM Press, 2003. [25] W. H. Riker. Liberalismo contra populismo. Waveland Press, 1982. [26] B. Roy. \n\nWaveland Press, 1982. [26] B. Roy. El enfoque de jerarquización y los fundamentos de los métodos ELECTRE. Teoría y decisión, 31:49-73, 1991. [27] B. Roy y J. Hugonnard. Clasificación de proyectos de extensión de líneas suburbanas en el sistema de metro de París mediante un método multicriterio. Investigación en Transporte, 16A(4):301-312, 1982. [28] L. Si y J. Callan. Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En las Actas ACM-SIGIR2002, páginas 19-26. ACM Press, 2002. [29] M. Truchon. \n\nACM Press, 2002. [29] M. Truchon. Una extensión del criterio de Condorcet y órdenes de Kemeny. Cuaderno 9813, Centro de Investigación en Economía y Finanzas Aplicadas, octubre de 1998. [30] H. Turtle y W. B. Croft. Redes de inferencia para la recuperación de documentos. En Actas de ACM-SIGIR90, páginas 1-24. ACM Press, 1990. [31] C. C. Vogt y G. W. Cottrell. Fusión a través de una combinación lineal de puntuaciones. Recuperación de información, 1(3):151-173, 1999. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "combsum and combmnz strategy": {
            "translated_key": "estrategia de combsum y combmnz",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "majoritarian method": {
            "translated_key": "método mayoritario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one <br>majoritarian method</br> which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [
                "We also examined the performance of one <br>majoritarian method</br> which is the Markov chain method (MC4)."
            ],
            "translated_annotated_samples": [
                "También examinamos el rendimiento de un <br>método mayoritario</br> que es el método de la cadena de Markov (MC4)."
            ],
            "translated_text": "Un enfoque de clasificación para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1]. El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23]. La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida al ranking exacto de los elementos. Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones. El resto del documento está organizado de la siguiente manera. Primero revisamos los métodos actuales de agregación de rangos en la Sección 2. Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR. Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2. TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar. Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme. Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos. Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n). Por lo tanto, di j di significa que di está clasificado mejor que di en j. Cuando Dj = D, se dice que j es una lista completa. De lo contrario, es una lista parcial. Si di pertenece a Dj, rj i denota la clasificación o posición de di en j. Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n). Restringir PR a los rankings que contienen el documento di define PRi. También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19]. El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di. Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos. Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28]. Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones. Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ. Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente. CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango. Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás. Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j). Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i . Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican según estas probabilidades. Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j). Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k. Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes. Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares. Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman. Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. 3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas. Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates. En este caso, llamamos a tales clasificaciones semiórdenes. Esto fue descrito en [13] como el problema de la agregación con empates. Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares. Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales. Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada. Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1. Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada. Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2. Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1). Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada. De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos. También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3. Algunos documentos candidatos faltan en algunas clasificaciones de entrada. Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición. H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4. Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso. En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos. En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4. Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal. Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes. Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso. Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates. Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio. Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías). Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos. La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones de prelación. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia. Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv. Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n). También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29]. Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en r clases clasificadas. Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria. Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk. Cada clase Ch resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \\ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1. Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1. Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4). Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente. Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia. Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|. Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅. Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple. Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}. Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \\ C1 = {d4, d5}. Son respectivamente 1 y -1. Entonces C3 = E1 = {d4}. El último documento d5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos. En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10]. En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004. Las actuaciones de estas carreras se informan en la tabla 3. Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediana) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10. Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos. En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del enfoque de clasificación cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del enfoque de clasificación con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada. Configuramos nuestro módulo de ejecución básico con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%). Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada. Por lo tanto, pueden variar de un ranking a otro. Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi. Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0). Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones. Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones. Después, estudiamos el impacto de ajustar los parámetros. Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del enfoque de clasificación por jerarquías bajo diferentes hipótesis de trabajo. En la Tabla 4: Impacto de las suposiciones de trabajo, se muestra que en la ejecución mcm22, en la que los documentos faltantes se colocan todos en la misma última posición de cada clasificación de entrada, se produce una disminución en el rendimiento con respecto a la ejecución mcm. Además, S@1 pasa de 41.33% a 34.67% (-16.11%). Esto muestra que varios documentos relevantes que inicialmente se ubicaron en la primera posición del ranking de consenso en mcm, pierden esta primera posición pero siguen clasificados en los 5 primeros documentos, ya que S@5 no cambió. También concluimos que los documentos que tienen posiciones bastante buenas en algunas clasificaciones de entrada son más propensos a ser relevantes, aunque falten en otras clasificaciones. Por consiguiente, cuando faltan en ciertas clasificaciones, asignarles rangos más bajos a estos documentos es perjudicial para el rendimiento. Además, a partir de la Tabla 4, encontramos que las actuaciones de las ejecuciones mcm y mcm23 son similares. Por lo tanto, el enfoque de clasificación no está sujeto a mantener las posiciones iniciales de los documentos candidatos o a recalcularlas descartando los excluidos. De la misma Tabla 4, el rendimiento del enfoque de clasificación aumenta significativamente para las ejecuciones mcm24 y mcm25. Por lo tanto, ya sea que consideremos todos los documentos presentes en la mitad de las clasificaciones (mcm24) o consideremos todos los documentos clasificados en las primeras 100 posiciones en una o más clasificaciones (mcm25), se incrementan los rendimientos. Este resultado era predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos. Las tablas 5 y 6 confirman esta evidencia. La Tabla 5, donde los valores entre corchetes de la primera columna indican el número de documentos que se retienen de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce a un aumento en el rendimiento. Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento. Tabla 5: Impacto del número de documentos retenidos. Identificador de ejecución MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% La Tabla 6 informa ejecuciones correspondientes a variaciones de H2 k. Los valores entre corchetes son éxitos de rango. Por ejemplo, en la ejecución mcm32, solo se consideraron exitosos los documentos que estaban presentes en 3 o más clasificaciones de entrada. Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos son descartados. Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos de RI. Tabla 6: Rendimiento considerando diferentes éxitos de rango. Identificación de ejecución MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% Para las ejecuciones mcm24 y mcm25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo por consulta aumentó y se situó en alrededor de 5 segundos. 5.2.2 Impacto de la Variación de los Parámetros. La Tabla 7 muestra la variación de rendimiento del enfoque de clasificación por preferencias cuando se consideran diferentes umbrales de preferencia. Encontramos una mejora en el rendimiento hasta valores de umbral de aproximadamente el 5%, luego hay una disminución en el rendimiento que se vuelve significativa para valores de umbral superiores al 10%. Además, S@1 mejora del 41.33% al 46.67% cuando el umbral de preferencia cambia de 0 a 5%. Por lo tanto, podemos concluir que las clasificaciones de entrada son semiordeles en lugar de órdenes completos. La Tabla 8 muestra la evolución de las medidas de rendimiento con respecto al umbral de concordancia. Podemos concluir que para colocar el documento di antes de di en la clasificación de consenso, en la Tabla 7: Impacto de la variación del umbral de preferencia del 0 al 12.5%. Ejecutar Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% al menos la mitad de las clasificaciones de entrada de PRi ∩ PRi deben ser concordantes. El rendimiento disminuye significativamente para valores muy bajos y muy altos del umbral de concordancia. De hecho, para tales valores, la condición de concordancia se cumple o bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente. Por lo tanto, la relación de clasificación se vuelve demasiado débil o demasiado fuerte respectivamente. Tabla 8: Impacto de la variación de cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% En los experimentos, variar el umbral de veto, así como el umbral de discordancia dentro de intervalos razonables, no tiene un impacto significativo en las medidas de rendimiento. De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron un rendimiento similar, aunque hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada veten fácilmente. Además, el ajuste del umbral de discordancia se realizó para valores del 50% y 75% del umbral de veto. Para estas ejecuciones no observamos ninguna variación de rendimiento notable, aunque para umbrales de discordancia bajos (dmax < 20%), el rendimiento disminuyó ligeramente. 5.2.3 Impacto de la Variación del Número de Clasificaciones de Entrada Para estudiar la evolución del rendimiento cuando se consideran diferentes conjuntos de clasificaciones de entrada, realizamos tres ejecuciones adicionales donde se consideran 2, 4 y 6 de los conjuntos de clasificaciones de entrada con mejor rendimiento. Los resultados reportados en la Tabla 9 parecen ser contraintuitivos y tampoco respaldan hallazgos previos en la investigación sobre la agregación de rangos [3]. Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento aportan más ruido que información para establecer la clasificación de consenso. Por lo tanto, cuando se consideran, el rendimiento disminuye. Tabla 9: Rendimiento considerando diferentes conjuntos de clasificaciones de entrada con mejor rendimiento. Identificador de ejecución MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de clasificaciones. En este conjunto de ejecuciones, comparamos el enfoque de clasificación con algunos métodos de agregación de clasificaciones estándar que han demostrado tener un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias CombSUM y CombMNZ. También examinamos el rendimiento de un <br>método mayoritario</br> que es el método de la cadena de Markov (MC4). Para las comparaciones, consideramos una relación de superación específica S∗ = S(5%,50%,50%,30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros. La primera fila de la Tabla 10 muestra el rendimiento de los métodos de agregación de rangos con respecto a un conjunto de suposiciones básicas A1 = (H1 100, H2 5, H4 nuevo): solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos los documentos presentes en 5 o más clasificaciones y actualizamos los rangos de los documentos exitosos. Para los métodos posicionales, colocamos los documentos faltantes en la cola de la clasificación (H3 sí), mientras que para nuestro método, al igual que para MC4, conservamos la hipótesis H3 no. Las tres filas siguientes de la Tabla 10 informan sobre los rendimientos al cambiar un elemento del conjunto de suposiciones básicas: la segunda fila corresponde al conjunto de suposiciones A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a 1000. La tercera fila corresponde al conjunto de supuestos A3 = (H1 100, H2 todos, H4 nuevos), es decir, considerando los documentos presentes en al menos un ranking. La cuarta fila corresponde al conjunto de supuestos A4 = (H1 100, H2 5, H4 init), es decir, manteniendo los rangos originales de los documentos exitosos. La quinta fila de la Tabla 10, etiquetada como A5, muestra el rendimiento cuando se consideran todas las 225 consultas de la pista web de TREC-2004. Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de Página de Inicio y Página Nombrada [10]) de la pista web TREC-2004. Este conjunto de pruebas tiene como objetivo demostrar si el rendimiento relativo de los diferentes métodos depende de la tarea. La última fila de la Tabla 10, etiquetada como A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de suposiciones A1 de la primera fila. Esto tiene como objetivo mostrar si el rendimiento relativo de los diferentes métodos cambia de un año a otro. Los valores entre corchetes de la Tabla 10 son variaciones del rendimiento de cada método de agregación de rangos con respecto al rendimiento del enfoque de superación. Tabla 10: Rendimiento (MAP) de diferentes métodos de agregación de rangos bajo 3 colecciones de pruebas diferentes mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) Del análisis de la tabla 10 se puede establecer lo siguiente: • para todas las ejecuciones, considerar todos los documentos en cada clasificación de entrada (A2) mejora significativamente el rendimiento (MAP aumenta en promedio un 11.62%). Esto es predecible ya que algunos documentos relevantes inicialmente no reportados recibirían mejores posiciones en la clasificación de consenso. • para todas las ejecuciones, considerar documentos incluso aquellos presentes en solo una clasificación de entrada (A3) mejora significativamente el rendimiento. Para mcm, combSUM y combMNZ, la mejora del rendimiento es más importante (el MAP aumenta en promedio un 20.27%) que para la ejecución de Markov (el MAP aumenta un 3.86%). • preservar las posiciones iniciales de los documentos (A4) o volver a calcularlas (A1) no tiene una influencia notable en el rendimiento para ambos métodos posicional y mayoritario. • considerar todas las consultas de la pista web de TREC2004 (A5) así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos. • considerando las consultas TD de la pista web de TREC2002, los rendimientos de todos los métodos de fusión de datos son más bajos que el del mejor ranking de entrada que tiene un valor de MAP de 18.58%. Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con la mejor, lo que añade más ruido a la clasificación de consenso. • los rendimientos de los métodos de fusión de datos mcm y markov son significativamente mejores que el de la mejor clasificación de entrada uogWebCAU150. Esto sigue siendo cierto solo para las ejecuciones combSUM y combMNZ bajo las suposiciones H1 todas o H2 todas. Esto demuestra que los métodos mayoritarios son menos sensibles a suposiciones que los métodos posicionales. El enfoque de superación siempre tiene un rendimiento significativamente mejor que los métodos posicionales combSUM y combMNZ. También tiene un mejor rendimiento que el método de la cadena de Markov, especialmente bajo la suposición H2 donde la diferencia de rendimientos se vuelve significativa. 6. CONCLUSIONES En este artículo, abordamos el problema de agregación de rangos donde se deben fusionar listas de documentos diferentes, pero no disjuntas. Notamos que las clasificaciones de entrada pueden ocultar empates, por lo que no deben considerarse como órdenes completos. Solo se debe utilizar información sólida de cada clasificación de entrada. Los métodos actuales de agregación de rangos, y especialmente los métodos posicionales (por ejemplo, combSUM [15]), no fueron diseñados inicialmente para trabajar con tales clasificaciones. Deben adaptarse teniendo en cuenta supuestos de trabajo específicos. Proponemos un nuevo método de clasificación para la agregación de rangos que está bien adaptado al contexto de la RI. De hecho, clasifica dos documentos con respecto a la intensidad de la diferencia de sus posiciones en cada clasificación de entrada y también considera el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico. Tampoco es necesario hacer suposiciones específicas sobre las posiciones de los documentos faltantes. Esta es una característica importante, ya que la ausencia de un documento en un ranking no necesariamente debe interpretarse de forma negativa. Los resultados experimentales muestran que el método de clasificación supera significativamente a los populares métodos clásicos de fusión de datos posicionales como las estrategias combSUM y combMNZ. También supera en rendimiento a los métodos mayoritarios de buen rendimiento, como el método de la cadena de Markov. Estos resultados se prueban con diferentes colecciones de pruebas y consultas. De los experimentos, también podemos concluir que para mejorar el rendimiento, deberíamos fusionar listas de resultados de modelos de IR con buen desempeño, y que los métodos de fusión de datos mayoritarios funcionan mejor que los métodos posicionales. El método propuesto puede tener un impacto real en el rendimiento de la metabúsqueda web, ya que la mayoría de los motores de búsqueda primarios solo proporcionan clasificaciones, mientras que la mayoría de los enfoques actuales necesitan puntuaciones para fusionar las listas de resultados en una sola lista. El trabajo adicional implica investigar si el enfoque de clasificación por jerarquía funciona bien en varios otros contextos, por ejemplo, utilizando las puntuaciones de los documentos o alguna combinación de los rangos y puntuaciones de los documentos. Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este artículo. 7. REFERENCIAS [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur. Fusión de enfoques intensivos en conocimiento y estadísticos para recuperar y anotar documentos genómicos textuales. En Actas de TREC2005. Publicación del NIST, 2005. [2] R. A. Baeza-Yates y B. A. Ribeiro-Neto. Recuperación de información moderna. ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de múltiples sistemas de recuperación clasificados. En Actas ACM-SIGIR94, páginas 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. \n\nSpringer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, y J. A. Shaw. Combinando evidencia de múltiples representaciones de consulta para la recuperación de información. IPM, 31(3):431-448, 1995. [5] J. Borda. \n\nIPM, 31(3):431-448, 1995. [5] J. Borda. Memoria sobre las elecciones por voto secreto. Historia de la Academia de Ciencias, 1781. [6] J. P. Callan, Z. Lu y W. B. Croft. Buscando colecciones distribuidas con redes de inferencia. En Actas ACM-SIGIR95, páginas 21-28, 1995. [7] M. Condorcet. Ensayo sobre la aplicación del análisis de probabilidad a las decisiones tomadas por mayoría de votos. Imprimerie Royale, París, 1785. [8] W. D. Cook y M. Kress. Clasificación ordinal con intensidad de preferencia. Ciencia de la Gestión, 31(1):26-32, 1985. [9] N. Craswell y D. Hawking. Resumen de la pista web TREC-2002. En Actas de TREC2002. Publicación del NIST, 2002. [10] N. Craswell y D. Hawking. Resumen de la TREC-2004 Web Track. En Actas de TREC2004. Publicación del NIST, 2004. [11] C. Dwork, S. R. Kumar, M. Naor y D. Sivakumar. Métodos de agregación de clasificaciones para la Web. En Actas WWW2001, páginas 613-622, 2001. [12] R. Fagin. Combinando información difusa de múltiples sistemas. JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar y E. Vee. Comparando y agregando clasificaciones con empates. En PODS, páginas 47-58, 2004. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas de los k mejores. SIAM J. en Matemáticas Discretas, 17(1):134-160, 2003. [15] E. A. Zorro y J. A. Shaw. Combinación de múltiples búsquedas. En Actas de TREC3. Publicación del NIST, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes y P. DasGupta. Un estudio de la superposición entre representaciones de documentos. Tecnología de la Información: Investigación y Desarrollo, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell y J. Callan. Selección de colecciones y fusión de resultados con patentes de EE. UU. organizadas por tema y datos de TREC. En las Actas ACM-CIKM2000, páginas 282-289. ACM Press, 2000. [18] A.\nACM Press, 2000. [18] A. Le Calv´e y J. Savoy. Estrategia de fusión de bases de datos basada en regresión logística. IPM, 36(3):341-359, 2000. [19] J. H. Lee. \n\nIPM, 36(3):341-359, 2000. [19] J. H. Lee. Análisis de la combinación de múltiples evidencias. En Actas ACM-SIGIR97, páginas 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier y J. Dunnion. Probfuse: un enfoque probabilístico para la fusión de datos. En las Actas ACM-SIGIR2006, páginas 139-146. ACM Press, 2006. [21] J. I. Marden. \n\nACM Press, 2006. [21] J. I. Marden. Analizando y modelando datos de rango. Número 64 en Monografías sobre Estadística y Probabilidad Aplicada. Chapman & Hall, 1995. [22] M. Montague y J. A. Aslam. Consistencia de la metabúsqueda. En las Actas ACM-SIGIR2001, páginas 386-387. ACM Press, 2001. [23] D. M. Pennock y E. Horvitz. Análisis de los fundamentos axiomáticos del filtrado colaborativo. En el Taller sobre Inteligencia Artificial para el Comercio Electrónico en la 16ª Conferencia Nacional de Inteligencia Artificial, 1999. [24] M. E. Renda y U. Straccia. Búsqueda web metasearch: métodos de agregación de rango basados en rango vs. puntuación. En las Actas de ACM-SAC2003, páginas 841-846. ACM Press, 2003. [25] W. H. Riker. \n\nACM Press, 2003. [25] W. H. Riker. Liberalismo contra populismo. Waveland Press, 1982. [26] B. Roy. \n\nWaveland Press, 1982. [26] B. Roy. El enfoque de jerarquización y los fundamentos de los métodos ELECTRE. Teoría y decisión, 31:49-73, 1991. [27] B. Roy y J. Hugonnard. Clasificación de proyectos de extensión de líneas suburbanas en el sistema de metro de París mediante un método multicriterio. Investigación en Transporte, 16A(4):301-312, 1982. [28] L. Si y J. Callan. Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En las Actas ACM-SIGIR2002, páginas 19-26. ACM Press, 2002. [29] M. Truchon. \n\nACM Press, 2002. [29] M. Truchon. Una extensión del criterio de Condorcet y órdenes de Kemeny. Cuaderno 9813, Centro de Investigación en Economía y Finanzas Aplicadas, octubre de 1998. [30] H. Turtle y W. B. Croft. Redes de inferencia para la recuperación de documentos. En Actas de ACM-SIGIR90, páginas 1-24. ACM Press, 1990. [31] C. C. Vogt y G. W. Cottrell. Fusión a través de una combinación lineal de puntuaciones. Recuperación de información, 1(3):151-173, 1999. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ir model": {
            "translated_key": "modelo de IR",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "datum fusion": {
            "translated_key": "fusión de datos",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "multiple criterium approach": {
            "translated_key": "Enfoque de múltiples criterios",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "outrank method": {
            "translated_key": "método de clasificación",
            "is_in_text": false,
            "original_annotated_sentences": [
                "An Outranking Approach for Rank Aggregation in Information Retrieval Mohamed Farah Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Paris Dauphine University Place du Mal de Lattre de Tassigny 75775 Paris Cedex 16, France vdp@lamsade.dauphine.fr ABSTRACT Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.).",
                "In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking.",
                "In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another.",
                "We show that the proposed method deals well with the Information Retrieval distinctive features.",
                "Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.",
                "Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval - Retrieval models.",
                "General Terms: Algorithms, Measurement, Experimentation, Performance, Theory. 1.",
                "INTRODUCTION A wide range of current Information Retrieval (IR) approaches are based on various search models (Boolean, Vector Space, Probabilistic, Language, etc. [2]) in order to retrieve relevant documents in response to a user request.",
                "The result lists produced by these approaches depend on the exact definition of the relevance concept.",
                "Rank aggregation approaches, also called data fusion approaches, consist in combining these result lists in order to produce a new and hopefully better ranking.",
                "Such approaches give rise to metasearch engines in the Web context.",
                "We consider, in the following, cases where only ranks are available and no other additional information is provided such as the relevance scores.",
                "This corresponds indeed to the reality, where only ordinal information is available.",
                "Data fusion is also relevant in other contexts, such as when the user writes several queries of his/her information need (e.g., a boolean query and a natural language query) [4], or when many document surrogates are available [16].",
                "Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods.",
                "For instance, experiments carried out in [16], [30], [4] and [19] showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant.",
                "Moreover, Lee [19] and Vogt and Cottrell [31] found that various retrieval approaches often return very different irrelevant documents, but many of the same relevant documents.",
                "Bartell et al. [3] also found that rank aggregation methods improve the performances w.r.t. those of the input methods, even when some of them have weak individual performances.",
                "These methods also tend to smooth out biases of the input methods according to Montague and Aslam [22].",
                "Data fusion has recently been proved to improve performances for both the ad-hoc retrieval and categorization tasks within the TREC genomics track in 2005 [1].",
                "The rank aggregation problem was addressed in various fields such as i) in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments [29], ii) in statistics when studying correlation between rankings, iii) in distributed databases when results from different databases must be combined [12], and iv) in collaborative filtering [23].",
                "Most current rank aggregation methods consider each input ranking as a permutation over the same set of items.",
                "They also give rigid interpretation to the exact ranking of the items.",
                "Both of these assumptions are rather not valid in the IR context, as will be shown in the following sections.",
                "The remaining of the paper is organized as follows.",
                "We first review current rank aggregation methods in Section 2.",
                "Then we outline the specificities of the data fusion problem in the IR context (Section 3).",
                "In Section 4, we present a new aggregation method which is proven to best fit the IR context.",
                "Experimental results are presented in Section 5 and conclusions are provided in a final section. 2.",
                "RELATED WORK As pointed out by Riker [25], we can distinguish two families of rank aggregation methods: positional methods which assign scores to items to be ranked according to the ranks they receive and majoritarian methods which are based on pairwise comparisons of items to be ranked.",
                "These two families of methods find their roots in the pioneering works of Borda [5] and Condorcet [7], respectively, in the social choice literature. 2.1 Preliminaries We first introduce some basic notations to present the rank aggregation methods in a uniform way.",
                "Let D = {d1, d2, . . . , dnd } be a set of nd documents.",
                "A list or a ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).",
                "Thus, di j di means di is ranked better than di in j.",
                "When Dj = D, j is said to be a full list.",
                "Otherwise, it is a partial list.",
                "If di belongs to Dj, rj i denotes the rank or position of di in j.",
                "We assume that the best answer (document) is assigned the position 1 and the worst one is assigned the position |Dj|.",
                "Let D be the set of all permutations on D or all subsets of D. A profile is a n-tuple of rankings PR = ( 1, 2, . . . , n).",
                "Restricting PR to the rankings containing document di defines PRi.",
                "We also call the number of rankings which contain document di the rank hits of di [19].",
                "The rank aggregation or data fusion problem consists of finding a ranking function or mechanism Ψ (also called a social welfare function in the social choice theory terminology) defined by: Ψ : n D → D PR = ( 1, 2, . . . , n) → σ = Ψ(PR) where σ is called a consensus ranking. 2.2 Positional Methods 2.2.1 Borda Count This method [5] first assigns a score n j=1 rj i to each document di.",
                "Documents are then ranked by increasing order of this score, breaking ties, if any, arbitrarily. 2.2.2 Linear Combination Methods This family of methods basically combine scores of documents.",
                "When used for the rank aggregation problem, ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it [3, 31, 17, 28].",
                "For instance, Callan et al. [6] used the inference networks model [30] to combine rankings.",
                "Fox and Shaw [15] proposed several combination strategies which are CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.",
                "The first three operators correspond to the sum, min and max operators, respectively.",
                "CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits.",
                "It is shown in [19] that the CombSUM and CombMNZ operators perform better than the others.",
                "Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings. 2.2.3 Footrule Optimal Aggregation In this method, a consensus ranking minimizes the Spearman footrule distance from the input rankings [21].",
                "Formally, given two full lists j and j , this distance is given by F( j, j ) = nd i=1 |rj i − rj i |.",
                "It extends to several lists as follows.",
                "Given a profile PR and a consensus ranking σ, the Spearman footrule distance of σ to PR is given by F(σ, PR) = n j=1 F(σ, j).",
                "Cook and Kress [8] proposed a similar method which consists in optimizing the distance D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, where rj i,i = rj i −rj i .",
                "This formulation has the advantage that it considers the intensity of preferences. 2.2.4 Probabilistic Methods This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance.",
                "During the training process, probabilities of relevance are calculated.",
                "For subsequent queries, documents are ranked based on these probabilities.",
                "For instance, in [20], each input ranking j is divided into a number of segments, and the conditional probability of relevance (R) of each document di depending on the segment k it occurs in, is computed, i.e. prob(R|di, k, j).",
                "For subsequent queries, the score of each document di is given by n j=1 prob(R|di,k, j ) k .",
                "Le Calve and Savoy [18] suggest using a logistic regression approach for combining scores.",
                "Training data is needed to infer the model parameters. 2.3 Majoritarian Methods 2.3.1 Condorcet Procedure The original Condorcet rule [7] specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest.",
                "Formally, let C(diσdi ) = { j∈ PR : di j di } be the coalition of rankings that are concordant with establishing diσdi , i.e. with the proposition di should be ranked better than di in the final ranking σ. di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.",
                "The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way: select the Condorcet winner, remove it from the lists, and repeat the previous two steps until there are no more documents to rank.",
                "Since there is not always Condorcet winners, variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory, with methods such as ELECTRE [26]. 2.3.2 Kemeny Optimal Aggregation As in section 2.2.3, a consensus ranking minimizes a geometric distance from the input rankings, where the Kendall tau distance is used instead of the Spearman footrule distance.",
                "Formally, given two full lists j and j , the Kendall tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, i.e. the number of pairwise disagreements between the two lists.",
                "It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem. 2.3.3 Markov Chain Methods Markov chains (MCs) have been used by Dwork et al. [11] as a natural method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event.",
                "In the same reference, the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one (see also [24]): • MC4: move from the current state di to the next state di by first choosing a document di uniformly from D. If for the majority of the rankings, we have rj i ≤ rj i , then move to di , else stay in di.",
                "The consensus ranking corresponds to the stationary distribution of MC4. 3.",
                "SPECIFICITIES OF THE RANK AGGREGATION PROBLEM IN THE IR CONTEXT 3.1 Limited Significance of the Rankings The exact positions of documents in one input ranking have limited significance and should not be overemphasized.",
                "For instance, having three relevant documents in the first three positions, any perturbation of these three items will have the same value.",
                "Indeed, in the IR context, the complete order provided by an input method may hide ties.",
                "In this case, we call such rankings semi orders.",
                "This was outlined in [13] as the problem of aggregation with ties.",
                "It is therefore important to build the consensus ranking based on robust information: • Documents with near positions in j are more likely to have similar interest or relevance.",
                "Thus a slight perturbation of the initial ranking is meaningless. • Assuming that document di is better ranked than document di in a ranking j, di is more likely to be definitively more relevant than di in j when the number of intermediate positions between di and di increases. 3.2 Partial Lists In real world applications, such as metasearch engines, rankings provided by the input methods are often partial lists.",
                "This was outlined in [14] as the problem of having to merge top-k results from various input lists.",
                "For instance, in the experiments carried out by Dwork et al. [11], authors found that among the top 100 best documents of 7 input search engines, 67% of the documents were present in only one search engine, whereas less than two documents were present in all the search engines.",
                "Rank aggregation of partial lists raises four major difficulties which we state hereafter, proposing for each of them various working assumptions: 1.",
                "Partial lists can have various lengths, which can favour long lists.",
                "We thus consider the following two working hypotheses: H1 k : We only consider the top k best documents from each input ranking.",
                "H1 all: We consider all the documents from each input ranking. 2.",
                "Since there are different documents in the input rankings, we must decide which documents should be kept in the consensus ranking.",
                "Two working hypotheses are therefore considered: H2 k : We only consider documents which are present in at least k input rankings (k > 1).",
                "H2 all: We consider all the documents which are ranked in at least one input ranking.",
                "Hereafter, we call documents which will be retained in the consensus ranking, candidate documents, and documents that will be excluded from the consensus ranking, excluded documents.",
                "We also call a candidate document which is missing in one or more rankings, a missing document. 3.",
                "Some candidate documents are missing documents in some input rankings.",
                "Main reasons for a missing document are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available.",
                "We consider the following two working hypotheses: H3 yes: Each missing document in each j is assigned a position.",
                "H3 no: No assumption is made, that is each missing document is considered neither better nor worse than any other document. 4.",
                "When assumption H2 k holds, each input ranking may contain documents which will not be considered in the consensus ranking.",
                "Regarding the positions of the candidate documents, we can consider the following working hypotheses: H4 init: The initial positions of candidate documents are kept in each input ranking.",
                "H4 new: Candidate documents receive new positions in each input ranking, after discarding excluded ones.",
                "In the IR context, rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties. 4.",
                "OUTRANKING APPROACH FOR RANK AGGREGATION 4.1 Presentation Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information.",
                "This constitutes a strong assumption that is questionable, especially when the input rankings have different lengths.",
                "Moreover, for positional methods, assumptions H3 and H4 , which are often arbitrary, have a strong impact on the results.",
                "For instance, let us consider an input ranking of 500 documents out of 1000 candidate documents.",
                "Whether we assign to each of the missing documents the position 1, 501, 750 or 1000 -corresponding to variations of H3 yes- will give rise to very contrasted results, especially regarding the top of the consensus ranking.",
                "Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings.",
                "Nevertheless, they suppose that such rankings are complete orders, ignoring that they may hide ties.",
                "Therefore, majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information.",
                "Trying to overcome the limits of current rank aggregation methods, we found that outranking approaches, which were initially used for multiple criteria aggregation problems [26], can also be used for the rank aggregation purpose, where each ranking plays the role of a criterion.",
                "Therefore, in order to decide whether a document di should be ranked better than di in the consensus ranking σ, the two following conditions should be met: • a concordance condition which ensures that a majority of the input rankings are concordant with diσdi (majority principle). • a discordance condition which ensures that none of the discordant input rankings strongly refutes dσd (respect of minorities principle).",
                "Formally, the concordance coalition with diσdi is Csp (diσdi ) = { j∈ PR : rj i ≤ rj i − sp} where sp is a preference threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between an indifference and a preference situation between documents.",
                "The discordance coalition with diσdi is Dsv (diσdi ) = { j∈ PR : rj i ≥ rj i + sv} where sv is a veto threshold which is the variation of document positions -whether it is absolute or relative to the ranking length- which draws the boundaries between a weak and a strong opposition to diσdi .",
                "Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules, several outranking relations can be defined.",
                "They can be more or less demanding depending on i) the values of the thresholds sp and sv, ii) the importance or minimal size cmin required for the concordance coalition, and iii) the importance or maximum size dmax of the discordance coalition.",
                "A generic outranking relation can thus be defined as follows: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin AND |Dsv (diσdi )| ≤ dmax This expression defines a family of nested outranking relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or sv ≤ sv.",
                "This expression also generalizes the majority rule which corresponds to the particular relation S(0,∞, n 2 ,n).",
                "It also satisfies important properties of rank aggregation methods, called neutrality, Pareto-optimality, Condorcet property and Extended Condorcet property, in the social choice literature [29].",
                "Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist.",
                "Therefore, we need specific procedures in order to derive a consensus ranking.",
                "We propose the following procedure which finds its roots in [27].",
                "It consists in partitioning the set of documents into r ranked classes.",
                "Each class Ch contains documents with the same relevance and results from the application of all relations (if possible) to the set of documents remaining after previous classes are computed.",
                "Documents within the same equivalence class are ranked arbitrarily.",
                "Formally, let • R be the set of candidate documents for a query, • S1 , S2 , . . . be a family of nested outranking relations, • Fk(di, E) = |{di ∈ E : diSk di }| be the number of documents in E(E ⊆ R) that could be considered worse than di according to relation Sk , • fk(di, E) = |{di ∈ E : di Sk di}| be the number of documents in E that could be considered better than di according to Sk , • sk(di, E) = Fk(di, E) − fk(di, E) be the qualification of di in E according to Sk .",
                "Each class Ch results from a distillation process.",
                "It corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . . where E0 = R \\ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced subset of Ek−1 resulting from the application of the following procedure: 1. compute for each di ∈ Ek−1 its qualification according to Sk , i.e. sk(di, Ek−1), 2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then 3.",
                "Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} When one outranking relation is used, the distillation process stops after the first application of the previous procedure, i.e., Ch corresponds to distillate E1.",
                "When different outranking relations are used, the distillation process stops when all the pre-defined outranking relations have been used or when |Ek| = 1. 4.2 Illustrative Example This section illustrates the concepts and procedures of section 4.1.",
                "Let us consider a set of candidate documents R = {d1, d2, d3, d4, d5}.",
                "The following table gives a profile PR of different rankings of the documents of R: PR = ( 1 , 2, 3, 4).",
                "Table 1: Rankings of documents rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Let us suppose that the preference and veto thresholds are set to values 1 and 4 respectively, and that the concordance and discordance thresholds are set to values 2 and 1 respectively.",
                "The following tables give the concordance, discordance and outranking matrices.",
                "Each entry csp (di, di ) (dsv (di, di )) in the concordance (discordance) matrix gives the number of rankings that are concordant (discordant) with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) = |Dsv (diσdi )|.",
                "Table 2: Computation of the outranking relation d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1Concordance Matrix d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0Discordance Matrix d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0Outranking Matrix (S1) For instance, the concordance coalition for the assertion d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance coalition for the same assertion is D4(d1σd4) = ∅.",
                "Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1 d4 holds.",
                "Notice that Fk(di, R) (fk(di, R)) is given by summing the values of the ith row (column) of the outranking matrix.",
                "The consensus ranking is obtained as follows: to get the first class C1, we compute the qualifications of all the documents of E0 = R with respect to S1 .",
                "They are respectively 2, 2, 2, -2 and -4.",
                "Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.",
                "Observe that, if we had used a second outranking relation S2(⊇ S1), these three documents could have been possibly discriminated.",
                "At this stage, we remove documents of C1 from the outranking matrix and compute the next class C2: we compute the new qualifications of the documents of E0 = R \\ C1 = {d4, d5}.",
                "They are respectively 1 and -1.",
                "So C3 = E1 = {d4}.",
                "The last document d5 is the only document of the last class C3.",
                "Thus, the consensus ranking is {d1, d2, d3} → {d4} → {d5}. 5.",
                "EXPERIMENTS AND RESULTS 5.1 Test Setting To facilitate empirical investigation of the proposed methodology, we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation.",
                "In this paper, we apply our approach to the Topic Distillation (TD) task of TREC-2004 Web track [10].",
                "In this task, there are 75 topics where only a short description of each is given.",
                "For each query, we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams.",
                "The performances of these runs are reported in table 3.",
                "Table 3: Performances of the 10 best runs of the TD task of TREC-2004 Run Id MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Average 14.7% 21.2% 34.9% 66.8% 78.94% For each query, each run provides a ranking of about 1000 documents.",
                "The number of documents retrieved by all these runs ranges from 543 to 5769.",
                "Their average (median) number is 3340 (3386).",
                "It is worth noting that we found similar distributions of the documents among the rankings as in [11].",
                "For evaluation, we used the trec eval standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.",
                "Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms.",
                "In the experiments, significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs.",
                "In the tables of the following section, statistically significant differences are marked with an asterisk.",
                "Values between brackets of the first column of each table, indicate the parameter value of the corresponding run. 5.2 Results We carried out several series of runs in order to i) study performance variations of the outranking approach when tuning the parameters and working assumptions, ii) compare performances of the outranking approach vs standard rank aggregation strategies , and iii) check whether rank aggregation performs better than the best input rankings.",
                "We set our basic run mcm with the following parameters.",
                "We considered that each input ranking is a complete order (sp = 0) and that an input ranking strongly refutes diσdi when the difference of both document positions is large enough (sv = 75%).",
                "Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking.",
                "They consequently may vary from one ranking to another.",
                "In addition, to accept the assertion diσdi , we supposed that the majority of the rankings must be concordant (cmin = 50%) and that every input ranking can impose its veto (dmax = 0).",
                "Concordance and discordance thresholds are computed for each tuple (di, di ) as the percentage of the input rankings of PRi ∩PRi .",
                "Thus, our choice of parameters leads to the definition of the outranking relation S(0,75%,50%,0).",
                "To test the run mcm, we had chosen the following assumptions.",
                "We retained the top 100 best documents from each input ranking (H1 100), only considered documents which are present in at least half of the input rankings (H2 5 ) and assumed H3 no and H4 new.",
                "In these conditions, the number of successful documents was about 100 on average, and the computation time per query was less than one second.",
                "Obviously, modifying the working assumptions should have deeper impact on the performances than tuning our model parameters.",
                "This was validated by preliminary experiments.",
                "Thus, we hereafter begin by studying performance variation when different sets of assumptions are considered.",
                "Afterwards, we study the impact of tuning parameters.",
                "Finally, we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms. 5.2.1 Impact of the Working Assumptions Table 4 summarizes the performance variation of the outranking approach under different working hypotheses.",
                "In Table 4: Impact of the working assumptions Run Id MAP S@1 S@5 S@10 mcm 18.47% 41.33% 81.33% 86.67% mcm22 (H3 yes) 17.72% (-4.06%) 34.67% 81.33% 86.67% mcm23 (H4 init) 18.26% (-1.14%) 41.33% 81.33% 86.67% mcm24 (H1 all) 20.67% (+11.91%*) 38.66% 80.00% 86.66% mcm25 (H2 all) 21.68% (+17.38%*) 40.00% 78.66% 89.33% this table, we first show that run mcm22, in which missing documents are all put in the same last position of each input ranking, leads to performance drop w.r.t. run mcm.",
                "Moreover, S@1 moves from 41.33% to 34.67% (-16.11%).",
                "This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm, lose this first position but remain ranked in the top 5 documents since S@5 did not change.",
                "We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant, even though they are missing in some other rankings.",
                "Consequently, when they are missing in some rankings, assigning worse ranks to these documents is harmful for performance.",
                "Also, from Table 4, we found that the performances of runs mcm and mcm23 are similar.",
                "Therefore, the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones.",
                "From the same Table 4, performance of the outranking approach increases significantly for runs mcm24 and mcm25.",
                "Therefore, whether we consider all the documents which are present in half of the rankings (mcm24) or we consider all the documents which are ranked in the first 100 positions in one or more rankings (mcm25), increases performances.",
                "This result was predictable since in both cases we have more detailed information on the relative importance of documents.",
                "Tables 5 and 6 confirm this evidence.",
                "Table 5, where values between brackets of the first column give the number of documents which are retained from each input ranking, shows that selecting more documents from each input ranking leads to performance increase.",
                "It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance.",
                "Table 5: Impact of the number of retained documents Run Id MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% Table 6 reports runs corresponding to variations of H2 k .",
                "Values between brackets are rank hits.",
                "For instance, in the run mcm32, only documents which are present in 3 or more input rankings, were considered successful.",
                "This table shows that performance is significantly better when rare documents are considered, whereas it decreases significantly when these documents are discarded.",
                "Therefore, we conclude that many of the relevant documents are retrieved by a rather small set of IR models.",
                "Table 6: Performance considering different rank hits Run Id MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% For both runs mcm24 and mcm25, the number of successful documents was about 1000 and therefore, the computation time per query increased and became around 5 seconds. 5.2.2 Impact of the Variation of the Parameters Table 7 shows performance variation of the outranking approach when different preference thresholds are considered.",
                "We found performance improvement up to threshold values of about 5%, then there is a decrease in the performance which becomes significant for threshold values greater than 10%.",
                "Moreover, S@1 improves from 41.33% to 46.67% when preference threshold changes from 0 to 5%.",
                "We can thus conclude that the input rankings are semi orders rather than complete orders.",
                "Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold.",
                "We can conclude that in order to put document di before di in the consensus ranking, Table 7: Impact of the variation of the preference threshold from 0 to 12.5% Run Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% at least half of the input rankings of PRi ∩ PRi should be concordant.",
                "Performance drops significantly for very low and very high values of the concordance threshold.",
                "In fact, for such values, the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all, respectively.",
                "Therefore, the outranking relation becomes either too weak or too strong respectively.",
                "Table 8: Impact of the variation of cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% In the experiments, varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures.",
                "In fact, runs with different veto thresholds (sv ∈ [50%; 100%]) had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily.",
                "Also, tuning the discordance threshold was carried out for values 50% and 75% of the veto threshold.",
                "For these runs we did not get any noticeable performance variation, although for low discordance thresholds (dmax < 20%), performance slightly decreased. 5.2.3 Impact of the Variation of the Number of Input Rankings To study performance evolution when different sets of input rankings are considered, we carried three more runs where 2, 4, and 6 of the best performing sets of the input rankings are considered.",
                "Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research [3].",
                "Nevertheless, this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking.",
                "Therefore, when they are considered, performance decreases.",
                "Table 9: Performance considering different best performing sets of input rankings Run Id MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparison of the Performance of Different Rank Aggregation Methods In this set of runs, we compare the outranking approach with some standard rank aggregation methods which were proven to have acceptable performance in previous studies: we considered two positional methods which are the CombSUM and the CombMNZ strategies.",
                "We also examined the performance of one majoritarian method which is the Markov chain method (MC4).",
                "For the comparisons, we considered a specific outranking relation S∗ = S(5%,50%,50%,30%) which results in good overall performances when tuning all the parameters.",
                "The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = (H1 100, H2 5 , H4 new): we only consider the 100 first documents from each ranking, then retain documents present in 5 or more rankings and update ranks of successful documents.",
                "For positional methods, we place missing documents at the queue of the ranking (H3 yes) whereas for our method as well as for MC4, we retained hypothesis H3 no.",
                "The three following rows of Table 10 report performances when changing one element from the basic assumption set: the second row corresponds to the assumption set A2 = (H1 1000, H2 5 , H4 new), i.e. changing the number of retained documents from 100 to 1000.",
                "The third row corresponds to the assumption set A3 = (H1 100, H2 all, H4 new), i.e. considering the documents present in at least one ranking.",
                "The fourth row corresponds to the assumption set A4 = (H1 100, H2 5 , H4 init), i.e. keeping the original ranks of successful documents.",
                "The fifth row of Table 10, labeled A5, gives performance when all the 225 queries of the Web track of TREC-2004 are considered.",
                "Obviously, performance level cannot be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks (Home Page and Named Page tasks [10]) of TREC-2004 Web track.",
                "This set of runs aims to show whether relative performance of the various methods is task-dependent.",
                "The last row of Table 10, labeled A6, reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004: we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries [9] considering the set of assumptions A1 of the first row.",
                "This aims to show whether relative performance of the various methods changes from year to year.",
                "Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach.",
                "Table 10: Performance (MAP) of different rank aggregation methods under 3 different test collections mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) From the analysis of table 10 the following can be established: • for all the runs, considering all the documents in each input ranking (A2) significantly improves performance (MAP increases by 11.62% on average).",
                "This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking. • for all the runs, considering documents even those present in only one input ranking (A3) significantly improves performance.",
                "For mcm, combSUM and combMNZ, performance improvement is more important (MAP increases by 20.27% on average) than for the markov run (MAP increases by 3.86%). • preserving the initial positions of documents (A4) or recomputing them (A1) does not have a noticeable influence on performance for both positional and majoritarian methods. • considering all the queries of the Web track of TREC2004 (A5) as well as the TD queries of the Web track of TREC-2002 (A6) does not alter the relative performance of the different data fusion methods. • considering the TD queries of the Web track of TREC2002, performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58%.",
                "This is because most of the fused input rankings have very low performances compared to the best one, which brings more noise to the consensus ranking. • performances of the data fusion methods mcm and markov are significantly better than that of the best input ranking uogWebCAU150.",
                "This remains true for runs combSUM and combMNZ only under assumptions H1 all or H2 all.",
                "This shows that majoritarian methods are less sensitive to assumptions than positional methods. • outranking approach always performs significantly better than positional methods combSUM and combMNZ.",
                "It has also better performances than the Markov chain method, especially under assumption H2 all where difference of performances becomes significant. 6.",
                "CONCLUSIONS In this paper, we address the rank aggregation problem where different, but not disjoint, lists of documents are to be fused.",
                "We noticed that the input rankings can hide ties, so they should not be considered as complete orders.",
                "Only robust information should be used from each input ranking.",
                "Current rank aggregation methods, and especially positional methods (e.g. combSUM [15]), are not initially designed to work with such rankings.",
                "They should be adapted by considering specific working assumptions.",
                "We propose a new outranking method for rank aggregation which is well adapted to the IR context.",
                "Indeed, it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document.",
                "There is also no need to make specific assumptions on the positions of the missing documents.",
                "This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively.",
                "Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies.",
                "It also out-performs a good performing majoritarian methods which is the Markov chain method.",
                "These results are tested against different test collections and queries.",
                "From the experiments, we can also conclude that in order to improve the performances, we should fuse result lists of well performing IR models, and that majoritarian data fusion methods perform better than positional methods.",
                "The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines, whereas most of the current approaches need scores to merge result lists into one single list.",
                "Further work involves investigating whether the outranking approach performs well in various other contexts, e.g. using the document scores or some combination of document ranks and scores.",
                "Acknowledgments The authors would like to thank Jacques Savoy for his valuable comments on a preliminary version of this paper. 7.",
                "REFERENCES [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe, and W. Wilbur.",
                "Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.",
                "In Proceedings TREC2005.",
                "NIST Publication, 2005. [2] R. A. Baeza-Yates and B.",
                "A. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "ACM Press , 1999. [3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings ACM-SIGIR94, pages 173-181.",
                "Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A.",
                "Fox, and J.",
                "A. Shaw.",
                "Combining evidence of multiple query representations for information retrieval.",
                "IPM, 31(3):431-448, 1995. [5] J. Borda.",
                "M´emoire sur les ´elections au scrutin.",
                "Histoire de lAcad´emie des Sciences, 1781. [6] J. P. Callan, Z. Lu, and W. B. Croft.",
                "Searching distributed collections with inference networks.",
                "In Proceedings ACM-SIGIR95, pages 21-28, 1995. [7] M. Condorcet.",
                "Essai sur lapplication de lanalyse `a la probabilit´e des d´ecisions rendues `a la pluralit´e des voix.",
                "Imprimerie Royale, Paris, 1785. [8] W. D. Cook and M. Kress.",
                "Ordinal ranking with intensity of preference.",
                "Management Science, 31(1):26-32, 1985. [9] N. Craswell and D. Hawking.",
                "Overview of the TREC-2002 Web Track.",
                "In Proceedings TREC2002.",
                "NIST Publication, 2002. [10] N. Craswell and D. Hawking.",
                "Overview of the TREC-2004 Web Track.",
                "In Proceedings of TREC2004.",
                "NIST Publication, 2004. [11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.",
                "Rank aggregation methods for the Web.",
                "In Proceedings WWW2001, pages 613-622, 2001. [12] R. Fagin.",
                "Combining fuzzy information from multiple systems.",
                "JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and E. Vee.",
                "Comparing and aggregating rankings with ties.",
                "In PODS, pages 47-58, 2004. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing top k lists.",
                "SIAM J. on Discrete Mathematics, 17(1):134-160, 2003. [15] E. A.",
                "Fox and J.",
                "A. Shaw.",
                "Combination of multiple searches.",
                "In Proceedings of TREC3.",
                "NIST Publication, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. DasGupta.",
                "A study of the overlap among document representations.",
                "Information Technology: Research and Development, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell, and J. Callan.",
                "Collection selection and results merging with topically organized U.S. patents and TREC data.",
                "In Proceedings ACM-CIKM2000, pages 282-289.",
                "ACM Press, 2000. [18] A.",
                "Le Calv´e and J. Savoy.",
                "Database merging strategy based on logistic regression.",
                "IPM, 36(3):341-359, 2000. [19] J. H. Lee.",
                "Analyses of multiple evidence combination.",
                "In Proceedings ACM-SIGIR97, pages 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.",
                "Probfuse: a probabilistic approach to data fusion.",
                "In Proceedings ACM-SIGIR2006, pages 139-146.",
                "ACM Press, 2006. [21] J. I. Marden.",
                "Analyzing and Modeling Rank Data.",
                "Number 64 in Monographs on Statistics and Applied Probability.",
                "Chapman & Hall, 1995. [22] M. Montague and J.",
                "A. Aslam.",
                "Metasearch consistency.",
                "In Proceedings ACM-SIGIR2001, pages 386-387.",
                "ACM Press, 2001. [23] D. M. Pennock and E. Horvitz.",
                "Analysis of the axiomatic foundations of collaborative filtering.",
                "In Workshop on AI for Electronic Commerce at the 16th National Conference on Artificial Intelligence, 1999. [24] M. E. Renda and U. Straccia.",
                "Web metasearch: rank vs. score based rank aggregation methods.",
                "In Proceedings ACM-SAC2003, pages 841-846.",
                "ACM Press, 2003. [25] W. H. Riker.",
                "Liberalism against populism.",
                "Waveland Press, 1982. [26] B. Roy.",
                "The outranking approach and the foundations of ELECTRE methods.",
                "Theory and Decision, 31:49-73, 1991. [27] B. Roy and J. Hugonnard.",
                "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method.",
                "Transportation Research, 16A(4):301-312, 1982. [28] L. Si and J. Callan.",
                "Using sampled data and regression to merge search engine results.",
                "In Proceedings ACM-SIGIR2002, pages 19-26.",
                "ACM Press, 2002. [29] M. Truchon.",
                "An extension of the Condorcet criterion and Kemeny orders.",
                "Cahier 9813, Centre de Recherche en Economie et Finance Appliqu´ees, Oct. 1998. [30] H. Turtle and W. B. Croft.",
                "Inference networks for document retrieval.",
                "In Proceedings of ACM-SIGIR90, pages 1-24.",
                "ACM Press, 1990. [31] C. C. Vogt and G. W. Cottrell.",
                "Fusion via a linear combination of scores.",
                "Information Retrieval, 1(3):151-173, 1999."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}