{
    "id": "H-9",
    "original_text": "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine. Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly. However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1. INTRODUCTION The utility of a search engine is affected by multiple factors. While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly. Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization. The most common strategy of presenting search results is a simple ranked list. Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results. However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group. For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team. Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD. In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list. Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document. As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28]. The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic. A label will be generated to indicate what each cluster is about. A user can then view the labels to decide which cluster to look into. Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26]. However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective. For example, users are often interested in finding either phone codes or zip codes when entering the query area codes. But the clusters discovered by the current methods may partition the results into local codes and international codes. Such clusters would not be very useful for users; even the best cluster would still have a low precision. Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster. There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms. For example, the ambiguous query jaguar may mean an animal or a car. A cluster may be labeled as panthera onca. Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful. In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results. That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly. Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects. For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query. In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar. More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data). Such aspects can be very useful for organizing future search results about car. Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways. Second, we will generate more meaningful cluster labels using past query words entered by users. Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects. Thus they can be better labels than those extracted from the ordinary contents of search results. To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs. Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs. We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster. We evaluate our method for result organization using logs of a commercial search engine. We compare our method with the default search engine ranking and the traditional clustering of search results. The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches. The rest of the paper is organized as follows. We first review the related work in Section 2. In Section 3, we describe search engine log data and our procedure of building a history collection. In Section 4, we present our approach in details. We describe the data set in Section 5 and the experimental results are discussed in Section 6. Finally, we conclude our paper and discuss future work in Section 7. 2. RELATED WORK Our work is closely related to the study of clustering search results. In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system. Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters. The system Grouper was described in [26, 27]. In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents. Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one. They also showed that using snippets is as effective as using whole documents. However, an important challenge of document clustering is to generate meaningful labels for clusters. To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results. In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster. Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22]. However, in all these works, the clusters are generated solely based on the search results. Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint. Methods of organizing search results based on text categorization are studied in [6, 8]. In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories. The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces. However predefined categories are often too general to reflect the finer granularity aspects of a query. Search logs have been exploited for several different purposes in the past. For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4]. Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1]. In our work, we explore past query history in order to better organize the search results for future queries. We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query. Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3. SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs. Different IDs mean different sessions. Web search. They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked. Search engine logs are separated by sessions. A session includes a single query and all the URLs that a user clicked after issuing the query [24]. A small sample of search log data is shown in Table 1. Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries. For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car. Different users are probably interested in different aspects of car. Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio. By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective. As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ... In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection. As shown above, search engine logs consist of sessions. Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks. However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately. To gather rich information, we enrich each URL with additional text content. Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session. All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session. Different sessions may contain the same queries. Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant. In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together. That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated. The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents. All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4. OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs. Given an input query, the general procedure of our approach is: 1. Get its related information from search engine logs. All the information forms a working set. 2. Learn aspects from the information in the working set. These aspects correspond to users interests given the input query. Each aspect is labeled with a representative query. 3. Categorize and organize the search results of the input query according to the aspects learned above. We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages. To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection. Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }. Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3. To find qs related queries in H, a natural way is to use a text retrieval algorithm. Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods. Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection. Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in. Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in. In this subsection, we propose to use a clustering method to discover these aspects. Any clustering algorithm could be applied here. In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2]. A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally. We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18]. Then the clusters are formed by dense subgraphs that are star-shaped. These clusters form a cover of the similarity graph. Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector. Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | . A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ. Each document di is a vertex of Gσ. If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices. After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1. Associate every vertex in Gσ with a flag, initialized as unmarked. 2. From those unmarked vertices, find the one which has the highest degree and let it be u. 3. Mark the flag of u as center. 4. Form a cluster C containing u and all its neighbors that are not marked as center. Mark all the selected neighbors as satellites. 5. Repeat from step 2 until all the vertices in Gσ are marked. Each cluster is star-shaped, which consists a single center and several satellites. There is only one parameter σ in the star clustering algorithm. A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small. On the other hand, a small σ will make the clusters big and less coherent. We will study the impact of this parameter in our experiments. A good feature of the star clustering algorithm is that it outputs a center for each cluster. In the past query collection Hq, each document corresponds to a query. This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally. All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results. Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm. In principle, any categorization algorithm can be used here. Here we use a simple centroid-based method for categorization. Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance. Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl. All these pis are used to categorize the search results. Specifically, for any search result sj, we build a TF-IDF vector. The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi. We then assign sj to the aspect with which it has the highest cosine similarity score. All the aspects are finally ranked according to the number of search results they have. Within each aspect, the search results are ranked according to their original search engine ranking. 5. DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14]. In total, this log data spans 31 days from 05/01/2006 to 05/31/2006. There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data. To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries. In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times). After cleaning, we get 169,057 unique queries in our history data collection totally. On average, each query has 3.5 distinct clicks. We build the pseudo-documents for all these queries as described in Section 3. The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB. We construct our test data from the last 1/3 data. According to the time, we separate this data into two test sets equally for cross-validation to set parameters. For each test set, we use every session as a test case. Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases. Different test cases may have the same queries but possibly different clicks.) Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents. Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs. Organizing search results into different aspects is expected to help informational queries. It thus makes sense to focus on the informational queries in our evaluation. For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query. Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection. Finally, we obtain 172 and 177 test cases in the first and second test sets respectively. On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6. EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results. For each test case, the first method is the default ranked list from a search engine (baseline). The second method is to organize the search results by clustering them (cluster-based). For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering). That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters. We compare our method (log-based) with the two baseline methods in the following experiments. For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine. To compare different result organization methods, we adopt a similar method as in the paper [9]. That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents. Organizing search results into clusters is to help users navigate into relevant documents quickly. The above metric is to simulate a scenario when users always choose the right cluster and look into it. Specifically, we download and organize the top 100 search results into aspects for each test case. We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods. P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents. We also use Mean Reciprocal Rank (MRR) as another metric. MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q. To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect. The number of aspects is fixed at 10 in all the following experiments. The star clustering algorithm can output different number of clusters for different input. To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates. We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid. In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results. In the following, we test our hypothesis from two perspectives - organization and labeling. Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5. We also show the percentage of relative improvement in the lower part. Comparison Test set 1 Test set 2 Impr./Decr. Impr./Decr. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5. We optimize the parameter σs for each collection individually based on P@5 values. This shows the best performance that each method can achieve. In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods. For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783. We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement). The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method. Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy. This is because cluster-based method organizes the search results only based on the contents. Thus it could organize the results differently from users preferences. This confirms our hypothesis of the bias of the cluster-based method. Comparing our method with the cluster-based method, we achieve significant improvement on both test collections. The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively. This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users. We showed the optimal results above. To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set. We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1. We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance. However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection. We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity. In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased. We can see that our method improves more test cases compared with the other two methods. In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty. All the analysis below is based on test set 1. Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters. In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity. If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse. In this case, we would expect our method to help more. The results are shown in Figure 2. In this figure, we partition the ratios into 4 bins. The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.) In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure. We can observe that when the ratio is smaller, the log-based method can improve more test cases. But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline. For example, in bin 1, 48 test cases are improved and 34 are decreased. But in bin 4, all the 4 test cases are decreased. This confirms our hypothesis that our method can help more if the query has more diverse results. This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio). Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5]. Here we analyze the effectiveness of our method in helping difficult queries. We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case. We then order the 172 test cases in test set 1 in an increasing order of MAP values. We partition the test cases into 4 bins with each having a roughly equal number of test cases. A small MAP means that the utility of the original ranking is low. Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs. For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased. Figure 3 shows the results. Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20). This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries. This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section. For the star clustering algorithm, we study the similarity threshold parameter σ. For the OKAPI retrieval function, we study the parameters k1 and b. We also study the impact of the number of past queries retrieved in our log-based method. Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets. We vary σ from 0.05 to 0.3 with step 0.05. Figure 4 shows that the performance is not very sensitive to the parameter σ. We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25. In Table 4, we show the impact of OKAPI parameters. We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2. From this table, it is clear that P@5 is also not very sensitive to the parameter setting. Most of the values are larger than 0.35. The default values k1 = 1.2 and b = 0.8 give approximately optimal results. We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods. We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects. The results on both test collections are shown in Figure 5. We can see that the performance gradually increases as we enlarge the number of past queries retrieved. Thus our method could potentially learn more as we accumulate more history. More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method. This query may mean phone codes or zip codes. Table 5 shows the representative keywords extracted from the three biggest clusters of both methods. In the clusterbased method, the results are partitioned based on locations: local or international. In the log-based method, the results are disambiguated into two senses: phone codes or zip codes. While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively. Therefore our log-based method is more effective in helping users to navigate into their desired results. Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method. The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster. Our log-based method can avoid this difficulty by taking advantage of queries. Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label. For log-based method, we use the center of each star cluster as the label for the corresponding cluster. In general, it is not easy to quantify the readability of a cluster label automatically. We use examples to show the difference between the cluster-based and the log-based methods. In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple. For the cluster-based method, we separate keywords by commas since they do not form a phrase. From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries. This is another advantage of our way of organizing search results over the clustering approach. Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7. CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner. To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective. Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned. We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking. The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse. Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results. There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple. It would be interesting to explore other potentially more effective methods. In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously. Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view). It would thus be interesting to study how to further improve the organization of the results based on such feedback information. Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8. ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments. This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9. REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR, pages 19-26, 2006. [2] J. A. Aslam, E. Pelekov, and D. Rus. The star clustering algorithm for static and dynamic information organization. Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Applications of web query mining. In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger. Agglomerative clustering of a search engine query log. In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais. Bringing order to the web: automatically categorizing search results. In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen. Optimizing search by showing results in context. In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen. Reexamining the cluster hypothesis: Scatter/gather on retrieval results. In SIGIR, pages 76-84, 1996. [10] T. Joachims. Optimizing search engines using clickthrough data. In KDD, pages 133-142, 2002. [11] T. Joachims. Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96. Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions. In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram. A hierarchical monothetic document clustering algorithm for summarization and browsing search results. In WWW, pages 658-665, 2004. [14] Microsoft Live Labs. Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl. Scatter/gather browsing communicates the topic structure of a very large text collection. In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using implicit feedback. In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen. Information Retrieval, second edition. Butterworths, London, 1979. [21] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai. Latent semantic analysis for multiple-type interrelated data objects. In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user queries of a search engine. In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval. In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni. Web document clustering: A feasibility demonstration. In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni. Grouper: A dynamic clustering interface to web search results. Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma. Learning to cluster web search results. In SIGIR, pages 210-217, 2004.",
    "original_translation": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004.",
    "original_sentences": [
        "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
        "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
        "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
        "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
        "We evaluate our proposed method on a commercial search engine log data.",
        "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
        "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
        "INTRODUCTION The utility of a search engine is affected by multiple factors.",
        "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
        "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
        "The most common strategy of presenting search results is a simple ranked list.",
        "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
        "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
        "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
        "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
        "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
        "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
        "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
        "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
        "A label will be generated to indicate what each cluster is about.",
        "A user can then view the labels to decide which cluster to look into.",
        "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
        "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
        "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
        "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
        "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
        "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
        "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
        "For example, the ambiguous query jaguar may mean an animal or a car.",
        "A cluster may be labeled as panthera onca.",
        "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
        "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
        "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
        "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
        "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
        "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
        "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
        "Such aspects can be very useful for organizing future search results about car.",
        "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
        "Second, we will generate more meaningful cluster labels using past query words entered by users.",
        "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
        "Thus they can be better labels than those extracted from the ordinary contents of search results.",
        "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
        "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
        "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
        "We evaluate our method for result organization using logs of a commercial search engine.",
        "We compare our method with the default search engine ranking and the traditional clustering of search results.",
        "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
        "The rest of the paper is organized as follows.",
        "We first review the related work in Section 2.",
        "In Section 3, we describe search engine log data and our procedure of building a history collection.",
        "In Section 4, we present our approach in details.",
        "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
        "Finally, we conclude our paper and discuss future work in Section 7. 2.",
        "RELATED WORK Our work is closely related to the study of clustering search results.",
        "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
        "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
        "The system Grouper was described in [26, 27].",
        "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
        "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
        "They also showed that using snippets is as effective as using whole documents.",
        "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
        "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
        "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
        "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
        "However, in all these works, the clusters are generated solely based on the search results.",
        "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
        "Methods of organizing search results based on text categorization are studied in [6, 8].",
        "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
        "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
        "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
        "Search logs have been exploited for several different purposes in the past.",
        "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
        "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
        "In our work, we explore past query history in order to better organize the search results for future queries.",
        "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
        "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
        "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
        "Different IDs mean different sessions.",
        "Web search.",
        "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
        "Search engine logs are separated by sessions.",
        "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
        "A small sample of search log data is shown in Table 1.",
        "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
        "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
        "Different users are probably interested in different aspects of car.",
        "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
        "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
        "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
        "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
        "As shown above, search engine logs consist of sessions.",
        "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
        "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
        "To gather rich information, we enrich each URL with additional text content.",
        "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
        "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
        "Different sessions may contain the same queries.",
        "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
        "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
        "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
        "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
        "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
        "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
        "Given an input query, the general procedure of our approach is: 1.",
        "Get its related information from search engine logs.",
        "All the information forms a working set. 2.",
        "Learn aspects from the information in the working set.",
        "These aspects correspond to users interests given the input query.",
        "Each aspect is labeled with a representative query. 3.",
        "Categorize and organize the search results of the input query according to the aspects learned above.",
        "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
        "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
        "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
        "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
        "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
        "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
        "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
        "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
        "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
        "In this subsection, we propose to use a clustering method to discover these aspects.",
        "Any clustering algorithm could be applied here.",
        "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
        "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
        "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
        "Then the clusters are formed by dense subgraphs that are star-shaped.",
        "These clusters form a cover of the similarity graph.",
        "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
        "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
        "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
        "Each document di is a vertex of Gσ.",
        "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
        "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
        "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
        "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
        "Mark the flag of u as center. 4.",
        "Form a cluster C containing u and all its neighbors that are not marked as center.",
        "Mark all the selected neighbors as satellites. 5.",
        "Repeat from step 2 until all the vertices in Gσ are marked.",
        "Each cluster is star-shaped, which consists a single center and several satellites.",
        "There is only one parameter σ in the star clustering algorithm.",
        "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
        "On the other hand, a small σ will make the clusters big and less coherent.",
        "We will study the impact of this parameter in our experiments.",
        "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
        "In the past query collection Hq, each document corresponds to a query.",
        "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
        "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
        "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
        "In principle, any categorization algorithm can be used here.",
        "Here we use a simple centroid-based method for categorization.",
        "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
        "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
        "All these pis are used to categorize the search results.",
        "Specifically, for any search result sj, we build a TF-IDF vector.",
        "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
        "We then assign sj to the aspect with which it has the highest cosine similarity score.",
        "All the aspects are finally ranked according to the number of search results they have.",
        "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
        "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
        "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
        "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
        "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
        "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
        "After cleaning, we get 169,057 unique queries in our history data collection totally.",
        "On average, each query has 3.5 distinct clicks.",
        "We build the pseudo-documents for all these queries as described in Section 3.",
        "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
        "We construct our test data from the last 1/3 data.",
        "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
        "For each test set, we use every session as a test case.",
        "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
        "Different test cases may have the same queries but possibly different clicks.)",
        "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
        "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
        "Organizing search results into different aspects is expected to help informational queries.",
        "It thus makes sense to focus on the informational queries in our evaluation.",
        "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
        "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
        "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
        "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
        "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
        "For each test case, the first method is the default ranked list from a search engine (baseline).",
        "The second method is to organize the search results by clustering them (cluster-based).",
        "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
        "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
        "We compare our method (log-based) with the two baseline methods in the following experiments.",
        "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
        "To compare different result organization methods, we adopt a similar method as in the paper [9].",
        "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
        "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
        "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
        "Specifically, we download and organize the top 100 search results into aspects for each test case.",
        "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
        "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
        "We also use Mean Reciprocal Rank (MRR) as another metric.",
        "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
        "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
        "The number of aspects is fixed at 10 in all the following experiments.",
        "The star clustering algorithm can output different number of clusters for different input.",
        "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
        "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
        "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
        "In the following, we test our hypothesis from two perspectives - organization and labeling.",
        "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
        "We also show the percentage of relative improvement in the lower part.",
        "Comparison Test set 1 Test set 2 Impr./Decr.",
        "Impr./Decr.",
        "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
        "We optimize the parameter σs for each collection individually based on P@5 values.",
        "This shows the best performance that each method can achieve.",
        "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
        "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
        "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
        "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
        "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
        "This is because cluster-based method organizes the search results only based on the contents.",
        "Thus it could organize the results differently from users preferences.",
        "This confirms our hypothesis of the bias of the cluster-based method.",
        "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
        "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
        "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
        "We showed the optimal results above.",
        "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
        "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
        "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
        "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
        "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
        "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
        "We can see that our method improves more test cases compared with the other two methods.",
        "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
        "All the analysis below is based on test set 1.",
        "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
        "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
        "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
        "In this case, we would expect our method to help more.",
        "The results are shown in Figure 2.",
        "In this figure, we partition the ratios into 4 bins.",
        "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
        "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
        "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
        "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
        "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
        "But in bin 4, all the 4 test cases are decreased.",
        "This confirms our hypothesis that our method can help more if the query has more diverse results.",
        "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
        "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
        "Here we analyze the effectiveness of our method in helping difficult queries.",
        "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
        "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
        "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
        "A small MAP means that the utility of the original ranking is low.",
        "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
        "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
        "Figure 3 shows the results.",
        "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
        "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
        "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
        "For the star clustering algorithm, we study the similarity threshold parameter σ.",
        "For the OKAPI retrieval function, we study the parameters k1 and b.",
        "We also study the impact of the number of past queries retrieved in our log-based method.",
        "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
        "We vary σ from 0.05 to 0.3 with step 0.05.",
        "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
        "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
        "In Table 4, we show the impact of OKAPI parameters.",
        "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
        "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
        "Most of the values are larger than 0.35.",
        "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
        "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
        "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
        "The results on both test collections are shown in Figure 5.",
        "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
        "Thus our method could potentially learn more as we accumulate more history.",
        "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
        "This query may mean phone codes or zip codes.",
        "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
        "In the clusterbased method, the results are partitioned based on locations: local or international.",
        "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
        "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
        "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
        "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
        "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
        "Our log-based method can avoid this difficulty by taking advantage of queries.",
        "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
        "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
        "In general, it is not easy to quantify the readability of a cluster label automatically.",
        "We use examples to show the difference between the cluster-based and the log-based methods.",
        "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
        "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
        "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
        "This is another advantage of our way of organizing search results over the clustering approach.",
        "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
        "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
        "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
        "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
        "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
        "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
        "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
        "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
        "It would be interesting to explore other potentially more effective methods.",
        "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
        "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
        "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
        "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
        "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
        "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
        "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
        "Improving web search ranking by incorporating user behavior information.",
        "In SIGIR, pages 19-26, 2006. [2] J.",
        "A. Aslam, E. Pelekov, and D. Rus.",
        "The star clustering algorithm for static and dynamic information organization.",
        "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
        "Applications of web query mining.",
        "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
        "Agglomerative clustering of a search engine query log.",
        "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
        "What makes a query difficult?",
        "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
        "Bringing order to the web: automatically categorizing search results.",
        "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
        "Predicting query performance.",
        "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
        "Optimizing search by showing results in context.",
        "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
        "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
        "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
        "Optimizing search engines using clickthrough data.",
        "In KDD, pages 133-142, 2002. [11] T. Joachims.",
        "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
        "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
        "Generating query substitutions.",
        "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
        "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
        "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
        "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
        "Scatter/gather browsing communicates the topic structure of a very large text collection.",
        "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
        "Query chains: learning to rank from implicit feedback.",
        "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
        "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
        "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
        "A vector space model for automatic indexing.",
        "Commun.",
        "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
        "Context-sensitive information retrieval using implicit feedback.",
        "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
        "Information Retrieval, second edition.",
        "Butterworths, London, 1979. [21] V. N. Vapnik.",
        "The Nature of Statistical Learning Theory.",
        "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
        "Latent semantic analysis for multiple-type interrelated data objects.",
        "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
        "Nie, and H. Zhang.",
        "Clustering user queries of a search engine.",
        "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
        "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
        "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
        "Web document clustering: A feasibility demonstration.",
        "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
        "Grouper: A dynamic clustering interface to web search results.",
        "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
        "Zeng, Q.-C.",
        "He, Z. Chen, W.-Y.",
        "Ma, and J. Ma.",
        "Learning to cluster web search results.",
        "In SIGIR, pages 210-217, 2004."
    ],
    "translated_text_sentences": [
        "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda.",
        "Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes.",
        "Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto.",
        "En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios.",
        "Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial.",
        "En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas.",
        "Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1.",
        "La utilidad de un motor de búsqueda se ve afectada por múltiples factores.",
        "Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda.",
        "Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda.",
        "La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada.",
        "De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados.",
        "Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés.",
        "Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar.",
        "Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar.",
        "En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada.",
        "El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante.",
        "Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28].",
        "La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta.",
        "Se generará una etiqueta para indicar de qué se trata cada grupo.",
        "Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar.",
        "Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26].",
        "Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios.",
        "Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área.",
        "Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales.",
        "Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión.",
        "Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto.",
        "Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos.",
        "Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche.",
        "Un grupo puede ser etiquetado como panthera onca.",
        "Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil.",
        "En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda.",
        "Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia.",
        "Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes.",
        "Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta.",
        "En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\".",
        "Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda).",
        "Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles.",
        "Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras.",
        "Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios.",
        "Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos.",
        "Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda.",
        "Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados.",
        "Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics.",
        "Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas.",
        "Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial.",
        "Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda.",
        "Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales.",
        "El resto del documento está organizado de la siguiente manera.",
        "Primero revisamos el trabajo relacionado en la Sección 2.",
        "En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial.",
        "En la Sección 4, presentamos nuestro enfoque en detalle.",
        "Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6.",
        "Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2.",
        "TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda.",
        "En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información.",
        "Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos.",
        "El sistema Grouper fue descrito en [26, 27].",
        "En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos.",
        "Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo.",
        "También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos.",
        "Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos.",
        "Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda.",
        "En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente.",
        "La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22].",
        "Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda.",
        "Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios.",
        "Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8].",
        "En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas.",
        "Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas.",
        "Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta.",
        "Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado.",
        "Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4].",
        "Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1].",
        "En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras.",
        "Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta.",
        "Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3.",
        "Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda.",
        "Diferentes identificaciones significan diferentes sesiones.",
        "Búsqueda web.",
        "Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic.",
        "Los registros del motor de búsqueda están separados por sesiones.",
        "Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24].",
        "Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1.",
        "Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas.",
        "Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles.",
        "Los usuarios probablemente están interesados en diferentes aspectos del automóvil.",
        "Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche.",
        "Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios.",
        "Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ...",
        "Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos.",
        "Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones.",
        "Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics.",
        "Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada.",
        "Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional.",
        "Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente.",
        "Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión.",
        "Las diferentes sesiones pueden contener las mismas consultas.",
        "Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante.",
        "Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas.",
        "Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas.",
        "Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos.",
        "Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4.",
        "NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda.",
        "Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1.",
        "Obtén su información relacionada de los registros del motor de búsqueda.",
        "Toda la información forma un conjunto de trabajo. 2.",
        "Aprende aspectos de la información en el conjunto de trabajo.",
        "Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada.",
        "Cada aspecto está etiquetado con una consulta representativa. 3.",
        "Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente.",
        "Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web.",
        "Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados.",
        "Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }.",
        "Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3.",
        "Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto.",
        "Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados.",
        "Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial.",
        "Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados.",
        "Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados.",
        "En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos.",
        "Cualquier algoritmo de agrupamiento podría aplicarse aquí.",
        "En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2].",
        "Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo.",
        "Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18].",
        "Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella.",
        "Estos grupos forman una cobertura del grafo de similitud.",
        "Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF.",
        "Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|.",
        "Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ.",
        "Cada documento di es un vértice de Gσ.",
        "Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes.",
        "Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1.",
        "Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2.",
        "De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u.",
        "Marca la bandera de u como centro. 4.",
        "Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro.",
        "Marque a todos los vecinos seleccionados como satélites. 5.",
        "Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados.",
        "Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites.",
        "Solo hay un parámetro σ en el algoritmo de agrupamiento estelar.",
        "Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños.",
        "Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes.",
        "Estudiaremos el impacto de este parámetro en nuestros experimentos.",
        "Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo.",
        "En la colección de consultas pasadas Hq, cada documento corresponde a una consulta.",
        "Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo.",
        "Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda.",
        "Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización.",
        "En principio, cualquier algoritmo de categorización puede ser utilizado aquí.",
        "Aquí utilizamos un método simple basado en el centroide para la categorización.",
        "Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor.",
        "Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl.",
        "Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda.",
        "Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF.",
        "El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi.",
        "Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta.",
        "Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen.",
        "Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5.",
        "RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14].",
        "En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006.",
        "Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar.",
        "Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras.",
        "En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces).",
        "Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos.",
        "En promedio, cada consulta tiene 3.5 clics distintos.",
        "Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3.",
        "La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB.",
        "Construimos nuestros datos de prueba a partir del último tercio de los datos.",
        "Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros.",
        "Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba.",
        "Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba).",
        "Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics.",
        "Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes.",
        "Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas.",
        "Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas.",
        "Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación.",
        "Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa.",
        "Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica.",
        "Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente.",
        "En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente.",
        "EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda.",
        "Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia).",
        "El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres).",
        "Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar).",
        "Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella.",
        "Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos.",
        "Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda.",
        "Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9].",
        "Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes.",
        "Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes.",
        "La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan.",
        "Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba.",
        "Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos.",
        "P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos.",
        "También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica.",
        "MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q.",
        "Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto.",
        "El número de aspectos está fijo en 10 en todos los experimentos siguientes.",
        "El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas.",
        "Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos.",
        "Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente.",
        "En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda.",
        "En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado.",
        "Tabla 2: Comparación de diferentes métodos por MMR y P@5.",
        "También mostramos el porcentaje de mejora relativa en la parte inferior.",
        "Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr.",
        "Aumento/Disminución.",
        "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5.",
        "Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5.",
        "Esto muestra el mejor rendimiento que cada método puede lograr.",
        "En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres.",
        "Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783.",
        "Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%).",
        "Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método.",
        "Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión.",
        "Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos.",
        "Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios.",
        "Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters.",
        "Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas.",
        "Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente.",
        "Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios.",
        "Mostramos los resultados óptimos arriba.",
        "Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto.",
        "Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1.",
        "Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo.",
        "Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas.",
        "Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados.",
        "En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye.",
        "Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos.",
        "En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta.",
        "Todo el análisis a continuación está basado en el conjunto de pruebas 1.",
        "Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos.",
        "Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad.",
        "Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos.",
        "En este caso, esperaríamos que nuestro método ayude más.",
        "Los resultados se muestran en la Figura 2.",
        "En esta figura, dividimos las proporciones en 4 contenedores.",
        "Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.)",
        "En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura.",
        "Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba.",
        "Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base.",
        "Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen.",
        "Pero en el contenedor 4, todos los 4 casos de prueba han disminuido.",
        "Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos.",
        "Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo).",
        "Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5].",
        "Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles.",
        "Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba.",
        "Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP.",
        "Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba.",
        "Un MAP pequeño significa que la utilidad del ranking original es baja.",
        "El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos.",
        "Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido.",
        "La Figura 3 muestra los resultados.",
        "Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20).",
        "Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles.",
        "Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección.",
        "Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ.",
        "Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b.",
        "También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros.",
        "La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas.",
        "Variamos σ de 0.05 a 0.3 con un paso de 0.05.",
        "La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ.",
        "Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25.",
        "En la Tabla 4, mostramos el impacto de los parámetros de OKAPI.",
        "Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2.",
        "De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros.",
        "La mayoría de los valores son mayores a 0.35.",
        "Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados.",
        "Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro.",
        "Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos.",
        "Los resultados en ambas colecciones de pruebas se muestran en la Figura 5.",
        "Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas.",
        "Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia.",
        "Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres.",
        "Esta consulta puede referirse a códigos telefónicos o códigos postales.",
        "La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos.",
        "En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales.",
        "En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales.",
        "Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente.",
        "Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados.",
        "Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros.",
        "El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster.",
        "Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas.",
        "Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster.",
        "Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente.",
        "En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática.",
        "Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros.",
        "En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana.",
        "Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase.",
        "Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios.",
        "Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento.",
        "Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres.",
        "CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario.",
        "Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios.",
        "Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos.",
        "Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda.",
        "Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos.",
        "Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda.",
        "Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples.",
        "Sería interesante explorar otros métodos potencialmente más efectivos.",
        "En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente.",
        "Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver).",
        "Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación.",
        "Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8.",
        "AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios.",
        "Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933.",
        "REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais.",
        "Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario.",
        "En SIGIR, páginas 19-26, 2006. [2] J.",
        "A. Aslam, E. Pelekov y D. Rus.",
        "El algoritmo de agrupamiento estelar para la organización de información estática y dinámica.",
        "Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
        "Aplicaciones de la minería de consultas web.",
        "En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger.",
        "Aglomeración de clústeres de un registro de consultas de un motor de búsqueda.",
        "En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg.",
        "¿Qué hace que una consulta sea difícil?",
        "En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais.",
        "Dando orden a la web: categorizando automáticamente los resultados de búsqueda.",
        "En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft.",
        "Predicción del rendimiento de la consulta.",
        "En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen.",
        "Optimizando la búsqueda mostrando resultados en contexto.",
        "En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen.",
        "Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación.",
        "En SIGIR, páginas 76-84, 1996. [10] T. Joachims.",
        "Optimización de motores de búsqueda utilizando datos de clics.",
        "En KDD, páginas 133-142, 2002. [11] T. Joachims.",
        "Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96.",
        "Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner.",
        "Generando sustituciones de consulta.",
        "En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram.",
        "Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda.",
        "En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs.",
        "Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl.",
        "La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande.",
        "En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims.",
        "Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita.",
        "En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker.",
        "Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística.",
        "En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang.",
        "Un modelo de espacio vectorial para indexación automática.",
        "Comunicación.",
        "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai.",
        "Recuperación de información sensible al contexto utilizando retroalimentación implícita.",
        "En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen.",
        "Recuperación de información, segunda edición.",
        "Butterworths, Londres, 1979. [21] V. N. Vapnik.",
        "La naturaleza de la teoría del aprendizaje estadístico.",
        "Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai.",
        "Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos.",
        "En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y.",
        "Nie, y H. Zhang.",
        "Agrupación de consultas de usuarios de un motor de búsqueda.",
        "En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow.",
        "Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida.",
        "En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni.",
        "Agrupamiento de documentos web: Una demostración de viabilidad.",
        "En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni.",
        "Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web.",
        "Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J.",
        "Zeng, Q.-C.",
        "Él, Z. Chen, W.-Y.",
        "Ma, y J. Ma.",
        "Aprendiendo a agrupar los resultados de búsqueda web.",
        "En SIGIR, páginas 210-217, 2004."
    ],
    "error_count": 1,
    "keys": {
        "retrieval model": {
            "translated_key": "modelo de recuperación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying <br>retrieval model</br> and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "While the primary factor is the soundness of the underlying <br>retrieval model</br> and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly."
            ],
            "translated_annotated_samples": [
                "Si bien el factor principal es la solidez del <br>modelo de recuperación</br> subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del <br>modelo de recuperación</br> subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ranking function": {
            "translated_key": "función de clasificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and <br>ranking function</br>, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "While the primary factor is the soundness of the underlying retrieval model and <br>ranking function</br>, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly."
            ],
            "translated_annotated_samples": [
                "Si bien el factor principal es la solidez del modelo de recuperación subyacente y la <br>función de clasificación</br>, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la <br>función de clasificación</br>, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "ambiguity": {
            "translated_key": "ambigüedad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to <br>ambiguity</br> or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "However, when the search results are diverse (e.g., due to <br>ambiguity</br> or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group."
            ],
            "translated_annotated_samples": [
                "Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la <br>ambigüedad</br> o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la <br>ambigüedad</br> o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "clustering view": {
            "translated_key": "vista de agrupación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a <br>clustering view</br> of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "In these examples, a <br>clustering view</br> of the search results would be much more useful to a user than a simple ranked list."
            ],
            "translated_annotated_samples": [
                "En estos ejemplos, una <br>vista de agrupación</br> de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una <br>vista de agrupación</br> de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "meaningful cluster label": {
            "translated_key": "etiquetas de clúster",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more <br>meaningful cluster label</br>s using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more <br>meaningful cluster label</br>s using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more <br>meaningful cluster label</br>s using past query words entered by users.",
                "Second, we will generate more <br>meaningful cluster label</br>s using past query words entered by users."
            ],
            "translated_annotated_samples": [
                "En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando <br>etiquetas de clúster</br> más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios.",
                "Segundo, generaremos <br>etiquetas de clúster</br> más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando <br>etiquetas de clúster</br> más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos <br>etiquetas de clúster</br> más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "history collection": {
            "translated_key": "colección de historial",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a <br>history collection</br> containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the <br>history collection</br> and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a <br>history collection</br>.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our <br>history collection</br>.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the <br>history collection</br> H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the <br>history collection</br>, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our <br>history collection</br> is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our <br>history collection</br>.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "To implement the ideas presented above, we rely on search engine logs and build a <br>history collection</br> containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the <br>history collection</br> and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "In Section 3, we describe search engine log data and our procedure of building a <br>history collection</br>.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our <br>history collection</br>.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the <br>history collection</br> H. These pseudo-documents contain the aspects that users are interested in."
            ],
            "translated_annotated_samples": [
                "Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una <br>colección de historial</br> que contiene las consultas pasadas y los clics asociados.",
                "Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la <br>colección de historial</br> y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics.",
                "En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una <br>colección de historial</br>.",
                "Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra <br>colección de historial</br>.",
                "Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la <br>colección de historial</br> H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una <br>colección de historial</br> que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la <br>colección de historial</br> y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una <br>colección de historial</br>. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra <br>colección de historial</br>. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la <br>colección de historial</br> H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "past query": {
            "translated_key": "palabras de consulta anteriores",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using <br>past query</br> words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using <br>past query</br> words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative <br>past query</br> in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using <br>past query</br> words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore <br>past query</br> history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a <br>past query</br>, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the <br>past query</br> collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the <br>past query</br> history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from <br>past query</br> history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using <br>past query</br> words entered by users.",
                "Second, we will generate more meaningful cluster labels using <br>past query</br> words entered by users.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative <br>past query</br> in the query cluster.",
                "The results show that our method is effective for improving search utility and the labels generated using <br>past query</br> words are more readable than those generated using traditional clustering approaches.",
                "In our work, we explore <br>past query</br> history in order to better organize the search results for future queries."
            ],
            "translated_annotated_samples": [
                "En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando <br>palabras de consulta anteriores</br> ingresadas por los usuarios.",
                "Segundo, generaremos etiquetas de clúster más significativas utilizando <br>palabras de consulta anteriores</br> ingresadas por los usuarios.",
                "Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la <br>consulta pasada</br> más representativa en el grupo de consultas.",
                "Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando <br>palabras de consultas anteriores</br> son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales.",
                "En nuestro trabajo, exploramos el <br>historial de consultas pasadas</br> para organizar mejor los resultados de búsqueda para consultas futuras."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando <br>palabras de consulta anteriores</br> ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando <br>palabras de consulta anteriores</br> ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la <br>consulta pasada</br> más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando <br>palabras de consultas anteriores</br> son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el <br>historial de consultas pasadas</br> para organizar mejor los resultados de búsqueda para consultas futuras. ",
            "candidates": [],
            "error": [
                [
                    "palabras de consulta anteriores",
                    "palabras de consulta anteriores",
                    "consulta pasada",
                    "palabras de consultas anteriores",
                    "historial de consultas pasadas"
                ]
            ]
        },
        "clickthrough": {
            "translated_key": "datos de clics",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with <br>clickthrough</br> information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated <br>clickthrough</br> information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using <br>clickthrough</br> data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using <br>clickthrough</br> Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "Each Qi corresponds to a unique query and is enriched with <br>clickthrough</br> information as discussed in Section 3.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated <br>clickthrough</br> information, and categorize the search results into the aspects learned.",
                "Optimizing search engines using <br>clickthrough</br> data.",
                "Evaluating Retrieval Performance Using <br>clickthrough</br> Data., pages 79-96."
            ],
            "translated_annotated_samples": [
                "Cada Qi corresponde a una consulta única y está enriquecido con <br>información de clics</br> como se discute en la Sección 3.",
                "Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos.",
                "Optimización de motores de búsqueda utilizando <br>datos de clics</br>.",
                "Evaluación del rendimiento de recuperación utilizando <br>datos de clics</br>., páginas 79-96."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con <br>información de clics</br> como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando <br>datos de clics</br>. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando <br>datos de clics</br>., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    "información de clics",
                    "datos de clics",
                    "datos de clics"
                ]
            ]
        },
        "star clustering algorithm": {
            "translated_key": "algoritmo de agrupamiento estelar",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the <br>star clustering algorithm</br> [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the <br>star clustering algorithm</br> [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the <br>star clustering algorithm</br> [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the <br>star clustering algorithm</br> below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the <br>star clustering algorithm</br> clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the <br>star clustering algorithm</br>.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the <br>star clustering algorithm</br> is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The <br>star clustering algorithm</br> can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the <br>star clustering algorithm</br>, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The <br>star clustering algorithm</br> for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the <br>star clustering algorithm</br> [2] to these past queries and clickthroughs.",
                "We use the <br>star clustering algorithm</br> [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "In this paper, we use an algorithm based on graph partition: the <br>star clustering algorithm</br> [2].",
                "We describe the <br>star clustering algorithm</br> below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "After the similarity graph Gσ is built, the <br>star clustering algorithm</br> clusters the documents using a greedy algorithm as follows: 1."
            ],
            "translated_annotated_samples": [
                "Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el <br>algoritmo de agrupamiento estelar</br> [2] a estas consultas pasadas y clics.",
                "Utilizamos el <br>algoritmo de agrupamiento estelar</br> [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta.",
                "En este artículo, utilizamos un algoritmo basado en partición de grafos: el <br>algoritmo de agrupamiento estelar</br> [2].",
                "Describimos el <br>algoritmo de agrupación de estrellas</br> a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18].",
                "Después de construir el grafo de similitud Gσ, el <br>algoritmo de agrupamiento estelar</br> agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el <br>algoritmo de agrupamiento estelar</br> [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el <br>algoritmo de agrupamiento estelar</br> [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el <br>algoritmo de agrupamiento estelar</br> [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el <br>algoritmo de agrupación de estrellas</br> a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el <br>algoritmo de agrupamiento estelar</br> agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. ",
            "candidates": [],
            "error": [
                [
                    "algoritmo de agrupamiento estelar",
                    "algoritmo de agrupamiento estelar",
                    "algoritmo de agrupamiento estelar",
                    "algoritmo de agrupación de estrellas",
                    "algoritmo de agrupamiento estelar"
                ]
            ]
        },
        "suffix tree clustering algorithm": {
            "translated_key": "algoritmo de Agrupamiento de Árbol de Sufijos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the <br>suffix tree clustering algorithm</br> (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "Several clustering algorithms are compared and the <br>suffix tree clustering algorithm</br> (STC) was shown to be the most effective one."
            ],
            "translated_annotated_samples": [
                "Varios algoritmos de agrupamiento son comparados y se demostró que el <br>algoritmo de Agrupamiento de Árbol de Sufijos</br> (STC) es el más efectivo."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el <br>algoritmo de Agrupamiento de Árbol de Sufijos</br> (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "search result snippet": {
            "translated_key": "fragmentos de resultados de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the <br>search result snippet</br>s and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the <br>search result snippet</br>s and these phrases were then used to group search results."
            ],
            "translated_annotated_samples": [
                "Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los <br>fragmentos de resultados de búsqueda</br> y luego se utilizaron estas frases para agrupar los resultados de búsqueda."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los <br>fragmentos de resultados de búsqueda</br> y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "monothetic clustering algorithm": {
            "translated_key": "algoritmo de agrupamiento monotético",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a <br>monothetic clustering algorithm</br>, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "In [13], the authors proposed to use a <br>monothetic clustering algorithm</br>, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster."
            ],
            "translated_annotated_samples": [
                "En [13], los autores propusieron utilizar un <br>algoritmo de agrupamiento monotético</br>, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un <br>algoritmo de agrupamiento monotético</br>, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "pseudo-document": {
            "translated_key": "pseudo-documento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a <br>pseudo-document</br> which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and <br>pseudo-document</br> Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "That is, for each unique query, we build a <br>pseudo-document</br> which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "Specifically, we use the following formula to calculate the similarity between query q and <br>pseudo-document</br> Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection."
            ],
            "translated_annotated_samples": [
                "Es decir, para cada consulta única, construimos un <br>pseudo-documento</br> que consiste en todas las descripciones de sus clics en todas las sesiones agregadas.",
                "Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el <br>pseudo-documento</br> Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un <br>pseudo-documento</br> que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el <br>pseudo-documento</br> Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "pairwise similarity graph": {
            "translated_key": "grafo de similitud par a par",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a <br>pairwise similarity graph</br> on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a <br>pairwise similarity graph</br> on this collection based on the vector space model in information retrieval [18]."
            ],
            "translated_annotated_samples": [
                "Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un <br>grafo de similitud par a par</br> en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un <br>grafo de similitud par a par</br> en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "similarity threshold parameter": {
            "translated_key": "parámetro de umbral de similitud",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a <br>similarity threshold parameter</br> σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the <br>similarity threshold parameter</br> σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "A similarity graph Gσ can then be constructed as follows using a <br>similarity threshold parameter</br> σ.",
                "For the star clustering algorithm, we study the <br>similarity threshold parameter</br> σ."
            ],
            "translated_annotated_samples": [
                "Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un <br>parámetro de umbral de similitud</br> σ.",
                "Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de <br>umbral de similitud</br> σ."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un <br>parámetro de umbral de similitud</br> σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de <br>umbral de similitud</br> σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    "parámetro de umbral de similitud",
                    "umbral de similitud"
                ]
            ]
        },
        "centroid-based method": {
            "translated_key": "método basado en el centroide",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple <br>centroid-based method</br> for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The <br>centroid-based method</br> computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "Here we use a simple <br>centroid-based method</br> for categorization.",
                "The <br>centroid-based method</br> computes the cosine similarity between the vector representation of sj and each centroid prototype pi."
            ],
            "translated_annotated_samples": [
                "Aquí utilizamos un método simple basado en el centroide para la categorización.",
                "El <br>método basado en el centroide</br> calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El <br>método basado en el centroide</br> calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "cosine similarity": {
            "translated_key": "similitud del coseno",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the <br>cosine similarity</br> between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest <br>cosine similarity</br> score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "The centroid-based method computes the <br>cosine similarity</br> between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest <br>cosine similarity</br> score."
            ],
            "translated_annotated_samples": [
                "El método basado en el centroide calcula la <br>similitud del coseno</br> entre la representación vectorial de sj y cada prototipo de centroide pi.",
                "Luego asignamos sj al aspecto con el que tiene la puntuación de <br>similitud de coseno</br> más alta."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la <br>similitud del coseno</br> entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de <br>similitud de coseno</br> más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    "similitud del coseno",
                    "similitud de coseno"
                ]
            ]
        },
        "centroid prototype": {
            "translated_key": "prototipo de centroide",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a <br>centroid prototype</br> pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each <br>centroid prototype</br> pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "Based on the pseudo-documents in each discovered aspect Ci, we build a <br>centroid prototype</br> pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each <br>centroid prototype</br> pi."
            ],
            "translated_annotated_samples": [
                "Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un <br>prototipo de centroide</br> pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl.",
                "El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada <br>prototipo de centroide</br> pi."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un <br>prototipo de centroide</br> pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada <br>prototipo de centroide</br> pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "reciprocal rank": {
            "translated_key": "rango recíproco",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean <br>reciprocal rank</br> (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "We also use Mean <br>reciprocal rank</br> (MRR) as another metric."
            ],
            "translated_annotated_samples": [
                "También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "log-based method": {
            "translated_key": "método basado en registros",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our <br>log-based method</br> is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our <br>log-based method</br>, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our <br>log-based method</br> can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that <br>log-based method</br> help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the <br>log-based method</br> can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the <br>log-based method</br> can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, <br>log-based method</br> may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our <br>log-based method</br>.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the <br>log-based method</br> and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the <br>log-based method</br>, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our <br>log-based method</br> is more effective in helping users to navigate into their desired results.",
                "Cluster-based method <br>log-based method</br> city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our <br>log-based method</br> 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our <br>log-based method</br> can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For <br>log-based method</br>, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our <br>log-based method</br> gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar <br>log-based method</br> Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple <br>log-based method</br> Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our <br>log-based method</br> with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our <br>log-based method</br> can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our <br>log-based method</br> can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "This shows that our <br>log-based method</br> is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "To test the sensitivity of the parameter σ of our <br>log-based method</br>, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our <br>log-based method</br> can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "In order to test the hypothesis that <br>log-based method</br> help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "We can observe that when the ratio is smaller, the <br>log-based method</br> can improve more test cases."
            ],
            "translated_annotated_samples": [
                "Esto demuestra que nuestro <br>método basado en registros</br> es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios.",
                "Para probar la sensibilidad del parámetro σ de nuestro <br>método basado en logaritmos</br>, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto.",
                "En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro <br>método basado en registros</br> puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta.",
                "Para probar la hipótesis de que el <br>método basado en registros</br> ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad.",
                "Podemos observar que cuando la proporción es menor, el <br>método basado en logaritmos</br> puede mejorar más casos de prueba."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro <br>método basado en registros</br> es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro <br>método basado en logaritmos</br>, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro <br>método basado en registros</br> puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el <br>método basado en registros</br> ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el <br>método basado en logaritmos</br> puede mejorar más casos de prueba. ",
            "candidates": [],
            "error": [
                [
                    "método basado en registros",
                    "método basado en logaritmos",
                    "método basado en registros",
                    "método basado en registros",
                    "método basado en logaritmos"
                ]
            ]
        },
        "mean average precision": {
            "translated_key": "Precisión Promedio Media",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the <br>mean average precision</br> (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "We quantify the query difficulty by the <br>mean average precision</br> (MAP) of the original search engine ranking for each test case."
            ],
            "translated_annotated_samples": [
                "Medimos la dificultad de la consulta mediante la <br>Precisión Promedio Media</br> (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la <br>Precisión Promedio Media</br> (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "search result organization": {
            "translated_key": "organización de resultados de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of <br>search result organization</br>.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the <br>search result organization</br> based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of <br>search result organization</br>.",
                "EXPERIMENTS In the section, we describe our experiments on the <br>search result organization</br> based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results."
            ],
            "translated_annotated_samples": [
                "Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la <br>organización de los resultados de búsqueda</br>.",
                "EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la <br>organización de resultados de búsqueda</br> basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de <br>organización de resultados de búsqueda</br>."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la <br>organización de los resultados de búsqueda</br>. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la <br>organización de resultados de búsqueda</br> basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de <br>organización de resultados de búsqueda</br>. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    "organización de los resultados de búsqueda",
                    "organización de resultados de búsqueda",
                    "organización de resultados de búsqueda"
                ]
            ]
        },
        "search engine log": {
            "translated_key": "datos de registro de un motor de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial <br>search engine log</br> data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe <br>search engine log</br> data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [
                "We evaluate our proposed method on a commercial <br>search engine log</br> data.",
                "In Section 3, we describe <br>search engine log</br> data and our procedure of building a history collection."
            ],
            "translated_annotated_samples": [
                "Evaluamos nuestro método propuesto en <br>datos de registro de un motor de búsqueda</br> comercial.",
                "En la Sección 3, describimos los <br>datos de registro del motor de búsqueda</br> y nuestro procedimiento para construir una colección de historial."
            ],
            "translated_text": "Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en <br>datos de registro de un motor de búsqueda</br> comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua \"jaguar\" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de \"jaguar\" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como \"imagen del equipo jaguar\", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua \"jaguar\" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo \"jaguar\", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de \"jaguar\". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los <br>datos de registro del motor de búsqueda</br> y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004. ",
            "candidates": [],
            "error": [
                [
                    "datos de registro de un motor de búsqueda",
                    "datos de registro del motor de búsqueda"
                ]
            ]
        },
        "interest aspect": {
            "translated_key": "aspecto de interés",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Learn from Web Search Logs to Organize Search Results Xuanhui Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 czhai@cs.uiuc.edu ABSTRACT Effective organization of search results is critical for improving the utility of any search engine.",
                "Clustering search results is an effective way to organize search results, which allows a user to navigate into relevant documents quickly.",
                "However, two deficiencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the users perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "In this paper, we propose to address these two deficiencies by (1) learning interesting aspects of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users.",
                "We evaluate our proposed method on a commercial search engine log data.",
                "Compared with the traditional methods of clustering search results, our method can give better result organization and more meaningful labels.",
                "Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search process General Terms: Algorithm, Experimentation 1.",
                "INTRODUCTION The utility of a search engine is affected by multiple factors.",
                "While the primary factor is the soundness of the underlying retrieval model and ranking function, how to organize and present search results is also a very important factor that can affect the utility of a search engine significantly.",
                "Compared with the vast amount of literature on retrieval models, however, there is relatively little research on how to improve the effectiveness of search result organization.",
                "The most common strategy of presenting search results is a simple ranked list.",
                "Intuitively, such a presentation strategy is reasonable for non-ambiguous, homogeneous search results; in general, it would work well when the search results are good and a user can easily find many relevant documents in the top ranked results.",
                "However, when the search results are diverse (e.g., due to ambiguity or multiple aspects of a topic) as is often the case in Web search, the ranked list presentation would not be effective; in such a case, it would be better to group the search results into clusters so that a user can easily navigate into a particular interesting group.",
                "For example, the results in the first page returned from Google for the ambiguous query jaguar (as of Dec. 2nd, 2006) contain at least four different senses of jaguar (i.e., car, animal, software, and a sports team); even for a more refined query such as jaguar team picture, the results are still quite ambiguous, including at least four different jaguar teams - a wrestling team, a jaguar car team, Southwestern College Jaguar softball team, and Jacksonville Jaguar football team.",
                "Moreover, if a user wants to find a place to download a jaguar software, a query such as download jaguar is also not very effective as the dominating results are about downloading jaguar brochure, jaguar wallpaper, and jaguar DVD.",
                "In these examples, a clustering view of the search results would be much more useful to a user than a simple ranked list.",
                "Clustering is also useful when the search results are poor, in which case, a user would otherwise have to go through a long list sequentially to reach the very first relevant document.",
                "As a primary alternative strategy for presenting search results, clustering search results has been studied relatively extensively [9, 15, 26, 27, 28].",
                "The general idea in virtually all the existing work is to perform clustering on a set of topranked search results to partition the results into natural clusters, which often correspond to different subtopics of the general query topic.",
                "A label will be generated to indicate what each cluster is about.",
                "A user can then view the labels to decide which cluster to look into.",
                "Such a strategy has been shown to be more useful than the simple ranked list presentation in several studies [8, 9, 26].",
                "However, this clustering strategy has two deficiencies which make it not always work well: First, the clusters discovered in this way do not necessarily correspond to the interesting aspects of a topic from the users perspective.",
                "For example, users are often interested in finding either phone codes or zip codes when entering the query area codes.",
                "But the clusters discovered by the current methods may partition the results into local codes and international codes.",
                "Such clusters would not be very useful for users; even the best cluster would still have a low precision.",
                "Second, the cluster labels generated are not informative enough to allow a user to identify the right cluster.",
                "There are two reasons for this problem: (1) The clusters are not corresponding to a users interests, so their labels would not be very meaningful or useful. (2) Even if a cluster really corresponds to an interesting aspect of the topic, the label may not be informative because it is usually generated based on the contents in a cluster, and it is possible that the user is not very familiar with some of the terms.",
                "For example, the ambiguous query jaguar may mean an animal or a car.",
                "A cluster may be labeled as panthera onca.",
                "Although this is an accurate label for a cluster with the animal sense of jaguar, if a user is not familiar with the phrase, the label would not be helpful.",
                "In this paper, we propose a different strategy for partitioning search results, which addresses these two deficiencies through imposing a user-oriented partitioning of the search results.",
                "That is, we try to figure out what aspects of a search topic are likely interesting to a user and organize the results accordingly.",
                "Specifically, we propose to do the following: First, we will learn interesting aspects of similar topics from search logs and organize search results based on these interesting aspects.",
                "For example, if the current query has occurred many times in the search logs, we can look at what kinds of pages viewed by the users in the results and what kind of words are used together with such a query.",
                "In case when the query is ambiguous such as jaguar we can expect to see some clear clusters corresponding different senses of jaguar.",
                "More importantly, even if a word is not ambiguous (e.g., car), we may still discover interesting aspects such as car rental and car pricing (which happened to be the two primary aspects discovered in our search log data).",
                "Such aspects can be very useful for organizing future search results about car.",
                "Note that in the case of car, clusters generated using regular clustering may not necessarily reflect such interesting aspects about car from a users perspective, even though the generated clusters are coherent and meaningful in other ways.",
                "Second, we will generate more meaningful cluster labels using past query words entered by users.",
                "Assuming that the past search logs can help us learn what specific aspects are interesting to users given the current query topic, we could also expect that those query words entered by users in the past that are associated with the current query can provide meaningful descriptions of the distinct aspects.",
                "Thus they can be better labels than those extracted from the ordinary contents of search results.",
                "To implement the ideas presented above, we rely on search engine logs and build a history collection containing the past queries and the associated clickthroughs.",
                "Given a new query, we find its related past queries from the history collection and learn aspects through applying the star clustering algorithm [2] to these past queries and clickthroughs.",
                "We can then organize the search results into these aspects using categorization techniques and label each aspect by the most representative past query in the query cluster.",
                "We evaluate our method for result organization using logs of a commercial search engine.",
                "We compare our method with the default search engine ranking and the traditional clustering of search results.",
                "The results show that our method is effective for improving search utility and the labels generated using past query words are more readable than those generated using traditional clustering approaches.",
                "The rest of the paper is organized as follows.",
                "We first review the related work in Section 2.",
                "In Section 3, we describe search engine log data and our procedure of building a history collection.",
                "In Section 4, we present our approach in details.",
                "We describe the data set in Section 5 and the experimental results are discussed in Section 6.",
                "Finally, we conclude our paper and discuss future work in Section 7. 2.",
                "RELATED WORK Our work is closely related to the study of clustering search results.",
                "In [9, 15], the authors used Scatter/Gather algorithm to cluster the top documents returned from a traditional information retrieval system.",
                "Their results validate the cluster hypothesis [20] that relevant documents tend to form clusters.",
                "The system Grouper was described in [26, 27].",
                "In these papers, the authors proposed to cluster the results of a real search engine based on the snippets or the contents of returned documents.",
                "Several clustering algorithms are compared and the Suffix Tree Clustering algorithm (STC) was shown to be the most effective one.",
                "They also showed that using snippets is as effective as using whole documents.",
                "However, an important challenge of document clustering is to generate meaningful labels for clusters.",
                "To overcome this difficulty, in [28], supervised learning algorithms were studied to extract meaningful phrases from the search result snippets and these phrases were then used to group search results.",
                "In [13], the authors proposed to use a monothetic clustering algorithm, in which a document is assigned to a cluster based on a single feature, to organize search results, and the single feature is used to label the corresponding cluster.",
                "Clustering search results has also attracted a lot of attention in industry and commercial Web services such as Vivisimo [22].",
                "However, in all these works, the clusters are generated solely based on the search results.",
                "Thus the obtained clusters do not necessarily reflect users preferences and the generated labels may not be informative from a users viewpoint.",
                "Methods of organizing search results based on text categorization are studied in [6, 8].",
                "In this work, a text classifier is trained using a Web directory and search results are then classified into the predefined categories.",
                "The authors designed and studied different category interfaces and they found that category interfaces are more effective than list interfaces.",
                "However predefined categories are often too general to reflect the finer granularity aspects of a query.",
                "Search logs have been exploited for several different purposes in the past.",
                "For example, clustering search queries to find those Frequent Asked Questions (FAQ) is studied in [24, 4].",
                "Recently, search logs have been used for suggesting query substitutes [12], personalized search [19], Web site design [3], Latent Semantic Analysis [23], and learning retrieval ranking functions [16, 10, 1].",
                "In our work, we explore past query history in order to better organize the search results for future queries.",
                "We use the star clustering algorithm [2], which is a graph partition based approach, to learn interesting aspects from search logs given a new query.",
                "Thus past queries are clustered in a query specific manner and this is another difference from previous works such as [24, 4] in which all queries in logs are clustered in an oﬄine batch manner. 3.",
                "SEARCH ENGINE LOGS Search engine logs record the activities of Web users, which reflect the actual users needs or interests when conducting ID Query URL Time 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 time zones http://www.timeanddate.com xxxx ... ... ... ... Table 1: Sample entries of search engine logs.",
                "Different IDs mean different sessions.",
                "Web search.",
                "They generally have the following information: text queries that users submitted, the URLs that they clicked after submitting the queries, and the time when they clicked.",
                "Search engine logs are separated by sessions.",
                "A session includes a single query and all the URLs that a user clicked after issuing the query [24].",
                "A small sample of search log data is shown in Table 1.",
                "Our idea of using search engine logs is to treat these logs as past history, learn users interests using this history data automatically, and represent their interests by representative queries.",
                "For example, in the search logs, a lot of queries are related to car and this reflects that a large number of users are interested in information about car.",
                "Different users are probably interested in different aspects of car.",
                "Some are looking for renting a car, thus may submit a query like car rental; some are more interested in buying a used car, and may submit a query like used car; and others may care more about buying a car accessory, so they may use a query like car audio.",
                "By mining all the queries which are related to the concept of car, we can learn the aspects that are likely interesting from a users perspective.",
                "As an example, the following is some aspects about car learned from our search log data (see Section 5). 1. car rental, hertz car rental, enterprise car rental, ... 2. car pricing, used car, car values, ... 3. car accidents, car crash, car wrecks, ... 4. car audio, car stereo, car speaker, ...",
                "In order to learn aspects from search engine logs, we preprocess the raw logs to build a history data collection.",
                "As shown above, search engine logs consist of sessions.",
                "Each session contains the information of the text query and the clicked Web page URLs, together with the time that the user did the clicks.",
                "However, this information is limited since URLs alone are not informative enough to tell the intended meaning of a submitted query accurately.",
                "To gather rich information, we enrich each URL with additional text content.",
                "Specifically, given the query in a session, we obtain its top-ranked results using the search engine from which we obtained our log data, and extract the snippets of the URLs that are clicked on according to the log information in the corresponding session.",
                "All the titles, snippets, and URLs of the clicked Web pages of that query are used to represent the session.",
                "Different sessions may contain the same queries.",
                "Thus the number of sessions could be quite huge and the information in the sessions with the same queries could be redundant.",
                "In order to improve the scalability and reduce data sparseness, we aggregate all the sessions which contain exactly the same queries together.",
                "That is, for each unique query, we build a pseudo-document which consists of all the descriptions of its clicks in all the sessions aggregated.",
                "The keywords contained in the queries themselves can be regarded as brief summaries of the pseudo-documents.",
                "All these pseudo-documents form our history data collection, which is used to learn interesting aspects in the following section. 4.",
                "OUR APPROACH Our approach is to organize search results by aspects learned from search engine logs.",
                "Given an input query, the general procedure of our approach is: 1.",
                "Get its related information from search engine logs.",
                "All the information forms a working set. 2.",
                "Learn aspects from the information in the working set.",
                "These aspects correspond to users interests given the input query.",
                "Each aspect is labeled with a representative query. 3.",
                "Categorize and organize the search results of the input query according to the aspects learned above.",
                "We now give a detailed presentation of each step. 4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.",
                "To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.",
                "Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.",
                "Each Qi corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.",
                "To find qs related queries in H, a natural way is to use a text retrieval algorithm.",
                "Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.",
                "Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history collection.",
                "Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.",
                "Each document in H corresponds to a past query, and thus the top ranked documents correspond to qs related past queries. 4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.",
                "In this subsection, we propose to use a clustering method to discover these aspects.",
                "Any clustering algorithm could be applied here.",
                "In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].",
                "A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.",
                "We describe the star clustering algorithm below. 4.2.1 Star Clustering Given Hq, star clustering starts with constructing a pairwise similarity graph on this collection based on the vector space model in information retrieval [18].",
                "Then the clusters are formed by dense subgraphs that are star-shaped.",
                "These clusters form a cover of the similarity graph.",
                "Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.",
                "Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .",
                "A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.",
                "Each document di is a vertex of Gσ.",
                "If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.",
                "After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.",
                "Associate every vertex in Gσ with a flag, initialized as unmarked. 2.",
                "From those unmarked vertices, find the one which has the highest degree and let it be u. 3.",
                "Mark the flag of u as center. 4.",
                "Form a cluster C containing u and all its neighbors that are not marked as center.",
                "Mark all the selected neighbors as satellites. 5.",
                "Repeat from step 2 until all the vertices in Gσ are marked.",
                "Each cluster is star-shaped, which consists a single center and several satellites.",
                "There is only one parameter σ in the star clustering algorithm.",
                "A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.",
                "On the other hand, a small σ will make the clusters big and less coherent.",
                "We will study the impact of this parameter in our experiments.",
                "A good feature of the star clustering algorithm is that it outputs a center for each cluster.",
                "In the past query collection Hq, each document corresponds to a query.",
                "This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.",
                "All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users. 4.3 Categorizing Search Results In order to organize the search results according to users interests, we use the learned aspects from the related past queries to categorize the search results.",
                "Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.",
                "In principle, any categorization algorithm can be used here.",
                "Here we use a simple centroid-based method for categorization.",
                "Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better performance.",
                "Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.",
                "All these pis are used to categorize the search results.",
                "Specifically, for any search result sj, we build a TF-IDF vector.",
                "The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.",
                "We then assign sj to the aspect with which it has the highest cosine similarity score.",
                "All the aspects are finally ranked according to the number of search results they have.",
                "Within each aspect, the search results are ranked according to their original search engine ranking. 5.",
                "DATA COLLECTION We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].",
                "In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.",
                "There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.",
                "To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine accumulated, and we use the last 1/3 to simulate future queries.",
                "In the history collection, we clean the data by only keeping those frequent, well-formatted, English queries (queries which only contain characters a, b, ..., z, and space, and appear more than 5 times).",
                "After cleaning, we get 169,057 unique queries in our history data collection totally.",
                "On average, each query has 3.5 distinct clicks.",
                "We build the pseudo-documents for all these queries as described in Section 3.",
                "The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.",
                "We construct our test data from the last 1/3 data.",
                "According to the time, we separate this data into two test sets equally for cross-validation to set parameters.",
                "For each test set, we use every session as a test case.",
                "Each session contains a single query and several clicks. (Note that we do not aggregate sessions for test cases.",
                "Different test cases may have the same queries but possibly different clicks.)",
                "Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.",
                "Using clicks as judgments, we can then compare different algorithms for organizing search results to see how well these algorithms can help users reach the clicked URLs.",
                "Organizing search results into different aspects is expected to help informational queries.",
                "It thus makes sense to focus on the informational queries in our evaluation.",
                "For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.",
                "Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.",
                "Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.",
                "On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 6.",
                "EXPERIMENTS In the section, we describe our experiments on the search result organization based past search engine logs. 6.1 Experimental Design We use two baseline methods to evaluate the proposed method for organizing search results.",
                "For each test case, the first method is the default ranked list from a search engine (baseline).",
                "The second method is to organize the search results by clustering them (cluster-based).",
                "For fair comparison, we use the same clustering algorithm as our logbased method (i.e., star clustering).",
                "That is, we treat each search result as a document, construct the similarity graph, and find the star-shaped clusters.",
                "We compare our method (log-based) with the two baseline methods in the following experiments.",
                "For both cluster-based and log-based methods, the search results within each cluster is ranked based on their original ranking given by the search engine.",
                "To compare different result organization methods, we adopt a similar method as in the paper [9].",
                "That is, we compare the quality (e.g., precision) of the best cluster, which is defined as the one with the largest number of relevant documents.",
                "Organizing search results into clusters is to help users navigate into relevant documents quickly.",
                "The above metric is to simulate a scenario when users always choose the right cluster and look into it.",
                "Specifically, we download and organize the top 100 search results into aspects for each test case.",
                "We use Precision at 5 documents (P@5) in the best cluster as the primary measure to compare different methods.",
                "P@5 is a very meaningful measure as it tells us the perceived precision when the user opens a cluster and looks at the first 5 documents.",
                "We also use Mean Reciprocal Rank (MRR) as another metric.",
                "MRR is calculated as MRR = 1 |T|   q∈T 1 rq where T is a set of test queries, rq is the rank of the first relevant document for q.",
                "To give a fair comparison across different organization algorithms, we force both cluster-based and log-based methods to output the same number of aspects and force each search result to be in one and only one aspect.",
                "The number of aspects is fixed at 10 in all the following experiments.",
                "The star clustering algorithm can output different number of clusters for different input.",
                "To constrain the number of clusters to 10, we order all the clusters by their sizes, select the top 10 as aspect candidates.",
                "We then re-assign each search result to one of these selected 10 aspects that has the highest similarity score with the corresponding aspect centroid.",
                "In our experiments, we observe that the sizes of the best clusters are all larger than 5, and this ensures that P@5 is a meaningful metric. 6.2 Experimental Results Our main hypothesis is that organizing search results based on the users interests learned from a search log data set is more beneficial than to organize results using a simple list or cluster search results.",
                "In the following, we test our hypothesis from two perspectives - organization and labeling.",
                "Method Test set 1 Test set 2 MMR P@5 MMR P@5 Baseline 0.7347 0.3325 0.7393 0.3288 Cluster-based 0.7735 0.3162 0.7666 0.2994 Log-based 0.7833 0.3534 0.7697 0.3389 Cluster/Baseline 5.28% -4.87% 3.69% -8.93% Log/Baseline 6.62% 6.31% 4.10% 3.09% Log/Cluster 1.27% 11.76% 0.40% 13.20% Table 2: Comparison of different methods by MMR and P@5.",
                "We also show the percentage of relative improvement in the lower part.",
                "Comparison Test set 1 Test set 2 Impr./Decr.",
                "Impr./Decr.",
                "Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Table 3: Pairwise comparison w.r.t the number of test cases whose P@5s are improved versus decreased w.r.t the baseline. 6.2.1 Overall performance We compare three methods, basic search engine ranking (baseline), traditional clustering based method (clusterbased), and our log based method (log-based), in Table 2 using MRR and P@5.",
                "We optimize the parameter σs for each collection individually based on P@5 values.",
                "This shows the best performance that each method can achieve.",
                "In this table, we can see that in both test collections, our method is better than both the baseline and the cluster-based methods.",
                "For example, in the first test collection, the baseline method of MMR is 0.734, the cluster-based method is 0.773 and our method is 0.783.",
                "We achieve higher accuracy than both cluster-based method (1.27% improvement) and the baseline method (6.62% improvement).",
                "The P@5 values are 0.332 for the baseline, 0.316 for cluster-based method, but 0.353 for our method.",
                "Our method improves over the baseline by 6.31%, while the cluster-based method even decreases the accuracy.",
                "This is because cluster-based method organizes the search results only based on the contents.",
                "Thus it could organize the results differently from users preferences.",
                "This confirms our hypothesis of the bias of the cluster-based method.",
                "Comparing our method with the cluster-based method, we achieve significant improvement on both test collections.",
                "The p-values of the significance tests based on P@5 on both collections are 0.01 and 0.02 respectively.",
                "This shows that our log-based method is effective to learn users preferences from the past query history, and thus it can organize the search results in a more useful way to users.",
                "We showed the optimal results above.",
                "To test the sensitivity of the parameter σ of our log-based method, we use one of the test sets to tune the parameter to be optimal and then use the tuned parameter on the other set.",
                "We compare this result (log tuned outside) with the optimal results of both cluster-based (cluster optimized) and log-based methods (log optimized) in Figure 1.",
                "We can see that, as expected, the performance using the parameter tuned on a separate set is worse than the optimal performance.",
                "However, our method still performs much better than the optimal results of cluster-based method on both test collections. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Test set 1 Test set 2 P@5 cluster optimized log optimized log tuned outside Figure 1: Results using parameters tuned from the other test collection.",
                "We compare it with the optimal performance of the cluster-based and our logbased methods. 0 10 20 30 40 50 60 1 2 3 4 Bin number #Queries Improved Decreased Figure 2: The correlation between performance change and result diversity.",
                "In Table 3, we show pairwise comparisons of the three methods in terms of the numbers of test cases for which P@5 is increased versus decreased.",
                "We can see that our method improves more test cases compared with the other two methods.",
                "In the next section, we show more detailed analysis to see what types of test cases can be improved by our method. 6.2.2 Detailed Analysis To better understand the cases where our log-based method can improve the accuracy, we test two properties: result diversity and query difficulty.",
                "All the analysis below is based on test set 1.",
                "Diversity Analysis: Intuitively, organizing search results into different aspects is more beneficial to those queries whose results are more diverse, as for such queries, the results tend to form two or more big clusters.",
                "In order to test the hypothesis that log-based method help more those queries with diverse results, we compute the size ratios of the biggest and second biggest clusters in our log-based results and use this ratio as an indicator of diversity.",
                "If the ratio is small, it means that the first two clusters have a small difference thus the results are more diverse.",
                "In this case, we would expect our method to help more.",
                "The results are shown in Figure 2.",
                "In this figure, we partition the ratios into 4 bins.",
                "The 4 bins correspond to the ratio ranges [1, 2), [2, 3), [3, 4), and [4, +∞) respectively. ([i, j) means that i ≤ ratio < j.)",
                "In each bin, we count the numbers of test cases whose P@5s are improved versus decreased with respect to the ranking baseline, and plot the numbers in this figure.",
                "We can observe that when the ratio is smaller, the log-based method can improve more test cases.",
                "But when 0 5 10 15 20 25 30 1 2 3 4 Bin number #Queries Improved Decreased Figure 3: The correlation between performance change and query difficulty. the ratio is large, the log-based method can not improve over the baseline.",
                "For example, in bin 1, 48 test cases are improved and 34 are decreased.",
                "But in bin 4, all the 4 test cases are decreased.",
                "This confirms our hypothesis that our method can help more if the query has more diverse results.",
                "This also suggests that we should turn off the option of re-organizing search results if the results are not very diverse (e.g., as indicated by the cluster size ratio).",
                "Difficulty Analysis: Difficult queries have been studied in recent years [7, 25, 5].",
                "Here we analyze the effectiveness of our method in helping difficult queries.",
                "We quantify the query difficulty by the Mean Average Precision (MAP) of the original search engine ranking for each test case.",
                "We then order the 172 test cases in test set 1 in an increasing order of MAP values.",
                "We partition the test cases into 4 bins with each having a roughly equal number of test cases.",
                "A small MAP means that the utility of the original ranking is low.",
                "Bin 1 contains those test cases with the lowest MAPs and bin 4 contains those test cases with the highest MAPs.",
                "For each bin, we compute the numbers of test cases whose P@5s are improved versus decreased.",
                "Figure 3 shows the results.",
                "Clearly, in bin 1, most of the test cases are improved (24 vs 3), while in bin 4, log-based method may decrease the performance (3 vs 20).",
                "This shows that our method is more beneficial to difficult queries, which is as expected since clustering search results is intended to help difficult queries.",
                "This also shows that our method does not really help easy queries, thus we should turn off our organization option for easy queries. 6.2.3 Parameter Setting We examine parameter sensitivity in this section.",
                "For the star clustering algorithm, we study the similarity threshold parameter σ.",
                "For the OKAPI retrieval function, we study the parameters k1 and b.",
                "We also study the impact of the number of past queries retrieved in our log-based method.",
                "Figure 4 shows the impact of the parameter σ for both cluster-based and log-based methods on both test sets.",
                "We vary σ from 0.05 to 0.3 with step 0.05.",
                "Figure 4 shows that the performance is not very sensitive to the parameter σ.",
                "We can always obtain the best result in range 0.1 ≤ σ ≤ 0.25.",
                "In Table 4, we show the impact of OKAPI parameters.",
                "We vary k1 from 1.0 to 2.0 with step 0.2 and b from 0 to 1 with step 0.2.",
                "From this table, it is clear that P@5 is also not very sensitive to the parameter setting.",
                "Most of the values are larger than 0.35.",
                "The default values k1 = 1.2 and b = 0.8 give approximately optimal results.",
                "We further study the impact of the amount of history 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 similarity threhold: sigma cluster-based 1 log-based 1 cluster-based 2 log-based 2 Figure 4: The impact of similarity threshold σ on both cluster-based and log-based methods.",
                "We show the result on both test collections. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Table 4: Impact of OKAPI parameters k1 and b. information to learn from by varying the number of past queries to be retrieved for learning aspects.",
                "The results on both test collections are shown in Figure 5.",
                "We can see that the performance gradually increases as we enlarge the number of past queries retrieved.",
                "Thus our method could potentially learn more as we accumulate more history.",
                "More importantly, as time goes, more and more queries will have sufficient history, so we can improve more and more queries. 6.2.4 An Illustrative Example We use the query area codes to show the difference in the results of the log-based method and the cluster-based method.",
                "This query may mean phone codes or zip codes.",
                "Table 5 shows the representative keywords extracted from the three biggest clusters of both methods.",
                "In the clusterbased method, the results are partitioned based on locations: local or international.",
                "In the log-based method, the results are disambiguated into two senses: phone codes or zip codes.",
                "While both are reasonable partitions, our evaluation indicates that most users using such a query are often interested in either phone codes or zip codes. since the P@5 values of cluster-based and log-based methods are 0.2 and 0.6, respectively.",
                "Therefore our log-based method is more effective in helping users to navigate into their desired results.",
                "Cluster-based method Log-based method city, state telephone, city, international local, area phone, dialing international zip, postal Table 5: An example showing the difference between the cluster-based method and our log-based method 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #queries retrieved Test set 1 Test set 2 Figure 5: The impact of the number of past queries retrieved. 6.2.5 Labeling Comparison We now compare the labels between the cluster-based method and log-based method.",
                "The cluster-based method has to rely on the keywords extracted from the snippets to construct the label for each cluster.",
                "Our log-based method can avoid this difficulty by taking advantage of queries.",
                "Specifically, for the cluster-based method, we count the frequency of a keyword appearing in a cluster and use the most frequent keywords as the cluster label.",
                "For log-based method, we use the center of each star cluster as the label for the corresponding cluster.",
                "In general, it is not easy to quantify the readability of a cluster label automatically.",
                "We use examples to show the difference between the cluster-based and the log-based methods.",
                "In Table 6, we list the labels of the top 5 clusters for two examples jaguar and apple.",
                "For the cluster-based method, we separate keywords by commas since they do not form a phrase.",
                "From this table, we can see that our log-based method gives more readable labels because it generates labels based on users queries.",
                "This is another advantage of our way of organizing search results over the clustering approach.",
                "Label comparison for query jaguar Log-based method Cluster-based method 1. jaguar animal 1. jaguar, auto, accessories 2. jaguar auto accessories 2. jaguar, type, prices 3. jaguar cats 3. jaguar, panthera, cats 4. jaguar repair 4. jaguar, services, boston 5. jaguar animal pictures 5. jaguar, collection, apparel Label comparison for query apple Log-based method Cluster-based method 1. apple computer 1. apple, support, product 2. apple ipod 2. apple, site, computer 3. apple crisp recipe 3. apple, world, visit 4. fresh apple cake 4. apple, ipod, amazon 5. apple laptop 5. apple, products, news Table 6: Cluster label comparison. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we studied the problem of organizing search results in a user-oriented manner.",
                "To attain this goal, we rely on search engine logs to learn interesting aspects from users perspective.",
                "Given a query, we retrieve its related queries from past query history, learn the aspects by clustering the past queries and the associated clickthrough information, and categorize the search results into the aspects learned.",
                "We compared our log-based method with the traditional cluster-based method and the baseline of search engine ranking.",
                "The experiments show that our log-based method can consistently outperform cluster-based method and improve over the ranking baseline, especially when the queries are difficult or the search results are diverse.",
                "Furthermore, our log-based method can generate more meaningful aspect labels than the cluster labels generated based on search results when we cluster search results.",
                "There are several interesting directions for further extending our work: First, although our experiment results have clearly shown promise of the idea of learning from search logs to organize search results, the methods we have experimented with are relatively simple.",
                "It would be interesting to explore other potentially more effective methods.",
                "In particular, we hope to develop probabilistic models for learning aspects and organizing results simultaneously.",
                "Second, with the proposed way of organizing search results, we can expect to obtain informative feedback information from a user (e.g., the aspect chosen by a user to view).",
                "It would thus be interesting to study how to further improve the organization of the results based on such feedback information.",
                "Finally, we can combine a general search log with any personal search log to customize and optimize the organization of search results for each individual user. 8.",
                "ACKNOWLEDGMENTS We thank the anonymous reviewers for their valuable comments.",
                "This work is in part supported by a Microsoft Live Labs Research Grant, a Google Research Grant, and an NSF CAREER grant IIS-0347933. 9.",
                "REFERENCES [1] E. Agichtein, E. Brill, and S. T. Dumais.",
                "Improving web search ranking by incorporating user behavior information.",
                "In SIGIR, pages 19-26, 2006. [2] J.",
                "A. Aslam, E. Pelekov, and D. Rus.",
                "The star clustering algorithm for static and dynamic information organization.",
                "Journal of Graph Algorithms and Applications, 8(1):95-129, 2004. [3] R. A. Baeza-Yates.",
                "Applications of web query mining.",
                "In ECIR, pages 7-22, 2005. [4] D. Beeferman and A. L. Berger.",
                "Agglomerative clustering of a search engine query log.",
                "In KDD, pages 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.",
                "What makes a query difficult?",
                "In SIGIR, pages 390-397, 2006. [6] H. Chen and S. T. Dumais.",
                "Bringing order to the web: automatically categorizing search results.",
                "In CHI, pages 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.",
                "Predicting query performance.",
                "In Proceedings of ACM SIGIR 2002, pages 299-306, 2002. [8] S. T. Dumais, E. Cutrell, and H. Chen.",
                "Optimizing search by showing results in context.",
                "In CHI, pages 277-284, 2001. [9] M. A. Hearst and J. O. Pedersen.",
                "Reexamining the cluster hypothesis: Scatter/gather on retrieval results.",
                "In SIGIR, pages 76-84, 1996. [10] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In KDD, pages 133-142, 2002. [11] T. Joachims.",
                "Evaluating Retrieval Performance Using Clickthrough Data., pages 79-96.",
                "Physica/Springer Verlag, 2003. in J. Franke and G. Nakhaeizadeh and I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani, and W. Greiner.",
                "Generating query substitutions.",
                "In WWW, pages 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and R. Krishnapuram.",
                "A hierarchical monothetic document clustering algorithm for summarization and browsing search results.",
                "In WWW, pages 658-665, 2004. [14] Microsoft Live Labs.",
                "Accelerating search in academic research, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst, and C. Diehl.",
                "Scatter/gather browsing communicates the topic structure of a very large text collection.",
                "In CHI, pages 213-220, 1996. [16] F. Radlinski and T. Joachims.",
                "Query chains: learning to rank from implicit feedback.",
                "In KDD, pages 239-248, 2005. [17] S. E. Robertson and S. Walker.",
                "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.",
                "In SIGIR, pages 232-241, 1994. [18] G. Salton, A. Wong, and C. S. Yang.",
                "A vector space model for automatic indexing.",
                "Commun.",
                "ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan, and C. Zhai.",
                "Context-sensitive information retrieval using implicit feedback.",
                "In SIGIR, pages 43-50, 2005. [20] C. J. van Rijsbergen.",
                "Information Retrieval, second edition.",
                "Butterworths, London, 1979. [21] V. N. Vapnik.",
                "The Nature of Statistical Learning Theory.",
                "Springer-Verlag, Berlin, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen, and C. Zhai.",
                "Latent semantic analysis for multiple-type interrelated data objects.",
                "In SIGIR, pages 236-243, 2006. [24] J.-R. Wen, J.-Y.",
                "Nie, and H. Zhang.",
                "Clustering user queries of a search engine.",
                "In WWW, pages 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.",
                "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval.",
                "In SIGIR, pages 512-519, 2005. [26] O. Zamir and O. Etzioni.",
                "Web document clustering: A feasibility demonstration.",
                "In SIGIR, pages 46-54, 1998. [27] O. Zamir and O. Etzioni.",
                "Grouper: A dynamic clustering interface to web search results.",
                "Computer Networks, 31(11-16):1361-1374, 1999. [28] H.-J.",
                "Zeng, Q.-C.",
                "He, Z. Chen, W.-Y.",
                "Ma, and J. Ma.",
                "Learning to cluster web search results.",
                "In SIGIR, pages 210-217, 2004."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}