{
    "id": "I-68",
    "original_text": "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve. In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs. Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP. First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval. Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP. We test both improvements independently in a crisis-management domain as well as for other types of domains. Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations. Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1. INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2]. Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time. Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs). Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research. In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses. Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks. To remedy that, locally optimal algorithms have been proposed [12] [4] [5]. In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons. Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains. OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration. However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values. The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval. Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods. This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies. In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP. VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval. Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions. Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem. This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building. In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers. Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements. Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2. MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome. One example domain is large-scale disaster, like a fire in a skyscraper. Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless. In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations. Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 . General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading. The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B. As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3). Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc. We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B. One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians. If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians. In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan. Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B. Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3. MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 . Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents. Agents cannot communicate during mission execution. Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø. Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time. Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations. In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system. Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints. For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates. In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints. We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system. For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints. Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline. Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi. Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents. Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W). In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + . In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution. Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled. The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a. A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4. SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used. Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}. This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ . Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11]. The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used. However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising. Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods. The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible. At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ]. It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ]. This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration. We call this step a value propagation phase. Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π . It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods. We call this step a probability propagation phase. If policy π does not improve π, the algorithm terminates. There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper. First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed. While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 . Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow. Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods. As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again. As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences. In the next two sections, we address both of these shortcomings. 5. VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase. However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase. To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled. Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future. Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise. We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods. At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived. Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ]. The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods. Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t). Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6. We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme). Figure 2: Fragment of an MDP of agent Ak. Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left). Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed. It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 . Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1. Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable. Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 . Furthermore, Vj0,i0 should be non-increasing. Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }. Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 . Let Ak be an agent assigned to the method mi0 . If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 . Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 . Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored. We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 . In general, the value function propagation scheme starts with sink nodes. It then visits at each time a method m, such that all the methods that m enables have already been marked as visited. The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified. Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration. We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived. Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase. If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise. Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t . We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 . The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default. We then visit at each time a method m such that all the methods that enable The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited. The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 . Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1. These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm. Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation. In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation. However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions. When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t). If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does. As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ). Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P . We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1. Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively. The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . PROOF. In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1. Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1. Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1. We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ). Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 . So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). We refer to this approach, as heuristic H 1,1 . Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation. Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed. For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik . If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t). If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later. As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences. Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0. Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences. Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies. We call such problem a starvation of method mk. That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided. We now prove that: THEOREM 2. Heuristic H 1,1 can overestimate the opportunity cost. PROOF. We prove the theorem by showing a case where the overestimation occurs. For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively. Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t). From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing. Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice. To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 . To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split. Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). For the new heuristics, we now prove, that: THEOREM 3. Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost. PROOF. When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 . Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing. For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t). Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does. However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation. Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods. However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable. Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7. EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations. Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present. Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100). We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400. We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost. The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function. In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%. When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 . We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)). To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)). We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics. Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward. Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark. We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods. Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200. We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm. We then run both algorithms for a total of 100 policy improvement iterations. Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds). As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%. For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP. We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)). We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached. We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale). As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP. We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)). In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400. We show the results in Figure (7b). Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%. We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. In particular, we consider 836 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting. Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods. For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially. We therefore vary the time horizons from 3000 to 4000, and then to 5000. We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8. CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains. In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs. Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP. In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4]. Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11]. Unfortunately, they fail to scale up to large-scale domains at present time. Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints. Finally, value function techniques have been studied in context of single agent MDPs [7] [9]. However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning. Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No. FA875005C0030. The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9. REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein. Decentralized MDPs with Event-Driven Interactions. In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman. Transition-Independent Decentralized Markov Decision Processes. In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman. The complexity of decentralized control of Markov decision processes. In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib. A polynomial algorithm for decentralized Markov decision processes with temporal constraints. In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib. An iterative algorithm for solving constrained decentralized Markov decision processes. In AAAI, pages 1089-1094, 2006. [6] C. Boutilier. Sequential optimality and coordination in multiagent systems. In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman. Exact solutions to time-dependent MDPs. In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein. Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman. Lazy approximation for solving continuous finite-horizon MDPs. In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig. Risk-sensitive planning with one-switch utility functions: Value iteration. In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy. Coordinated plan management using multiagent MDPs. In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella. Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings. In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo. Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs. In IJCAI, pages 1758-1760, 2005. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837",
    "original_translation": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837",
    "original_sentences": [
        "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
        "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
        "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
        "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
        "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
        "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
        "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
        "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
        "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
        "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
        "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
        "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
        "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
        "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
        "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
        "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
        "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
        "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
        "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
        "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
        "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
        "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
        "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
        "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
        "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
        "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
        "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
        "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
        "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
        "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
        "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
        "One example domain is large-scale disaster, like a fire in a skyscraper.",
        "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
        "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
        "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
        "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
        "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
        "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
        "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
        "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
        "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
        "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
        "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
        "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
        "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
        "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
        "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
        "Agents cannot communicate during mission execution.",
        "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
        "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
        "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
        "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
        "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
        "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
        "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
        "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
        "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
        "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
        "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
        "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
        "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
        "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
        "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
        "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
        "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
        "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
        "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
        "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
        "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
        "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
        "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
        "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
        "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
        "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
        "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
        "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
        "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
        "We call this step a value propagation phase.",
        "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
        "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
        "We call this step a probability propagation phase.",
        "If policy π does not improve π, the algorithm terminates.",
        "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
        "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
        "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
        "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
        "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
        "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
        "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
        "In the next two sections, we address both of these shortcomings. 5.",
        "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
        "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
        "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
        "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
        "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
        "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
        "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
        "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
        "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
        "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
        "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
        "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
        "Figure 2: Fragment of an MDP of agent Ak.",
        "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
        "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
        "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
        "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
        "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
        "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
        "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
        "Furthermore, Vj0,i0 should be non-increasing.",
        "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
        "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
        "Let Ak be an agent assigned to the method mi0 .",
        "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
        "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
        "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
        "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
        "In general, the value function propagation scheme starts with sink nodes.",
        "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
        "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
        "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
        "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
        "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
        "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
        "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
        "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
        "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
        "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
        "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
        "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
        "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
        "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
        "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
        "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
        "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
        "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
        "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
        "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
        "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
        "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
        "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
        "PROOF.",
        "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
        "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
        "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
        "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
        "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
        "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
        "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
        "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
        "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
        "We refer to this approach, as heuristic H 1,1 .",
        "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
        "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
        "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
        "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
        "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
        "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
        "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
        "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
        "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
        "We call such problem a starvation of method mk.",
        "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
        "We now prove that: THEOREM 2.",
        "Heuristic H 1,1 can overestimate the opportunity cost.",
        "PROOF.",
        "We prove the theorem by showing a case where the overestimation occurs.",
        "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
        "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
        "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
        "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
        "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
        "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
        "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
        "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
        "For the new heuristics, we now prove, that: THEOREM 3.",
        "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
        "PROOF.",
        "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
        "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
        "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
        "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
        "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
        "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
        "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
        "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
        "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
        "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
        "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
        "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
        "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
        "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
        "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
        "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
        "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
        "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
        "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
        "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
        "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
        "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
        "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
        "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
        "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
        "We then run both algorithms for a total of 100 policy improvement iterations.",
        "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
        "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
        "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
        "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
        "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
        "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
        "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
        "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
        "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
        "We show the results in Figure (7b).",
        "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
        "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
        "In particular, we consider 836 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
        "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
        "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
        "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
        "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
        "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
        "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
        "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
        "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
        "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
        "Unfortunately, they fail to scale up to large-scale domains at present time.",
        "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
        "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
        "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
        "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
        "FA875005C0030.",
        "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
        "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
        "Decentralized MDPs with Event-Driven Interactions.",
        "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
        "Transition-Independent Decentralized Markov Decision Processes.",
        "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
        "The complexity of decentralized control of Markov decision processes.",
        "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
        "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
        "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
        "An iterative algorithm for solving constrained decentralized Markov decision processes.",
        "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
        "Sequential optimality and coordination in multiagent systems.",
        "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
        "Exact solutions to time-dependent MDPs.",
        "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
        "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
        "Lazy approximation for solving continuous finite-horizon MDPs.",
        "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
        "Risk-sensitive planning with one-switch utility functions: Value iteration.",
        "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
        "Coordinated plan management using multiagent MDPs.",
        "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
        "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
        "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
        "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
        "In IJCAI, pages 1758-1760, 2005.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
    ],
    "translated_text_sentences": [
        "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver.",
        "En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes.",
        "Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP.",
        "Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo.",
        "Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP.",
        "Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios.",
        "Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones.",
        "Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1.",
        "INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2].",
        "Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo.",
        "Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs).",
        "Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva.",
        "En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases.",
        "Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo.",
        "Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5].",
        "En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos.",
        "Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real.",
        "OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior.",
        "Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes.",
        "La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método.",
        "Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros.",
        "Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas.",
        "En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP.",
        "VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo.",
        "Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor.",
        "Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad.",
        "Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas.",
        "En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local.",
        "Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP.",
        "Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2.",
        "EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado.",
        "Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos.",
        "Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles.",
        "En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes.",
        "Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso).",
        "Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague.",
        "El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B.",
        "Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3).",
        "Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc.",
        "Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B.",
        "Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles.",
        "Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles.",
        "En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión.",
        "Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B.",
        "Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3.",
        "DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2.",
        "Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes.",
        "Los agentes no pueden comunicarse durante la ejecución de la misión.",
        "Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø.",
        "Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez.",
        "Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método.",
        "En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema.",
        "Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo.",
        "Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine.",
        "En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor.",
        "Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema.",
        "Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas.",
        "Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión.",
        "Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi.",
        "Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes.",
        "Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W).",
        "En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + .",
        "En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución.",
        "Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán.",
        "La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a.",
        "Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4.",
        "TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}.",
        "Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}.",
        "Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗.",
        "Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11].",
        "La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1].",
        "Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores.",
        "Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos.",
        "La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras.",
        "En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ].",
        "Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ].",
        "Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior.",
        "Llamamos a este paso una fase de propagación de valores.",
        "Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π.",
        "Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero.",
        "Llamamos a este paso una fase de propagación de probabilidad.",
        "Si la política π no mejora a π, el algoritmo termina.",
        "Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo.",
        "Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado.",
        "Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo.",
        "Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente.",
        "Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores.",
        "Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente.",
        "Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas.",
        "En las dos secciones siguientes, abordamos ambas deficiencias. 5.",
        "La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad.",
        "Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad.",
        "Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado.",
        "Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro.",
        "Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario.",
        "Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl.",
        "Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen.",
        "En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0.",
        "Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ].",
        "La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros.",
        "Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t).",
        "Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6.",
        "Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema).",
        "Figura 2: Fragmento de un MDP del agente Ak.",
        "Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda).",
        "Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado.",
        "Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0.",
        "Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
        "Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1.",
        "Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable.",
        "Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0.",
        "Además, Vj0,i0 debería ser no decreciente.",
        "Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }.",
        "Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0.",
        "Que Ak sea un agente asignado al método mi0.",
        "Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0.",
        "Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0.",
        "Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1.",
        "Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0.",
        "En general, el esquema de propagación de la función de valor comienza con los nodos sumidero.",
        "Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados.",
        "La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados.",
        "Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración.",
        "Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada.",
        "Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor.",
        "Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario.",
        "Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t.",
        "Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0.",
        "El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada.",
        "Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl.",
        "La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros.",
        "La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0.",
        "Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1.",
        "Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo.",
        "De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función.",
        "En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9].",
        "Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL).",
        "Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t).",
        "Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP.",
        "Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ).",
        "Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P.",
        "Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1.",
        "Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente.",
        "El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri.",
        "PRUEBA.",
        "Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1.",
        "Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1.",
        "Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1.",
        "Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P).",
        "Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
        "Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
        "Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
        "DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0.",
        "Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
        "Nos referimos a este enfoque como heurística H 1,1.",
        "Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez.",
        "Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0.",
        "Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik.",
        "Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t).",
        "Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde.",
        "Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas.",
        "De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0.",
        "Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas.",
        "Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica.",
        "Llamamos a este problema una falta de método mk.",
        "Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez.",
        "Ahora demostramos que: TEOREMA 2.",
        "La heurística H 1,1 puede sobreestimar el costo de oportunidad.",
        "PRUEBA.",
        "Demostramos el teorema mostrando un caso donde ocurre la sobreestimación.",
        "Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente.",
        "Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t).",
        "A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente.",
        "Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
        "Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica.",
        "Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0.",
        "Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir.",
        "Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
        "Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3.",
        "Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad.",
        "PRUEBA.",
        "Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0.",
        "Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente.",
        "Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
        "Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t).",
        "Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad.",
        "El Sexto Internacional.",
        "La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0.",
        "Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición.",
        "La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores.",
        "Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable.",
        "La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7.",
        "EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión.",
        "Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0.",
        "Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100).",
        "Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400.",
        "Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad.",
        "El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0.",
        "En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%.",
        "Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0.",
        "Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2).",
        "Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)).",
        "Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas.",
        "La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real.",
        "Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia.",
        "Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos.",
        "Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200.",
        "Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP.",
        "Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas.",
        "La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos).",
        "Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%.",
        "Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP.",
        "A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)).",
        "Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados.",
        "Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica).",
        "Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP.",
        "Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)).",
        "En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400.",
        "Mostramos los resultados en la Figura (7b).",
        "Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%.",
        "Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
        "En particular, consideramos 836 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad.",
        "Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos.",
        "Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente.",
        "Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000.",
        "Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP.",
        "CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real.",
        "En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes.",
        "Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP.",
        "En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4].",
        "Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11].",
        "Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad.",
        "Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales.",
        "Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9].",
        "Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada.",
        "Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No.",
        "FA875005C0030.",
        "Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9.",
        "REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein.",
        "MDPs descentralizados con interacciones impulsadas por eventos.",
        "En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman.",
        "Procesos de decisión de Markov descentralizados independientes de la transición.",
        "En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman.",
        "La complejidad del control descentralizado de procesos de decisión de Markov.",
        "En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib.",
        "Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales.",
        "En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib.",
        "Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones.",
        "En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier.",
        "Optimalidad secuencial y coordinación en sistemas multiagentes.",
        "En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman.",
        "Soluciones exactas para procesos de decisión de Markov dependientes del tiempo.",
        "En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein.",
        "Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman.",
        "Aproximación perezosa para resolver MDPs continuos de horizonte finito.",
        "En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig.",
        "Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor.",
        "En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy.",
        "Gestión de planes coordinados utilizando MDPs multiagentes.",
        "En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella.",
        "Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes.",
        "En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo.",
        "POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs.",
        "En IJCAI, páginas 1758-1760, 2005.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837"
    ],
    "error_count": 3,
    "keys": {
        "agent-coordination problem": {
            "translated_key": "problemas de coordinación de agentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of <br>agent-coordination problem</br>s in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of <br>agent-coordination problem</br>s, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of <br>agent-coordination problem</br>s in domains with uncertainty and time constraints but very difficult to solve.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of <br>agent-coordination problem</br>s, it is very difficult to solve, especially for the real-world domains."
            ],
            "translated_annotated_samples": [
                "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de <br>problemas de coordinación de agentes</br> en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver.",
                "CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar <br>problemas de coordinación de agentes</br>, es muy difícil de resolver, especialmente para los dominios del mundo real."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de <br>problemas de coordinación de agentes</br> en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar <br>problemas de coordinación de agentes</br>, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "value function propagation": {
            "translated_key": "Propagación de Función de Valor",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called <br>value function propagation</br> (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "<br>value function propagation</br> (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the <br>value function propagation</br> phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 <br>value function propagation</br> Phase Suppose, that we are performing a <br>value function propagation</br> phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the <br>value function propagation</br> scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The <br>value function propagation</br> phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since <br>value function propagation</br> phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last <br>value function propagation</br> phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the <br>value function propagation</br> phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of <br>value function propagation</br> phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single <br>value function propagation</br> operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of <br>value function propagation</br> operations is |C≺|, the total error π of the <br>value function propagation</br> phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when <br>value function propagation</br> for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called <br>value function propagation</br> (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "Our heuristic solution method, called <br>value function propagation</br> (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "<br>value function propagation</br> (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the <br>value function propagation</br> phase and the probability function propagation phase.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 <br>value function propagation</br> Phase Suppose, that we are performing a <br>value function propagation</br> phase during which the value functions are propagated from the sink methods to the source methods.",
                "In general, the <br>value function propagation</br> scheme starts with sink nodes."
            ],
            "translated_annotated_samples": [
                "Nuestro método de solución heurística, llamado <br>Propagación de Función de Valor</br> (VFP), combina dos mejoras ortogonales de OC-DEC-MDP.",
                "La <br>función de propagación de valor</br> (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad.",
                "Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como <br>la fase de propagación de la función de valor</br> y la fase de propagación de la función de probabilidad.",
                "Supongamos que estamos realizando una fase de <br>propagación de funciones de valor</br> durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen.",
                "En general, el esquema de propagación de la <br>función de valor</br> comienza con los nodos sumidero."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado <br>Propagación de Función de Valor</br> (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La <br>función de propagación de valor</br> (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como <br>la fase de propagación de la función de valor</br> y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de <br>propagación de funciones de valor</br> durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la <br>función de valor</br> comienza con los nodos sumidero. ",
            "candidates": [],
            "error": [
                [
                    "Propagación de Función de Valor",
                    "función de propagación de valor",
                    "la fase de propagación de la función de valor",
                    "propagación de funciones de valor",
                    "función de valor"
                ]
            ]
        },
        "decision-theoretic model": {
            "translated_key": "modelo teórico de decisión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, <br>decision-theoretic model</br>s have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key <br>decision-theoretic model</br>s that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "Because of the uncertain and dynamic characteristics of such domains, <br>decision-theoretic model</br>s have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key <br>decision-theoretic model</br>s that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs)."
            ],
            "translated_annotated_samples": [
                "Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo.",
                "Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs)."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "decentralized partially observable markov decision process": {
            "translated_key": "proceso de decisión de Markov parcialmente observable descentralizado",
            "is_in_text": false,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "opportunity cost": {
            "translated_key": "Costo de Oportunidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, <br>opportunity cost</br> DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the <br>opportunity cost</br>, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the <br>opportunity cost</br>, and to remedy that, we introduce a set of heuristics, that correct the <br>opportunity cost</br> overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the <br>opportunity cost</br> overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. <br>opportunity cost</br> Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where <br>opportunity cost</br> functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the <br>opportunity cost</br> Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full <br>opportunity cost</br>, postponing the discussion on different techniques for splitting the <br>opportunity cost</br> Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the <br>opportunity cost</br> of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the <br>opportunity cost</br> Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the <br>opportunity cost</br> Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and <br>opportunity cost</br> values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if <br>opportunity cost</br> functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE <br>opportunity cost</br> FUNCTIONS In section 5 we left out the discussion about how the <br>opportunity cost</br> function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the <br>opportunity cost</br> function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the <br>opportunity cost</br>, we discuss three problems that might occur when splitting the <br>opportunity cost</br> functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the <br>opportunity cost</br> function Vik from immediate reward rk and <br>opportunity cost</br> function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the <br>opportunity cost</br> for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the <br>opportunity cost</br> of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the <br>opportunity cost</br> function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the <br>opportunity cost</br>.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of <br>opportunity cost</br>, we must identify t0 ∈ [0, ..., Δ] such that the <br>opportunity cost</br> PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the <br>opportunity cost</br> PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the <br>opportunity cost</br> Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of <br>opportunity cost</br> overestimation, we propose three alternative heuristics that split the <br>opportunity cost</br> functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid <br>opportunity cost</br> overestimation, we normalize the split functions when their sum exceeds the <br>opportunity cost</br> function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the <br>opportunity cost</br>.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the <br>opportunity cost</br> function Vj0 , only one method (e.g. mik ) gets the <br>opportunity cost</br> for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the <br>opportunity cost</br> function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the <br>opportunity cost</br>.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the <br>opportunity cost</br>, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if <br>opportunity cost</br> functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split <br>opportunity cost</br> functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split <br>opportunity cost</br> function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different <br>opportunity cost</br> function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the <br>opportunity cost</br>.",
                "The first graph confirms, that when the <br>opportunity cost</br> function Vj0 was split into <br>opportunity cost</br> functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the <br>opportunity cost</br> functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the <br>opportunity cost</br> of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "In particular, <br>opportunity cost</br> DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "This reward, also referred to as the <br>opportunity cost</br>, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the <br>opportunity cost</br>, and to remedy that, we introduce a set of heuristics, that correct the <br>opportunity cost</br> overestimation problem.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the <br>opportunity cost</br> overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. <br>opportunity cost</br> Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled."
            ],
            "translated_annotated_samples": [
                "En particular, el <br>Costo de Oportunidad</br> DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos.",
                "Esta recompensa, también conocida como el <br>costo de oportunidad</br>, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas.",
                "Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el <br>costo de oportunidad</br>, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del <br>costo de oportunidad</br>.",
                "Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del <br>costo de oportunidad</br>, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2.",
                "Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de <br>Costo de Oportunidad</br>, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el <br>Costo de Oportunidad</br> DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el <br>costo de oportunidad</br>, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el <br>costo de oportunidad</br>, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del <br>costo de oportunidad</br>. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del <br>costo de oportunidad</br>, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de <br>Costo de Oportunidad</br>, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "policy iteration": {
            "translated_key": "iteración de políticas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous <br>policy iteration</br>.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous <br>policy iteration</br>."
            ],
            "translated_annotated_samples": [
                "OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la <br>iteración de políticas</br> anterior."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la <br>iteración de políticas</br> anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "rescue mission": {
            "translated_key": "misión de rescate",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage <br>rescue mission</br> [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage <br>rescue mission</br> [11] to the coordination of Autonomous Mars Exploration Rovers [2]."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una <br>misión de rescate</br> de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una <br>misión de rescate</br> de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "probability function propagation": {
            "translated_key": "función de probabilidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the <br>probability function propagation</br> phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and <br>probability function propagation</br> phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 <br>probability function propagation</br> Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the <br>probability function propagation</br> phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The <br>probability function propagation</br> phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and <br>probability function propagation</br> without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of <br>probability function propagation</br> phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the <br>probability function propagation</br> phase.",
                "We now develop an analytical technique for performing the value function and <br>probability function propagation</br> phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 <br>probability function propagation</br> Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "The general, the <br>probability function propagation</br> phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "The <br>probability function propagation</br> phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 ."
            ],
            "translated_annotated_samples": [
                "Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la <br>función de probabilidad</br>.",
                "Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la <br>función de probabilidad</br>. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl.",
                "La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados.",
                "El general, la fase de propagación de la <br>función de probabilidad</br> comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada.",
                "La fase de propagación de la <br>función de probabilidad</br> termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la <br>función de probabilidad</br>. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la <br>función de probabilidad</br>. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la <br>función de probabilidad</br> comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la <br>función de probabilidad</br> termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multiplication": {
            "translated_key": "multiplicaciones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) <br>multiplication</br>s to compute f(t), VFP only needs to perform O(k · Δ) <br>multiplication</br>s to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "As a result, instead of performing O(Δ2 ) <br>multiplication</br>s to compute f(t), VFP only needs to perform O(k · Δ) <br>multiplication</br>s to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ)."
            ],
            "translated_annotated_samples": [
                "Como resultado, en lugar de realizar O(Δ2) <br>multiplicaciones</br> para calcular f(t), VFP solo necesita realizar O(k · Δ) <br>multiplicaciones</br> para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ)."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) <br>multiplicaciones</br> para calcular f(t), VFP solo necesita realizar O(k · Δ) <br>multiplicaciones</br> para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "heuristic performance": {
            "translated_key": "rendimiento heurístico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the <br>heuristic performance</br>, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "To obtain the baseline for the <br>heuristic performance</br>, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a))."
            ],
            "translated_annotated_samples": [
                "Para obtener la línea base del <br>rendimiento heurístico</br>, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a))."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del <br>rendimiento heurístico</br>, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "multi-agent system": {
            "translated_key": "sistemas multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative <br>multi-agent system</br>s, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "Optimizing information exchange in cooperative <br>multi-agent system</br>s, 2003. [9] L. Li and M. Littman."
            ],
            "translated_annotated_samples": [
                "Optimizando el intercambio de información en <br>sistemas multiagente</br> cooperativos, 2003. [9] L. Li y M. Littman."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en <br>sistemas multiagente</br> cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "decentralized markov decision process": {
            "translated_key": "Proceso de Decisión de Markov Descentralizado",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS <br>decentralized markov decision process</br> (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "CONCLUSIONS <br>decentralized markov decision process</br> (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains."
            ],
            "translated_annotated_samples": [
                "CONCLUSIONES El <br>Proceso de Decisión de Markov Descentralizado</br> (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las restricciones temporales como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de restricciones temporales en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El <br>Proceso de Decisión de Markov Descentralizado</br> (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con restricciones temporales [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las restricciones temporales. Finalmente, las técnicas de función de valor han sido estudiadas en el contexto de MDPs de agente único [7] [9]. Sin embargo, al igual que [6], no logran abordar la falta de conocimiento del estado global, que es un problema fundamental en la planificación descentralizada. Agradecimientos: Este material se basa en trabajos respaldados por el programa COORDINATORS de DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el Contrato No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios. 9. REFERENCIAS [1] R. Becker, V. Lesser y S. Zilberstein. MDPs descentralizados con interacciones impulsadas por eventos. En AAMAS, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión de Markov descentralizados independientes de la transición. En AAMAS, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Immerman. La complejidad del control descentralizado de procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinómico para procesos de decisión de Markov descentralizados con restricciones temporales. En AAMAS, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimalidad secuencial y coordinación en sistemas multiagentes. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para procesos de decisión de Markov dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimizando el intercambio de información en sistemas multiagente cooperativos, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDPs continuos de horizonte finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un solo interruptor: Iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDPs multiagentes. En el Simposio de Primavera de AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Domando POMDP descentralizados: Hacia una computación eficiente de políticas para entornos multiagentes. En IJCAI, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDPs distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPs. En IJCAI, páginas 1758-1760, 2005. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 837 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "temporal constraint": {
            "translated_key": "restricciones temporales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both <br>temporal constraint</br>s and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with <br>temporal constraint</br>s and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of <br>temporal constraint</br>s in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with <br>temporal constraint</br>s [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and <br>temporal constraint</br>s.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with <br>temporal constraint</br>s.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [
                "Additionally, OC-DEC-MDP is unique in its ability to address both <br>temporal constraint</br>s and uncertain method execution durations, which is an important factor for real-world domains.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with <br>temporal constraint</br>s and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of <br>temporal constraint</br>s in the system.",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with <br>temporal constraint</br>s [1] [11].",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and <br>temporal constraint</br>s."
            ],
            "translated_annotated_samples": [
                "Además, OC-DEC-MDP es único en su capacidad para abordar tanto las <br>restricciones temporales</br> como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real.",
                "En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con <br>restricciones temporales</br> y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP.",
                "En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de <br>restricciones temporales</br> en el sistema.",
                "Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con <br>restricciones temporales</br> [1] [11].",
                "Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las <br>restricciones temporales</br>."
            ],
            "translated_text": "Sobre técnicas oportunísticas para resolver Procesos de Decisión de Markov Descentralizados con Restricciones Temporales Janusz Marecki y Milind Tambe Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {marecki, tambe}@usc.edu RESUMEN Los Procesos de Decisión de Markov Descentralizados (DEC-MDPs) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y restricciones de tiempo, pero muy difíciles de resolver. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que escala a DEC-MDPs más grandes. Nuestro método de solución heurística, llamado Propagación de Función de Valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP en un orden de magnitud al mantener y manipular una función de valor para cada estado (como función del tiempo) en lugar de un valor separado para cada par de estado e intervalo de tiempo. Además, logra una mejor calidad de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DEC-MDP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como en otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como una mayor calidad de solución en una variedad de situaciones. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial DistribuidaSistemas Multiagente Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes actuando como un equipo en dominios inciertos y críticos en tiempo se ha convertido recientemente en un campo de investigación muy activo con aplicaciones potenciales que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de Rovers de Exploración de Marte Autónomos [2]. Debido a las características inciertas y dinámicas de dichos dominios, los modelos de teoría de decisiones han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos clave de teoría de decisiones que se han vuelto populares en la literatura incluyen los Procesos de Decisión de Markov Descentralizados (DECMDPs) y los Procesos de Decisión de Markov Parcialmente Observables Descentralizados (DEC-POMDPs). Desafortunadamente, resolver estos modelos de manera óptima ha demostrado ser NEXP-completo [3], por lo tanto, subclases más manejables de estos modelos han sido objeto de una investigación intensiva. En particular, el POMDP Distribuido en Red [13], que asume que no todos los agentes interactúan entre sí, el DEC-MDP Independiente de Transición [2], que asume que la función de transición es descomponible en funciones de transición locales, o el DEC-MDP con Interacciones Dirigidas por Eventos [1], que asume que las interacciones entre agentes ocurren en puntos de tiempo fijos, constituyen buenos ejemplos de tales subclases. Aunque los algoritmos globalmente óptimos para estas subclases han demostrado resultados prometedores, los dominios en los que estos algoritmos se ejecutan siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos intervalos de tiempo. Para remediar eso, se han propuesto algoritmos óptimos locales [12] [4] [5]. En particular, el Costo de Oportunidad DEC-MDP [4] [5], referido como OC-DEC-MDP, es especialmente notable, ya que se ha demostrado que se escala a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar tanto las <br>restricciones temporales</br> como las duraciones de ejecución del método inciertas, lo cual es un factor importante para los dominios del mundo real. OC-DEC-MDP es capaz de escalar a dominios tan grandes principalmente porque en lugar de buscar la solución óptima global, lleva a cabo una serie de iteraciones de políticas; en cada iteración realiza una iteración de valores que reutiliza los datos calculados durante la iteración de políticas anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente a medida que el horizonte temporal y el número de métodos se acercan a valores grandes. La razón de los tiempos de ejecución prolongados de OC-DEC-MDP para tales dominios es una consecuencia de su enorme espacio de estados, es decir, OC-DEC-MDP introduce un estado separado para cada par posible de método e intervalo de ejecución del método. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir al permitir la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, desempeña un papel crucial en la toma de decisiones del agente, y como mostraremos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= Propagación de Función de Valor), una técnica de solución eficiente para el modelo DEC-MDP con <br>restricciones temporales</br> y duraciones de ejecución de métodos inciertas, que se basa en el éxito de OC-DEC-MDP. VFP introduce nuestras dos ideas ortogonales: Primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS y manipulamos una función de valor a lo largo del tiempo para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos temporales en los que la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. Segundo, demostramos (tanto teóricamente como empíricamente) que OC-DEC-MDP sobreestima el costo de oportunidad, y para remediarlo, introducimos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: En la sección 2 motivamos esta investigación presentando un dominio de rescate civil donde un equipo de bomberos debe coordinarse para rescatar a civiles atrapados en un edificio en llamas. En la sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con Restricciones Temporales y en la sección 4 discutimos cómo se podrían resolver los problemas codificados en nuestro modelo utilizando solucionadores óptimos a nivel global y local. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de vanguardia que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) Las nuevas heurísticas corrigen el problema de sobreestimación del costo de oportunidad, lo que conduce a políticas de mayor calidad, y (ii) Al permitir un intercambio sistemático de calidad de solución por tiempo, el algoritmo VFP se ejecuta mucho más rápido que el algoritmo OC-DEC-MDP 2. EJEMPLO MOTIVADOR Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes a lo largo del tiempo, a pesar de la incertidumbre en la duración de la ejecución del plan y el resultado. Un ejemplo de dominio es un desastre a gran escala, como un incendio en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden saturarse rápidamente y volverse inútiles. En particular, se deben enviar pequeños equipos de bomberos en misiones separadas para rescatar a los civiles atrapados en docenas de ubicaciones diferentes. Imagina un pequeño plan de misión de la Figura (1), donde se ha asignado la tarea a tres brigadas de bomberos de rescatar a los civiles atrapados en el sitio B, accesible desde el sitio A (por ejemplo, una oficina accesible desde el piso). Los procedimientos generales de lucha contra incendios implican tanto: (i) apagar las llamas, como (ii) ventilar el lugar para permitir que los gases tóxicos de alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar que el fuego se propague. El equipo estima que los civiles tienen 20 minutos antes de que el fuego en el sitio B se vuelva insoportable, y que el fuego en el sitio A debe ser apagado para abrir el acceso al sitio B. Como ha ocurrido en el pasado en desastres a gran escala, la comunicación a menudo se interrumpe; por lo tanto, asumimos en este ámbito que no hay comunicación entre los cuerpos de bomberos 1, 2 y 3 (denominados como CB1, CB2 y CB3). Por lo tanto, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el incendio en el sitio B, etc. Asignamos una recompensa de 50 por evacuar a los civiles del sitio B, y una recompensa menor de 20 por la exitosa ventilación del sitio A, ya que los propios civiles podrían lograr escapar del sitio B. Se puede ver claramente el dilema al que se enfrenta FB2: solo puede estimar las duraciones de los métodos de lucha contra incendios en el sitio A que serán ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado pronto, el fuego se propagará fuera de control, mientras que si FB2 espera con el método de ventilación demasiado tiempo, el fuego en el sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1 Explicamos la notación EST y LET en la sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente. Decisiones difíciles; en particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar a ventilar el sitio A, y luego (dependiendo del tiempo que tomó ventilar el sitio A), elegir cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando. 3. DESCRIPCIÓN DEL MODELO Codificamos nuestros problemas de decisión en un modelo al que nos referimos como MDP Descentralizado con Restricciones Temporales 2. Cada instancia de nuestros problemas de decisión puede ser descrita como una tupla M, A, C, P, R donde M = {mi} |M| i=1 es el conjunto de métodos, y A = {Ak} |A| k=1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente Ak está asignado a un conjunto Mk de métodos, de tal manera que S|A| k=1 Mk = M y ∀i,j;i=jMi ∩ Mj = ø. Además, cada método del agente Ak solo puede ejecutarse una vez, y el agente Ak solo puede ejecutar un método a la vez. Los tiempos de ejecución del método son inciertos y P = {pi} |M| i=1 es el conjunto de distribuciones de las duraciones de ejecución del método. En particular, pi(t) es la probabilidad de que la ejecución del método mi consuma tiempo t. C es un conjunto de <br>restricciones temporales</br> en el sistema. Los métodos están parcialmente ordenados y cada método tiene ventanas de tiempo fijas dentro de las cuales puede ser ejecutado, es decir, C = C≺ ∪ C[ ] donde C≺ es el conjunto de restricciones de predecesores y C[ ] es el conjunto de restricciones de ventanas de tiempo. Para c ∈ C≺, c = mi, mj significa que el método mi precede al método mj, es decir, la ejecución de mj no puede comenzar antes de que mi termine. En particular, para un agente Ak, todos sus métodos forman una cadena vinculada por restricciones de predecesor. Suponemos que el grafo G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes) y sus vértices fuente y sumidero identifican los métodos fuente y sumidero del sistema. Para c ∈ C[ ], c = mi, EST, LET significa que la ejecución de mi solo puede comenzar después del Tiempo de Inicio Más Temprano EST y debe finalizar antes del Tiempo de Finalización Más Tardío LET; permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjuntas. Aunque las distribuciones pi pueden extenderse a horizontes temporales infinitos, dadas las restricciones de la ventana de tiempo, el horizonte de planificación Δ = max m,τ,τ ∈C[ ] τ se considera como la fecha límite de la misión. Finalmente, R = {ri} |M| i=1 es el conjunto de recompensas no negativas, es decir, ri se obtiene al ejecutar exitosamente mi. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya hayan sido habilitados. También se podría utilizar el marco OC-DEC-MDP, que modela tanto las restricciones de tiempo como de recursos. La Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 831 por otros agentes. Por lo tanto, si mj ∈ Mk es el próximo método a ser ejecutado por el agente Ak y el tiempo actual es t ∈ [0, Δ], el agente tiene que tomar una decisión de si Ejecutar el método mj (denotado como E), o Esperar (denotado como W). En caso de que el agente Ak decida esperar, permanece inactivo durante un tiempo pequeño arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método mj) en el tiempo t + . En caso de que el agente Ak decida ejecutar el siguiente método, dos resultados son posibles: Éxito: El agente Ak recibe la recompensa rj y pasa al siguiente método (si existe) siempre y cuando se cumplan las siguientes condiciones: (i) Todos los métodos {mi| mi, mj ∈ C≺} que habilitan directamente el método mj ya han sido completados, (ii) La ejecución del método mj comenzó en algún momento dentro de la ventana de tiempo del método mj, es decir, ∃ mj ,τ,τ ∈C[ ] tal que t ∈ [τ, τ ], y (iii) La ejecución del método mj finalizó dentro de la misma ventana de tiempo, es decir, el agente Ak completó el método mj en un tiempo menor o igual a τ − t. Fracaso: Si alguna de las condiciones mencionadas anteriormente no se cumple, el agente Ak detiene su ejecución. Otros agentes pueden continuar con su ejecución, pero los métodos mk ∈ {m| mj, m ∈ C≺} nunca se activarán. La política πk de un agente Ak es una función πk : Mk × [0, Δ] → {W, E}, y πk( m, t ) = a significa que si Ak está en el método m en el tiempo t, elegirá realizar la acción a. Una política conjunta π = [πk] |A| k=1 se considera óptima (denotada como π∗), si maximiza la suma de recompensas esperadas para todos los agentes. 4. TÉCNICAS DE SOLUCIÓN 4.1 Algoritmos óptimos La política conjunta óptima π∗ suele encontrarse utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método mj, se utilizan las políticas óptimas para los métodos mk ∈ {m| mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método mj también depende de las políticas para los métodos mi ∈ {m| m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método mj en el tiempo t también depende de la probabilidad de que el método mj esté habilitado en el tiempo t. En consecuencia, si el tiempo está discretizado, es necesario considerar Δ|M| políticas candidatas para encontrar π∗. Por lo tanto, es poco probable que los algoritmos globalmente óptimos utilizados para resolver problemas del mundo real terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida; en particular, si cada método mj se permitiera estar habilitado en puntos de tiempo t ∈ Tj ⊂ [0, Δ], se podría utilizar el Algoritmo de Conjunto de Cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es exponencial doble en el tamaño de Ti, y para nuestros dominios Tj puede almacenar todos los valores que van desde 0 hasta Δ. 4.2 Algoritmos Localmente Óptimos Dada la limitada aplicabilidad de los algoritmos globalmente óptimos para DEC-MDPs con Restricciones Temporales, los algoritmos localmente óptimos parecen más prometedores. Específicamente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado poder escalarse fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la política de tiempo de inicio más temprana π0 (según la cual un agente comenzará a ejecutar el método m tan pronto como m tenga una probabilidad distinta de cero de estar ya habilitado), y luego mejorarla de forma iterativa, hasta que no sea posible realizar más mejoras. En cada iteración, el algoritmo comienza con una política π, que determina de manera única las probabilidades Pi,[τ,τ ] de que el método mi se realice en el intervalo de tiempo [τ, τ ]. Luego realiza dos pasos: Paso 1: Propaga desde los métodos de destino a los métodos de origen los valores Vi,[τ,τ], que representan la utilidad esperada de ejecutar el método mi en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi,[τ,τ ] de la iteración del algoritmo anterior. Llamamos a este paso una fase de propagación de valores. Paso 2: Dados los valores Vi,[τ,τ ] del Paso 1, el algoritmo elige los intervalos de ejecución del método más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi,[τ,τ ] desde los métodos fuente a los métodos sumidero. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora a π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este artículo. Primero, cada uno de los estados OC-DEC-MDP es un par mj, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el cual el método mj puede ser ejecutado. Si bien esta representación estatal es beneficiosa, ya que el problema se puede resolver con un algoritmo estándar de iteración de valores, difumina el mapeo intuitivo del tiempo t a la recompensa total esperada por comenzar la ejecución de mj en el tiempo t. En consecuencia, si algún método mi habilita el método mj, y se conocen los valores Vj,[τ,τ ]∀τ,τ ∈[0,Δ], la operación que calcula los valores Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (durante la fase de propagación de valores), se ejecuta en tiempo O(I2), donde I es el número de intervalos de tiempo. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para horizontes temporales grandes Δ, el algoritmo OC-DECMDP se ejecuta lentamente. Segundo, si bien OC-DEC-MDP se enfoca en el cálculo preciso de los valores Vj,[τ,τ], no aborda un problema crítico que determina cómo se dividen los valores Vj,[τ,τ] dado que el método mj tiene múltiples métodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide Vj,[τ,τ ] en partes que pueden sobreestimar Vj,[τ,τ ] al sumarse nuevamente. Como resultado, los métodos que preceden al método mj sobreestiman el valor para habilitar mj, lo cual, como mostraremos más adelante, puede tener consecuencias desastrosas. En las dos secciones siguientes, abordamos ambas deficiencias. 5. La función de propagación de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, en el sentido de que realiza una serie de iteraciones de mejora de política, cada una de las cuales implica una Fase de Propagación de Valor y Probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Con este fin, para cada método mi ∈ M, definimos tres nuevas funciones: Función de Valor, denotada como vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t. Función de Costo de Oportunidad, denotada como Vi(t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada por comenzar la ejecución del método mi en el tiempo t asumiendo que mi está habilitado. Función de probabilidad, denotada como Pi(t), que mapea el tiempo t ∈ [0, Δ] a la probabilidad de que el método mi se complete antes del tiempo t. Esta representación funcional nos permite leer fácilmente la política actual, es decir, si un agente Ak está en el método mi en el tiempo t, entonces esperará siempre y cuando la función de valor vi(t) sea mayor en el futuro. Formalmente: πk( mi, t ) = j W si ∃t >t tal que vi(t) < vi(t ) E en caso contrario. Ahora desarrollamos una técnica analítica para llevar a cabo las fases de propagación de la función de valor y la función de probabilidad. 3 De manera similar para la fase de propagación de la probabilidad 832 The Sixth Intl. Supongamos que estamos realizando una fase de propagación de funciones de valor durante la cual las funciones de valor se propagan desde los métodos de destino a los métodos de origen. En cualquier momento durante esta fase nos encontramos con una situación mostrada en la Figura 2, donde se conocen las funciones de costo de oportunidad [Vjn]N n=0 de los métodos [mjn]N n=0, y se debe derivar el costo de oportunidad Vi0 del método mi0. Sea pi0 la función de distribución de probabilidad de la duración de la ejecución del método mi0, y ri0 la recompensa inmediata por comenzar y completar la ejecución del método mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 ∈ C[τ, τ]. La función Vi0 se deriva entonces de ri0 y los costos de oportunidad Vjn,i0 (t) n = 1, ..., N de los métodos futuros. Formalmente: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt si existe mi0 τ,τ ∈C[ ] tal que t ∈ [τ, τ ] 0 de lo contrario (1) Nota que para t ∈ [τ, τ ], si h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) entonces Vi0 es una convolución de p y h: vi0 (t) = (pi0 ∗h)(τ −t). Por ahora, asumamos que Vjn,i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad Vj0 en [Vj0,ik ]K k=0 hasta la sección 6. Ahora mostramos cómo derivar Vj0,i0 (la derivación de Vjn,i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente Ak. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (de derecha a izquierda). Sea V j0,i0 (t) el costo de oportunidad de comenzar la ejecución del método mj0 en el tiempo t dado que el método mi0 ha sido completado. Se obtiene multiplicando Vi0 por las funciones de probabilidad de todos los métodos que no sean mi0 y que permitan mj0. Formalmente: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t). Donde, de manera similar a [4] y [5], ignoramos la dependencia de [Plk ]K k=1. Observe que V j0,i0 no tiene que ser monótonamente decreciente, es decir, retrasar la ejecución del método mi0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad Vj0,i0 (t) de habilitar el método mi0 en el tiempo t debe ser mayor o igual a V j0,i0. Además, Vj0,i0 debería ser no decreciente. Formalmente: Vj0,i0 = min f∈F f (2) donde F = {f | f ≥ V j0,i0 y f(t) ≥ f(t ) ∀t<t }. Conociendo el costo de oportunidad Vi0, podemos derivar fácilmente la función de valor vi0. Que Ak sea un agente asignado al método mi0. Si Ak está a punto de comenzar la ejecución de mi0, significa que Ak debe haber completado su parte del plan de misión hasta el método mi0. Dado que Ak no sabe si otros agentes han completado los métodos [mlk]k=K k=1, para derivar vi0, tiene que multiplicar Vi0 por las funciones de probabilidad de todos los métodos de otros agentes que permiten mi0. Formalmente: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) donde también se ignora la dependencia de [Plk]K k=1. Hemos mostrado consecuentemente un esquema general sobre cómo propagar las funciones de valor: Conociendo [vjn]N n=0 y [Vjn]N n=0 de los métodos [mjn]N n=0, podemos derivar vi0 y Vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos sumidero. Luego visita en cada momento un método m, de modo que todos los métodos que m habilita ya han sido marcados como visitados. La fase de propagación de la función de valor termina cuando todos los métodos fuente han sido marcados como visitados. 5.2 Lectura de la Política Para determinar la política del agente Ak para el método mj0, debemos identificar el conjunto Zj0 de intervalos [z, z] ⊂ [0, ..., Δ], tal que: ∀t∈[z,z] πk( mj0 , t ) = W. Se pueden identificar fácilmente los intervalos de Zj0 observando los intervalos de tiempo en los que la función de valor vj0 no disminuye monótonamente. 5.3 Fase de Propagación de la Función de Probabilidad Supongamos ahora que las funciones de valor y los valores de costo de oportunidad han sido propagados desde los métodos sumidero hasta los nodos fuente y los conjuntos Zj para todos los métodos mj ∈ M han sido identificados. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi(t) para los métodos mi ∈ M y los tiempos t ∈ [0, Δ] encontrados en la iteración previa del algoritmo, ahora tenemos que encontrar nuevos valores Pi(t), para preparar el algoritmo para su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) se propagan las funciones de probabilidad hacia adelante a través de un método, es decir, asumimos que las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 son conocidas, y la función de probabilidad Pj0 del método mj0 debe ser derivada. Sea pj0 la función de distribución de probabilidad de la duración de la ejecución del método mj0, y Zj0 el conjunto de intervalos de inactividad para el método mj0, encontrados durante la última fase de propagación de la función de valor. Si ignoramos la dependencia de [Pik ]K k=0 entonces la probabilidad Pj0 (t) de que la ejecución del método mj0 comience antes del tiempo t está dada por: Pj0 (t) = (QK k=0 Pik (τ) si ∃(τ, τ ) ∈ Zj0 tal que t ∈ (τ, τ ) QK k=0 Pik (t) en caso contrario. Dada Pj0 (t), la probabilidad Pj0 (t) de que el método mj0 se complete para el tiempo t se deriva por: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Lo cual puede escribirse de forma compacta como ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t. Hemos demostrado consecuentemente cómo propagar las funciones de probabilidad [Pik]K k=0 de los métodos [mik]K k=0 para obtener la función de probabilidad Pj0 del método mj0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen msi para los cuales sabemos que Psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada momento un método m tal que todos los métodos que permiten The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) ya ha marcado como visitados 833 metros. La fase de propagación de la función de probabilidad termina cuando todos los métodos de destino han sido marcados como visitados. 5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de la política con la política de tiempo de inicio más temprano π0. Luego, en cada iteración: (i) Propaga las funciones de valor [vi] |M| i=1 utilizando las antiguas funciones de probabilidad [Pi] |M| i=1 de la iteración previa del algoritmo y establece los nuevos conjuntos [Zi] |M| i=1 de intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [Pi] |M| i=1 utilizando los conjuntos recién establecidos [Zi] |M| i=1. Estas nuevas funciones [Pi ] |M| i=1 luego son utilizadas en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP se detiene si una nueva política no mejora la política de la iteración del algoritmo anterior. 5.5 Implementación de Operaciones de Funciones. Hasta ahora, hemos derivado las operaciones funcionales para la propagación de la función de valor y la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y se tiene la libertad de elegir una técnica de aproximación de función deseada, como la aproximación lineal por tramos [7] o la aproximación constante por tramos [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo existente OC-DEC-MDP, que solo funciona para tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y de probabilidad con funciones lineales por tramos (PWL). Cuando el algoritmo VFP propaga las funciones de valor y funciones de probabilidad, lleva a cabo constantemente operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones p(t) y h(t). Si el tiempo está discretizado, las funciones p(t) y h(t) son discretas; sin embargo, h(t) puede aproximarse de manera precisa con una función PWL bh(t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar O(Δ2) multiplicaciones para calcular f(t), VFP solo necesita realizar O(k · Δ) multiplicaciones para calcular f(t), donde k es el número de segmentos lineales de bh(t) (nota que dado que h(t) es monótona, bh(t) suele estar cerca de h(t) con k Δ). Dado que los valores de Pi están en el rango [0, 1] y los valores de Vi están en el rango [0, P mi∈M ri], sugerimos aproximar Vi(t) con bVi(t) con un error V, y Pi(t) con bPi(t) con un error P. Ahora demostramos que el error de aproximación acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: TEOREMA 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con Restricciones Temporales, y P y V sean los errores de aproximación de la función de probabilidad y la función de valor respectivamente. El error general π = maxV supt∈[0,Δ]|V (t) − bV (t)| de la fase de propagación de la función de valor está entonces acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri. PRUEBA. Para establecer el límite para π, primero demostramos por inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π(P) = maxP supt∈[0,Δ]|P(t) − bP(t)| está limitado por (1 + P)|C≺| - 1. Base de inducción: Si n = 1, solo hay dos métodos presentes, y realizaremos la operación identificada por la Ecuación (3) solo una vez, introduciendo el error π(P) = P = (1 + P)|C≺| − 1. Paso de inducción: Supongamos que π(P) para |C≺| = n está acotado por (1 + P)n - 1, y queremos demostrar que esta afirmación se cumple para |C≺| = n. Sea G = M, C≺ un grafo con a lo sumo n + 1 aristas, y G = M, C≺ un subgrafo de G, tal que C≺ = C≺ - {mi, mj}, donde mj ∈ M es un nodo sumidero en G. A partir de la suposición de inducción, tenemos que C≺ introduce el error de fase de propagación de probabilidad acotado por (1 + P)n - 1. Ahora agregamos de nuevo el enlace {mi, mj} a C≺, lo cual afecta el error de solo una función de probabilidad, es decir, Pj, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ estaba limitado por (1 + P )n − 1, en C≺ = C≺ ∪ { mi, mj } puede ser a lo sumo ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1. Por lo tanto, si las funciones de costo de oportunidad no están sobreestimadas, están limitadas por P mi∈M ri y el error de una operación de propagación de función de valor único será como máximo Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es |C≺|, el error total π de la fase de propagación de la función de valor está acotado por: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6. DIVIDIENDO LAS FUNCIONES DE COSTO DE OPORTUNIDAD En la sección 5 omitimos la discusión sobre cómo se divide la función de costo de oportunidad Vj0 del método mj0 en funciones de costo de oportunidad [Vj0,ik ]K k=0 enviadas de regreso a los métodos [mik ]K k=0 , que habilitan directamente al método mj0. Hasta ahora, hemos seguido el mismo enfoque que en [4] y [5] en el sentido de que la función de costo de oportunidad Vj0,ik que el método mik envía de vuelta al método mj0 es una función mínima y no decreciente que domina la función V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). Nos referimos a este enfoque como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que podrían ocurrir al dividir las funciones de costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) escasez. Considera la situación en la Figura 3: Dividiendo la función de valor del método mj0 entre los métodos [mik]K k=0, cuando se realiza la propagación de la función de valor para los métodos [mik]K k=0. Para cada k = 0, ..., K, la Ecuación (1) deriva la función de costo de oportunidad Vik a partir de la recompensa inmediata rk y la función de costo de oportunidad Vj0,ik. Si m0 es el único método que precede al método mk, entonces V ik,0 = Vik se propaga al método m0, y en consecuencia, el costo de oportunidad de completar el método m0 en el tiempo t es igual a PK k=0 Vik,0(t). Si este costo está sobreestimado, entonces un agente A0 en el método m0 tendrá demasiado incentivo para finalizar la ejecución de m0 en el tiempo t. En consecuencia, aunque la probabilidad P(t) de que m0 sea habilitado por otros agentes para el tiempo t sea baja, el agente A0 aún podría encontrar que la utilidad esperada de comenzar la ejecución de m0 en el tiempo t es mayor que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método m0 en lugar de esperar, lo cual puede tener consecuencias desastrosas. De manera similar, si PK k=0 Vik,0(t) está subestimado, el agente A0 podría perder interés en habilitar los métodos futuros [mik]K k=0 y simplemente enfocarse en 834 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) maximizando la probabilidad de obtener su recompensa inmediata r0. Dado que esta posibilidad aumenta cuando el agente A0 espera, considerará en el momento t que es más rentable esperar en lugar de comenzar la ejecución de m0, lo cual puede tener consecuencias igualmente desastrosas. Finalmente, si Vj0 se divide de tal manera que, para algún k, Vj0,ik = 0, es el método mik el que subestima el costo de oportunidad de habilitar el método mj0, y el razonamiento similar se aplica. Llamamos a este problema una falta de método mk. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad Vj0 de tal manera que se evite la sobreestimación, la subestimación y el problema de escasez. Ahora demostramos que: TEOREMA 2. La heurística H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Demostramos el teorema mostrando un caso donde ocurre la sobreestimación. Para el plan de misión de la Figura (3), permita que H 1,1 divida Vj0 en [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 enviados a los métodos [mik ]K k=0 respectivamente. Además, suponga que los métodos [mik]K k=0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0; ESTik = 0, LETik = Δ para k = 0, ..., K. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] tal que el costo de oportunidad PK k=0 Vik (t) para los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] sea mayor que el costo de oportunidad Vj0 (t). A partir de la Ecuación (1) tenemos: Vik (t) = Z Δ−t 0 pik (t) Vj0,ik (t + t) dt Sumando sobre todos los métodos [mik]K k=0 obtenemos: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt (4) ≥ KX k=0 Z Δ−t 0 pik (t) Vj0,ik (t + t) dt = KX k=0 Z Δ−t 0 pik (t) Vj0 (t + t) Y k ∈{0,...,K} k =k Pik (t + t) dt Sea c ∈ (0, 1] una constante y t0 ∈ [0, Δ] tal que ∀t>t0 y ∀k=0,..,K tenemos Q k ∈{0,...,K} k =k Pik (t) > c. Entonces: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t) Vj0 (t0 + t) · c dt Porque Pjk es no decreciente. Ahora, supongamos que existe t1 ∈ (t0, Δ], tal que PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) . Dado que al disminuir el límite superior de la integral sobre una función positiva también disminuye la integral, tenemos: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt Y dado que Vj0 (t ) es no creciente, tenemos: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Suponiendo LET0 t En consecuencia, el costo de oportunidad PK k=0 Vik (t0) de comenzar la ejecución de los métodos [mik]K k=0 en el tiempo t ∈ [0, .., Δ] es mayor que el costo de oportunidad Vj0 (t0) lo cual demuestra el teorema. La Figura 4 muestra que la sobreestimación del costo de oportunidad es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación del costo de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones de costo de oportunidad: • Heurística H 1,0 : Solo un método, mik, recibe la recompensa esperada completa por habilitar el método mj0, es decir, V j0,ik (t) = 0 para k ∈ {0, ..., K}\\{k} y V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heurística H 1/2,1/2 : Cada método [mik]K k=0 recibe el costo de oportunidad completo por habilitar el método mj0 dividido por el número K de métodos que habilitan el método mj0, es decir, V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) para k ∈ {0, ..., K}. • Heurística bH 1,1 : Esta es una versión normalizada de la heurística H 1,1 en la que cada método [mik]K k=0 inicialmente recibe el costo de oportunidad completo por habilitar el método mj0. Para evitar la sobreestimación del costo de oportunidad, normalizamos las funciones de división cuando su suma excede la función de costo de oportunidad a dividir. Formalmente: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) si PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) en otro caso Donde V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t). Para las nuevas heurísticas, ahora demostramos que: TEOREMA 3. Las heurísticas H 1,0, H 1/2,1/2 y bH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando se utiliza la heurística H 1,0 para dividir la función de costo de oportunidad Vj0, solo un método (por ejemplo, mik) obtiene el costo de oportunidad para habilitar el método mj0. Por lo tanto: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) Y dado que Vj0 es no decreciente ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) La última desigualdad también es consecuencia del hecho de que Vj0 es no decreciente. Para la heurística H 1/2,1/2, de manera similar tenemos: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t). Para la heurística bH 1,1, la función de costo de oportunidad Vj0 está definida de tal manera que se divide de forma que PK k=0 Vik (t) ≤ Vj0 (t). Por consiguiente, hemos demostrado que nuestras nuevas heurísticas H 1,0, H 1/2,1/2 y bH 1,1 evitan la sobreestimación del costo de oportunidad. El Sexto Internacional. La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: Dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método mik recibirá la recompensa por habilitar el método mj0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja K − 1 métodos que preceden al método mj0 sin ninguna recompensa, lo que lleva a la inanición. La inanición se puede evitar si las funciones de costo de oportunidad se dividen utilizando la heurística H 1/2,1/2, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad divididas para la heurística H 1/2,1/2 puede ser menor que la función de costo de oportunidad dividida no nula para la heurística H 1,0, lo cual es claramente indeseable. La situación mencionada (Figura 4, heurística H 1,0 ) ocurre porque la media f+g 2 de dos funciones f, g no es menor que f ni que g, a menos que f = g. Por esta razón, hemos propuesto la heurística bH 1,1, la cual, por definición, evita los problemas de sobreestimación, subestimación y falta de recursos. 7. EVALUACIÓN EXPERIMENTAL Dado que el algoritmo VFP que introdujimos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-MDP, la evaluación experimental que realizamos consistió en dos partes: En la parte 1, probamos empíricamente la calidad de las soluciones que un solucionador localmente óptimo (ya sea OC-DEC-MDP o VFP) encuentra, dado que utiliza diferentes heurísticas de división de la función de costo de oportunidad, y en la parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-DEC-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración genérica del plan de misión de la Figura 3 donde solo estaban presentes los métodos mj0, mi1, mi2 y m0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración pj0 del método mj0 fue uniforme, es decir, pj0 (t) = 1 400 y las duraciones pi1, pi2 de los métodos mi1, mi2 fueron distribuciones normales, es decir, pi1 = N(μ = 250, σ = 20) y pi2 = N(μ = 200, σ = 100). Supusimos que solo el método mj0 proporcionaba recompensa, es decir, rj0 = 10 era la recompensa por finalizar la ejecución del método mj0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje x de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que, cuando la función de costo de oportunidad Vj0 se dividió en las funciones de costo de oportunidad Vi1 y Vi2 utilizando la heurística H 1,1, la función Vi1 + Vi2 no siempre estaba por debajo de la función Vj0. En particular, Vi1 (280) + Vi2 (280) superó a Vj0 (280) en un 69%. Cuando se utilizaron las heurísticas H 1,0 , H 1/2,1/2 y bH 1,1 (gráficos 2, 3 y 4), la función Vi1 + Vi2 siempre estuvo por debajo de Vj0. Luego dirigimos nuestra atención al ámbito del rescate civil presentado en la Figura 1, para el cual muestreamos todas las duraciones de ejecución de las acciones de la distribución normal N = (μ = 5, σ = 2). Para obtener la línea base del rendimiento heurístico, implementamos un solucionador globalmente óptimo que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. La figura (6a), que representa en el eje y la recompensa total esperada de una política, complementa nuestros resultados anteriores: la heurística H 1,1 sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar al solucionador localmente óptimo cerca de una recompensa total esperada real. Parte 2: Luego elegimos H 1,1 para dividir las funciones de costo de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones de planes de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como referencia. Iniciamos las pruebas de escalabilidad de VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para la cual las duraciones de ejecución del método se extendieron a distribuciones normales N(μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil, (b) cadena de n métodos, (c) árbol de n métodos con factor de ramificación = 3 y (d) malla cuadrada de n métodos. Figura 6: Rendimiento de VFP en el ámbito del rescate civil. 30, σ = 5), y el plazo límite se extendió a Δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP ejecutándose con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrada por VFP se mantuviera dentro del 1%, 5% y 10% de la solución encontrada por el algoritmo OC-DEC-MDP. Luego ejecutamos ambos algoritmos durante un total de 100 iteraciones de mejora de políticas. La figura (6b) muestra el rendimiento del algoritmo VFP en el ámbito del rescate civil (el eje y muestra el tiempo de ejecución en milisegundos). Como podemos ver, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. Para comparación, la solución óptima a nivel global no se terminó en las primeras tres horas de su ejecución, lo que muestra la fortaleza de los solucionadores oportunistas, como OC-DEC-MDP. A continuación, decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5b)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo las ventanas de tiempo del método a 350, 700 y 1050 para asegurar que los métodos posteriores puedan ser alcanzados. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y representamos en el eje y el tiempo de ejecución del algoritmo (notar la escala logarítmica). Al observar, al ampliar el dominio se revela el alto rendimiento de VFP: Dentro del 1% de error, corre hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo VFP se escala, dado que los métodos están organizados en un árbol (Figura (5c)). En particular, consideramos árboles con un factor de ramificación de 3 y una profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300 y luego a 400. Mostramos los resultados en la Figura (7b). Aunque las mejoras en la velocidad son menores que en el caso de una cadena, el algoritmo VFP sigue siendo hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error inferior al 1%. Finalmente probamos cómo VFP maneja los dominios con métodos organizados en una malla n × n, es decir, C≺ = { mi,j, mk,j+1 } para i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1. En particular, consideramos 836 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red. mallas de 3×3, 4×4 y 5×5 métodos. Para tales configuraciones, debemos aumentar significativamente el horizonte temporal, ya que las probabilidades de habilitar los métodos finales para un momento específico disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se ejecuta hasta un orden de magnitud más rápido que OC-DEC-MDP mientras encuentra una política que está dentro de menos del 1% de la política encontrada por OC-DEC-MDP. CONCLUSIONES El Proceso de Decisión de Markov Descentralizado (DEC-MDP) ha sido muy popular para modelar problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de vanguardia para DEC-MDPs, llamado OC-DEC-MDP, que recientemente se ha demostrado que es escalable para DEC-MDPs grandes. Nuestro método de solución heurístico, llamado Propagación de Función de Valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) Aceleró OC-DEC-MDP en un orden de magnitud al mantener y manipular una función de valor para cada método en lugar de un valor separado para cada par de método e intervalo de tiempo, y (ii) logró una mejor calidad de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En cuanto al trabajo relacionado, hemos discutido extensamente el algoritmo OCDEC-MDP [4]. Además, como se discute en la Sección 4, existen algoritmos óptimos a nivel global para resolver DEC-MDPs con <br>restricciones temporales</br> [1] [11]. Desafortunadamente, no logran escalar a dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, existen otros algoritmos localmente óptimos para DEC-MDPs y DECPOMDPs [8] [12], [13], sin embargo, tradicionalmente no han abordado los tiempos de ejecución inciertos y las <br>restricciones temporales</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "locally optimal solution": {
            "translated_key": "solución óptima local",
            "is_in_text": false,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "decentralize markov decision process": {
            "translated_key": "Descentralizar el proceso de decisión de Markov.",
            "is_in_text": false,
            "original_annotated_sentences": [
                "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints Janusz Marecki and Milind Tambe Computer Science Department University of Southern California 941 W 37th Place, Los Angeles, CA 90089 {marecki, tambe}@usc.edu ABSTRACT Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve.",
                "In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP.",
                "First, it speeds up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval.",
                "Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP.",
                "We test both improvements independently in a crisis-management domain as well as for other types of domains.",
                "Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial IntelligenceMulti-agent Systems General Terms Algorithms, Theory 1.",
                "INTRODUCTION The development of algorithms for effective coordination of multiple agents acting as a team in uncertain and time critical domains has recently become a very active research field with potential applications ranging from coordination of agents during a hostage rescue mission [11] to the coordination of Autonomous Mars Exploration Rovers [2].",
                "Because of the uncertain and dynamic characteristics of such domains, decision-theoretic models have received a lot of attention in recent years, mainly thanks to their expressiveness and the ability to reason about the utility of actions over time.",
                "Key decision-theoretic models that have become popular in the literature include Decentralized Markov Decision Processes (DECMDPs) and Decentralized, Partially Observable Markov Decision Processes (DEC-POMDPs).",
                "Unfortunately, solving these models optimally has been proven to be NEXP-complete [3], hence more tractable subclasses of these models have been the subject of intensive research.",
                "In particular, Network Distributed POMDP [13] which assume that not all the agents interact with each other, Transition Independent DEC-MDP [2] which assume that transition function is decomposable into local transition functions or DEC-MDP with Event Driven Interactions [1] which assume that interactions between agents happen at fixed time points constitute good examples of such subclasses.",
                "Although globally optimal algorithms for these subclasses have demonstrated promising results, domains on which these algorithms run are still small and time horizons are limited to only a few time ticks.",
                "To remedy that, locally optimal algorithms have been proposed [12] [4] [5].",
                "In particular, Opportunity Cost DEC-MDP [4] [5], referred to as OC-DEC-MDP, is particularly notable, as it has been shown to scale up to domains with hundreds of tasks and double digit time horizons.",
                "Additionally, OC-DEC-MDP is unique in its ability to address both temporal constraints and uncertain method execution durations, which is an important factor for real-world domains.",
                "OC-DEC-MDP is able to scale up to such domains mainly because instead of searching for the globally optimal solution, it carries out a series of policy iterations; in each iteration it performs a value iteration that reuses the data computed during the previous policy iteration.",
                "However, OC-DEC-MDP is still slow, especially as the time horizon and the number of methods approach large values.",
                "The reason for high runtimes of OC-DEC-MDP for such domains is a consequence of its huge state space, i.e., OC-DEC-MDP introduces a separate state for each possible pair of method and method execution interval.",
                "Furthermore, OC-DEC-MDP overestimates the reward that a method expects to receive for enabling the execution of future methods.",
                "This reward, also referred to as the opportunity cost, plays a crucial role in agent decision making, and as we show later, its overestimation leads to highly suboptimal policies.",
                "In this context, we present VFP (= Value Function P ropagation), an efficient solution technique for the DEC-MDP model with temporal constraints and uncertain method execution durations, that builds on the success of OC-DEC-MDP.",
                "VFP introduces our two orthogonal ideas: First, similarly to [7] [9] and [10], we maintain 830 978-81-904262-7-5 (RPS) c 2007 IFAAMAS and manipulate a value function over time for each method rather than a separate value for each pair of method and time interval.",
                "Such representation allows us to group the time points for which the value function changes at the same rate (= its slope is constant), which results in fast, functional propagation of value functions.",
                "Second, we prove (both theoretically and empirically) that OC-DEC- MDP overestimates the opportunity cost, and to remedy that, we introduce a set of heuristics, that correct the opportunity cost overestimation problem.",
                "This paper is organized as follows: In section 2 we motivate this research by introducing a civilian rescue domain where a team of fire- brigades must coordinate in order to rescue civilians trapped in a burning building.",
                "In section 3 we provide a detailed description of our DEC-MDP model with Temporal Constraints and in section 4 we discuss how one could solve the problems encoded in our model using globally optimal and locally optimal solvers.",
                "Sections 5 and 6 discuss the two orthogonal improvements to the state-of-the-art OC-DEC-MDP algorithm that our VFP algorithm implements.",
                "Finally, in section 7 we demonstrate empirically the impact of our two orthogonal improvements, i.e., we show that: (i) The new heuristics correct the opportunity cost overestimation problem leading to higher quality policies, and (ii) By allowing for a systematic tradeoff of solution quality for time, the VFP algorithm runs much faster than the OC-DEC-MDP algorithm 2.",
                "MOTIVATING EXAMPLE We are interested in domains where multiple agents must coordinate their plans over time, despite uncertainty in plan execution duration and outcome.",
                "One example domain is large-scale disaster, like a fire in a skyscraper.",
                "Because there can be hundreds of civilians scattered across numerous floors, multiple rescue teams have to be dispatched, and radio communication channels can quickly get saturated and useless.",
                "In particular, small teams of fire-brigades must be sent on separate missions to rescue the civilians trapped in dozens of different locations.",
                "Picture a small mission plan from Figure (1), where three firebrigades have been assigned a task to rescue the civilians trapped at site B, accessed from site A (e.g. an office accessed from the floor)1 .",
                "General fire fighting procedures involve both: (i) putting out the flames, and (ii) ventilating the site to let the toxic, high temperature gases escape, with the restriction that ventilation should not be performed too fast in order to prevent the fire from spreading.",
                "The team estimates that the civilians have 20 minutes before the fire at site B becomes unbearable, and that the fire at site A has to be put out in order to open the access to site B.",
                "As has happened in the past in large scale disasters, communication often breaks down; and hence we assume in this domain that there is no communication between the fire-brigades 1,2 and 3 (denoted as FB1, FB2 and FB3).",
                "Consequently, FB2 does not know if it is already safe to ventilate site A, FB1 does not know if it is already safe to enter site A and start fighting fire at site B, etc.",
                "We assign the reward 50 for evacuating the civilians from site B, and a smaller reward 20 for the successful ventilation of site A, since the civilians themselves might succeed in breaking out from site B.",
                "One can clearly see the dilemma, that FB2 faces: It can only estimate the durations of the Fight fire at site A methods to be executed by FB1 and FB3, and at the same time FB2 knows that time is running out for civilians.",
                "If FB2 ventilates site A too early, the fire will spread out of control, whereas if FB2 waits with the ventilation method for too long, fire at site B will become unbearable for the civilians.",
                "In general, agents have to perform a sequence of such 1 We explain the EST and LET notation in section 3 Figure 1: Civilian rescue domain and a mission plan.",
                "Dotted arrows represent implicit precedence constraints within an agent. difficult decisions; in particular, decision process of FB2 involves first choosing when to start ventilating site A, and then (depending on the time it took to ventilate site A), choosing when to start evacuating the civilians from site B.",
                "Such sequence of decisions constitutes the policy of an agent, and it must be found fast because time is running out. 3.",
                "MODEL DESCRIPTION We encode our decision problems in a model which we refer to as Decentralized MDP with Temporal Constraints 2 .",
                "Each instance of our decision problems can be described as a tuple M, A, C, P, R where M = {mi} |M| i=1 is the set of methods, and A = {Ak} |A| k=1 is the set of agents.",
                "Agents cannot communicate during mission execution.",
                "Each agent Ak is assigned to a set Mk of methods, such that S|A| k=1 Mk = M and ∀i,j;i=jMi ∩ Mj = ø.",
                "Also, each method of agent Ak can be executed only once, and agent Ak can execute only one method at a time.",
                "Method execution times are uncertain and P = {pi} |M| i=1 is the set of distributions of method execution durations.",
                "In particular, pi(t) is the probability that the execution of method mi consumes time t. C is a set of temporal constraints in the system.",
                "Methods are partially ordered and each method has fixed time windows inside which it can be executed, i.e., C = C≺ ∪ C[ ] where C≺ is the set of predecessor constraints and C[ ] is the set of time window constraints.",
                "For c ∈ C≺, c = mi, mj means that method mi precedes method mj i.e., execution of mj cannot start before mi terminates.",
                "In particular, for an agent Ak, all its methods form a chain linked by predecessor constraints.",
                "We assume, that the graph G = M, C≺ is acyclic, does not have disconnected nodes (the problem cannot be decomposed into independent subproblems), and its source and sink vertices identify the source and sink methods of the system.",
                "For c ∈ C[ ], c = mi, EST, LET means that execution of mi can only start after the Earliest Starting Time EST and must finish before the Latest End Time LET; we allow methods to have multiple disjoint time window constraints.",
                "Although distributions pi can extend to infinite time horizons, given the time window constraints, the planning horizon Δ = max m,τ,τ ∈C[ ] τ is considered as the mission deadline.",
                "Finally, R = {ri} |M| i=1 is the set of non-negative rewards, i.e., ri is obtained upon successful execution of mi.",
                "Since there is no communication allowed, an agent can only estimate the probabilities that its methods have already been enabled 2 One could also use the OC-DEC-MDP framework, which models both time and resource constraints The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 831 by other agents.",
                "Consequently, if mj ∈ Mk is the next method to be executed by the agent Ak and the current time is t ∈ [0, Δ], the agent has to make a decision whether to Execute the method mj (denoted as E), or to Wait (denoted as W).",
                "In case agent Ak decides to wait, it remains idle for an arbitrary small time , and resumes operation at the same place (= about to execute method mj) at time t + .",
                "In case agent Ak decides to Execute the next method, two outcomes are possible: Success: The agent Ak receives reward rj and moves on to its next method (if such method exists) so long as the following conditions hold: (i) All the methods {mi| mi, mj ∈ C≺} that directly enable method mj have already been completed, (ii) Execution of method mj started in some time window of method mj, i.e., ∃ mj ,τ,τ ∈C[ ] such that t ∈ [τ, τ ], and (iii) Execution of method mj finished inside the same time window, i.e., agent Ak completed method mj in time less than or equal to τ − t. Failure: If any of the above-mentioned conditions does not hold, agent Ak stops its execution.",
                "Other agents may continue their execution, but methods mk ∈ {m| mj, m ∈ C≺} will never become enabled.",
                "The policy πk of an agent Ak is a function πk : Mk × [0, Δ] → {W, E}, and πk( m, t ) = a means, that if Ak is at method m at time t, it will choose to perform the action a.",
                "A joint policy π = [πk] |A| k=1 is considered to be optimal (denoted as π∗ ), if it maximizes the sum of expected rewards for all the agents. 4.",
                "SOLUTION TECHNIQUES 4.1 Optimal Algorithms Optimal joint policy π∗ is usually found by using the Bellman update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ C≺} are used.",
                "Unfortunately, for our model, the optimal policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ C≺}.",
                "This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .",
                "Thus, globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].",
                "The complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be enabled at time points t ∈ Tj ⊂ [0, Δ], the Coverage Set Algorithm (CSA) [1] could be used.",
                "However, CSA complexity is double exponential in the size of Ti, and for our domains Tj can store all values ranging from 0 to Δ. 4.2 Locally Optimal Algorithms Following the limited applicability of globally optimal algorithms for DEC-MDPs with Temporal Constraints, locally optimal algorithms appear more promising.",
                "Specially, the OC-DEC-MDP algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.",
                "The idea of the OC-DECMDP algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is possible.",
                "At each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities Pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].",
                "It then performs two steps: Step 1: It propagates from sink methods to source methods the values Vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].",
                "This propagation uses the probabilities Pi,[τ,τ ] from previous algorithm iteration.",
                "We call this step a value propagation phase.",
                "Step 2: Given the values Vi,[τ,τ ] from Step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .",
                "It then propagates the new probabilities Pi,[τ,τ ] from source methods to sink methods.",
                "We call this step a probability propagation phase.",
                "If policy π does not improve π, the algorithm terminates.",
                "There are two shortcomings of the OC-DEC-MDP algorithm that we address in this paper.",
                "First, each of OC-DEC-MDP states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.",
                "While such state representation is beneficial, in that the problem can be solved with a standard value iteration algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently, if some method mi enables method mj, and the values Vj,[τ,τ ]∀τ,τ ∈[0,Δ] are known, the operation that calculates the values Vi,[τ,τ ]∀τ, τ ∈ [0, Δ] (during the value propagation phase), runs in time O(I2 ), where I is the number of time intervals 3 .",
                "Since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons Δ, the OC- DECMDP algorithm runs slow.",
                "Second, while OC-DEC-MDP emphasizes on precise calculation of values Vj,[τ,τ ], it fails to address a critical issue that determines how the values Vj,[τ,τ ] are split given that the method mj has multiple enabling methods.",
                "As we show later, OC-DEC-MDP splits Vj,[τ,τ ] into parts that may overestimate Vj,[τ,τ ] when summed up again.",
                "As a result, methods that precede the method mj overestimate the value for enabling mj which, as we show later, can have disastrous consequences.",
                "In the next two sections, we address both of these shortcomings. 5.",
                "VALUE FUNCTION PROPAGATION (VFP) The general scheme of the VFP algorithm is identical to the OCDEC-MDP algorithm, in that it performs a series of policy improvement iterations, each one involving a Value and Probability Propagation Phase.",
                "However, instead of propagating separate values, VFP maintains and propagates the whole functions, we therefore refer to these phases as the value function propagation phase and the probability function propagation phase.",
                "To this end, for each method mi ∈ M, we define three new functions: Value Function, denoted as vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t. Opportunity Cost Function, denoted as Vi(t), that maps time t ∈ [0, Δ] to the expected total reward for starting the execution of method mi at time t assuming that mi is enabled.",
                "Probability Function, denoted as Pi(t), that maps time t ∈ [0, Δ] to the probability that method mi will be completed before time t. Such functional representation allows us to easily read the current policy, i.e., if an agent Ak is at method mi at time t, then it will wait as long as value function vi(t) will be greater in the future.",
                "Formally: πk( mi, t ) = j W if ∃t >t such that vi(t) < vi(t ) E otherwise.",
                "We now develop an analytical technique for performing the value function and probability function propagation phases. 3 Similarly for the probability propagation phase 832 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.1 Value Function Propagation Phase Suppose, that we are performing a value function propagation phase during which the value functions are propagated from the sink methods to the source methods.",
                "At any time during this phase we encounter a situation shown in Figure 2, where opportunity cost functions [Vjn ]N n=0 of methods [mjn ]N n=0 are known, and the opportunity cost Vi0 of method mi0 is to be derived.",
                "Let pi0 be the probability distribution function of method mi0 execution duration, and ri0 be the immediate reward for starting and completing the execution of method mi0 inside a time interval [τ, τ ] such that mi0 τ, τ ∈ C[ ].",
                "The function Vi0 is then derived from ri0 and opportunity costs Vjn,i0 (t) n = 1, ..., N from future methods.",
                "Formally: Vi0 (t) = 8 >>< >>: R τ −t 0 pi0 (t )(ri0 + PN n=0 Vjn,i0 (t + t ))dt if ∃ mi0 τ,τ ∈C[ ] such that t ∈ [τ, τ ] 0 otherwise (1) Note, that for t ∈ [τ, τ ], if h(t) := ri0 + PN n=0 Vjn,i0 (τ −t) then Vi0 is a convolution of p and h: vi0 (t) = (pi0 ∗h)(τ −t).",
                "Assume for now, that Vjn,i0 represents a full opportunity cost, postponing the discussion on different techniques for splitting the opportunity cost Vj0 into [Vj0,ik ]K k=0 until section 6.",
                "We now show how to derive Vj0,i0 (derivation of Vjn,i0 for n = 0 follows the same scheme).",
                "Figure 2: Fragment of an MDP of agent Ak.",
                "Probability functions propagate forward (left to right) whereas value functions propagate backward (right to left).",
                "Let V j0,i0 (t) be the opportunity cost of starting the execution of method mj0 at time t given that method mi0 has been completed.",
                "It is derived by multiplying Vi0 by the probability functions of all methods other than mi0 that enable mj0 .",
                "Formally: V j0,i0 (t) = Vj0 (t) · KY k=1 Pik (t).",
                "Where similarly to [4] and [5] we ignored the dependency of [Plk ]K k=1.",
                "Observe that V j0,i0 does not have to be monotonically decreasing, i.e., delaying the execution of the method mi0 can sometimes be profitable.",
                "Therefore the opportunity cost Vj0,i0 (t) of enabling method mi0 at time t must be greater than or equal to V j0,i0 .",
                "Furthermore, Vj0,i0 should be non-increasing.",
                "Formally: Vj0,i0 = min f∈F f (2) Where F = {f | f ≥ V j0,i0 and f(t) ≥ f(t ) ∀t<t }.",
                "Knowing the opportunity cost Vi0 , we can then easily derive the value function vi0 .",
                "Let Ak be an agent assigned to the method mi0 .",
                "If Ak is about to start the execution of mi0 it means, that Ak must have completed its part of the mission plan up to the method mi0 .",
                "Since Ak does not know if other agents have completed methods [mlk ]k=K k=1 , in order to derive vi0 , it has to multiply Vi0 by the probability functions of all methods of other agents that enable mi0 .",
                "Formally: vi0 (t) = Vi0 (t) · KY k=1 Plk (t) Where the dependency of [Plk ]K k=1 is also ignored.",
                "We have consequently shown a general scheme how to propagate the value functions: Knowing [vjn ]N n=0 and [Vjn ]N n=0 of methods [mjn ]N n=0 we can derive vi0 and Vi0 of method mi0 .",
                "In general, the value function propagation scheme starts with sink nodes.",
                "It then visits at each time a method m, such that all the methods that m enables have already been marked as visited.",
                "The value function propagation phase terminates when all the source methods have been marked as visited. 5.2 Reading the Policy In order to determine the policy of agent Ak for the method mj0 we must identify the set Zj0 of intervals [z, z ] ⊂ [0, ..., Δ], such that: ∀t∈[z,z ] πk( mj0 , t ) = W. One can easily identify the intervals of Zj0 by looking at the time intervals in which the value function vj0 does not decrease monotonically. 5.3 Probability Function Propagation Phase Assume now, that value functions and opportunity cost values have all been propagated from sink methods to source nodes and the sets Zj for all methods mj ∈ M have been identified.",
                "Since value function propagation phase was using probabilities Pi(t) for methods mi ∈ M and times t ∈ [0, Δ] found at previous algorithm iteration, we now have to find new values Pi(t), in order to prepare the algorithm for its next iteration.",
                "We now show how in the general case (Figure 2) propagate the probability functions forward through one method, i.e., we assume that the probability functions [Pik ]K k=0 of methods [mik ]K k=0 are known, and the probability function Pj0 of method mj0 must be derived.",
                "Let pj0 be the probability distribution function of method mj0 execution duration, and Zj0 be the set of intervals of inactivity for method mj0 , found during the last value function propagation phase.",
                "If we ignore the dependency of [Pik ]K k=0 then the probability Pj0 (t) that the execution of method mj0 starts before time t is given by: Pj0 (t) = (QK k=0 Pik (τ) if ∃(τ, τ ) ∈ Zj0 s.t. t ∈ (τ, τ ) QK k=0 Pik (t) otherwise.",
                "Given Pj0 (t), the probability Pj0 (t) that method mj0 will be completed by time t is derived by: Pj0 (t) = Z t 0 Z t 0 ( ∂Pj0 ∂t )(t ) · pj0 (t − t )dt dt (3) Which can be written compactly as ∂Pj0 ∂t = pj0 ∗ ∂P j0 ∂t .",
                "We have consequently shown how to propagate the probability functions [Pik ]K k=0 of methods [mik ]K k=0 to obtain the probability function Pj0 of method mj0 .",
                "The general, the probability function propagation phase starts with source methods msi for which we know that Psi = 1 since they are enabled by default.",
                "We then visit at each time a method m such that all the methods that enable The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 833 m have already been marked as visited.",
                "The probability function propagation phase terminates when all the sink methods have been marked as visited. 5.4 The Algorithm Similarly to the OC-DEC-MDP algorithm, VFP starts the policy improvement iterations with the earliest starting time policy π0 .",
                "Then at each iteration it: (i) Propagates the value functions [vi] |M| i=1 using the old probability functions [Pi] |M| i=1 from previous algorithm iteration and establishes the new sets [Zi] |M| i=1 of method inactivity intervals, and (ii) propagates the new probability functions [Pi ] |M| i=1 using the newly established sets [Zi] |M| i=1.",
                "These new functions [Pi ] |M| i=1 are then used in the next iteration of the algorithm.",
                "Similarly to OC-DEC-MDP, VFP terminates if a new policy does not improve the policy from the previous algorithm iteration. 5.5 Implementation of Function Operations So far, we have derived the functional operations for value function and probability function propagation without choosing any function representation.",
                "In general, our functional operations can handle continuous time, and one has freedom to choose a desired function approximation technique, such as piecewise linear [7] or piecewise constant [9] approximation.",
                "However, since one of our goals is to compare VFP with the existing OC-DEC- MDP algorithm, that works only for discrete time, we also discretize time, and choose to approximate value functions and probability functions with piecewise linear (PWL) functions.",
                "When the VFP algorithm propagates the value functions and probability functions, it constantly carries out operations represented by equations (1) and (3) and we have already shown that these operations are convolutions of some functions p(t) and h(t).",
                "If time is discretized, functions p(t) and h(t) are discrete; however, h(t) can be nicely approximated with a PWL function bh(t), which is exactly what VFP does.",
                "As a result, instead of performing O(Δ2 ) multiplications to compute f(t), VFP only needs to perform O(k · Δ) multiplications to compute f(t), where k is the number of linear segments of bh(t) (note, that since h(t) is monotonic, bh(t) is usually close to h(t) with k Δ).",
                "Since Pi values are in range [0, 1] and Vi values are in range [0, P mi∈M ri], we suggest to approximate Vi(t) with bVi(t) within error V , and Pi(t) with bPi(t) within error P .",
                "We now prove that the overall approximation error accumulated during the value function propagation phase can be expressed in terms of P and V : THEOREM 1.",
                "Let C≺ be a set of precedence constraints of a DEC-MDP with Temporal Constraints, and P and V be the probability function and value function approximation errors respectively.",
                "The overall error π = maxV supt∈[0,Δ]|V (t) − bV (t)| of value function propagation phase is then bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri .",
                "PROOF.",
                "In order to establish the bound for π, we first prove by induction on the size of C≺, that the overall error of probability function propagation phase, π(P ) = maxP supt∈[0,Δ]|P(t) − bP(t)| is bounded by (1 + P )|C≺| − 1.",
                "Induction base: If n = 1 only two methods are present, and we will perform the operation identified by Equation (3) only once, introducing the error π(P ) = P = (1 + P )|C≺| − 1.",
                "Induction step: Suppose, that π(P ) for |C≺| = n is bounded by (1 + P )n − 1, and we want to prove that this statement holds for |C≺| = n. Let G = M, C≺ be a graph with at most n + 1 edges, and G = M, C≺ be a subgraph of G, such that C≺ = C≺ − { mi, mj }, where mj ∈ M is a sink node in G. From the induction assumption we have, that C≺ introduces the probability propagation phase error bounded by (1 + P )n − 1.",
                "We now add back the link { mi, mj } to C≺, which affects the error of only one probability function, namely Pj, by a factor of (1 + P ).",
                "Since probability propagation phase error in C≺ was bounded by (1 + P )n − 1, in C≺ = C≺ ∪ { mi, mj } it can be at most ((1 + P )n − 1)(1 + P ) < (1 + P )n+1 − 1.",
                "Thus, if opportunity cost functions are not overestimated, they are bounded by P mi∈M ri and the error of a single value function propagation operation will be at most Z Δ 0 p(t)( V +((1+ P ) |C≺| −1) X mi∈M ri) dt < V +((1+ P ) |C≺| −1) X mi∈M ri.",
                "Since the number of value function propagation operations is |C≺|, the total error π of the value function propagation phase is bounded by: |C≺| V + ((1 + P )|C≺| − 1) P mi∈M ri . 6.",
                "SPLITTING THE OPPORTUNITY COST FUNCTIONS In section 5 we left out the discussion about how the opportunity cost function Vj0 of method mj0 is split into opportunity cost functions [Vj0,ik ]K k=0 sent back to methods [mik ]K k=0 , that directly enable method mj0 .",
                "So far, we have taken the same approach as in [4] and [5] in that the opportunity cost function Vj0,ik that the method mik sends back to the method mj0 is a minimal, non-increasing function that dominates function V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t).",
                "We refer to this approach, as heuristic H 1,1 .",
                "Before we prove that this heuristic overestimates the opportunity cost, we discuss three problems that might occur when splitting the opportunity cost functions: (i) overestimation, (ii) underestimation and (iii) starvation.",
                "Consider the situation in Figure Figure 3: Splitting the value function of method mj0 among methods [mik ]K k=0. (3) when value function propagation for methods [mik ]K k=0 is performed.",
                "For each k = 0, ..., K, Equation (1) derives the opportunity cost function Vik from immediate reward rk and opportunity cost function Vj0,ik .",
                "If m0 is the only methods that precedes method mk, then V ik,0 = Vik is propagated to method m0, and consequently the opportunity cost for completing the method m0 at time t is equal to PK k=0 Vik,0(t).",
                "If this cost is overestimated, then an agent A0 at method m0 will have too much incentive to finish the execution of m0 at time t. Consequently, although the probability P(t) that m0 will be enabled by other agents by time t is low, agent A0 might still find the expected utility of starting the execution of m0 at time t higher than the expected utility of doing it later.",
                "As a result, it will choose at time t to start executing method m0 instead of waiting, which can have disastrous consequences.",
                "Similarly, if PK k=0 Vik,0(t) is underestimated, agent A0 might loose interest in enabling the future methods [mik ]K k=0 and just focus on 834 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) maximizing the chance of obtaining its immediate reward r0.",
                "Since this chance is increased when agent A0 waits4 , it will consider at time t to be more profitable to wait, instead of starting the execution of m0, which can have similarly disastrous consequences.",
                "Finally, if Vj0 is split in a way, that for some k, Vj0,ik = 0, it is the method mik that underestimates the opportunity cost of enabling method mj0 , and the similar reasoning applies.",
                "We call such problem a starvation of method mk.",
                "That short discussion shows the importance of splitting the opportunity cost function Vj0 in such a way, that overestimation, underestimation, and starvation problem is avoided.",
                "We now prove that: THEOREM 2.",
                "Heuristic H 1,1 can overestimate the opportunity cost.",
                "PROOF.",
                "We prove the theorem by showing a case where the overestimation occurs.",
                "For the mission plan from Figure (3), let H 1,1 split Vj0 into [V j0,ik = Vj0 · Q k ∈{0,...,K} k =k Pik ]K k=0 sent to methods [mik ]K k=0 respectively.",
                "Also, assume that methods [mik ]K k=0 provide no local reward and have the same time windows, i.e., rik = 0; ESTik = 0, LETik = Δ for k = 0, ..., K. To prove the overestimation of opportunity cost, we must identify t0 ∈ [0, ..., Δ] such that the opportunity cost PK k=0 Vik (t) for methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t).",
                "From Equation (1) we have: Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt Summing over all methods [mik ]K k=0 we obtain: KX k=0 Vik (t) = KX k=0 Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (4) ≥ KX k=0 Z Δ−t 0 pik (t )V j0,ik (t + t )dt = KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t ) Y k ∈{0,...,K} k =k Pik (t + t )dt Let c ∈ (0, 1] be a constant and t0 ∈ [0, Δ] be such that ∀t>t0 and ∀k=0,..,K we have Q k ∈{0,...,K} k =k Pik (t) > c. Then: KX k=0 Vik (t0) > KX k=0 Z Δ−t0 0 pik (t )Vj0 (t0 + t ) · c dt Because Pjk is non-decreasing.",
                "Now, suppose there exists t1 ∈ (t0, Δ], such that PK k=0 R t1−t0 0 pik (t )dt > Vj0 (t0) c·Vj0 (t1) .",
                "Since decreasing the upper limit of the integral over positive function also decreases the integral, we have: KX k=0 Vik (t0) > c KX k=0 Z t1 t0 pik (t − t0)Vj0 (t )dt And since Vj0 (t ) is non-increasing we have: KX k=0 Vik (t0) > c · Vj0 (t1) KX k=0 Z t1 t0 pik (t − t0)dt (5) = c · Vj0 (t1) KX k=0 Z t1−t0 0 pik (t )dt > c · Vj0 (t1) Vj(t0) c · Vj(t1) = Vj(t0) 4 Assuming LET0 t Consequently, the opportunity cost PK k=0 Vik (t0) of starting the execution of methods [mik ]K k=0 at time t ∈ [0, .., Δ] is greater than the opportunity cost Vj0 (t0) which proves the theorem.Figure 4 shows that the overestimation of opportunity cost is easily observable in practice.",
                "To remedy the problem of opportunity cost overestimation, we propose three alternative heuristics that split the opportunity cost functions: • Heuristic H 1,0 : Only one method, mik gets the full expected reward for enabling method mj0 , i.e., V j0,ik (t) = 0 for k ∈ {0, ..., K}\\{k} and V j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pik )(t). • Heuristic H 1/2,1/2 : Each method [mik ]K k=0 gets the full opportunity cost for enabling method mj0 divided by the number K of methods enabling the method mj0 , i.e., V j0,ik (t) = 1 K (Vj0 · Q k ∈{0,...,K} k =k Pik )(t) for k ∈ {0, ..., K}. • Heuristic bH 1,1 : This is a normalized version of the H 1,1 heuristic in that each method [mik ]K k=0 initially gets the full opportunity cost for enabling the method mj0 .",
                "To avoid opportunity cost overestimation, we normalize the split functions when their sum exceeds the opportunity cost function to be split.",
                "Formally: V j0,ik (t) = 8 >< >: V H 1,1 j0,ik (t) if PK k=0 V H 1,1 j0,ik (t) < Vj0 (t) Vj0 (t) V H 1,1 j0,ik (t) PK k=0 V H 1,1 j0,ik (t) otherwise Where V H 1,1 j0,ik (t) = (Vj0 · Q k ∈{0,...,K} k =k Pjk )(t).",
                "For the new heuristics, we now prove, that: THEOREM 3.",
                "Heuristics H 1,0 , H 1/2,1/2 and bH 1,1 do not overestimate the opportunity cost.",
                "PROOF.",
                "When heuristic H 1,0 is used to split the opportunity cost function Vj0 , only one method (e.g. mik ) gets the opportunity cost for enabling method mj0 .",
                "Thus: KX k =0 Vik (t) = Z Δ−t 0 pik (t )Vj0,ik (t + t )dt (6) And since Vj0 is non-increasing ≤ Z Δ−t 0 pik (t )Vj0 (t + t ) · Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ Vj0 (t) The last inequality is also a consequence of the fact that Vj0 is non-increasing.",
                "For heuristic H 1/2,1/2 we similarly have: KX k=0 Vik (t) ≤ KX k=0 Z Δ−t 0 pik (t ) 1 K Vj0 (t + t ) Y k ∈{0,...,K} k =k Pjk (t + t )dt ≤ 1 K KX k=0 Z Δ−t 0 pik (t )Vj0 (t + t )dt ≤ 1 K · K · Vj0 (t) = Vj0 (t).",
                "For heuristic bH 1,1 , the opportunity cost function Vj0 is by definition split in such manner, that PK k=0 Vik (t) ≤ Vj0 (t).",
                "Consequently, we have proved, that our new heuristics H 1,0 , H 1/2,1/2 and bH 1,1 avoid the overestimation of the opportunity cost.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 835 The reason why we have introduced all three new heuristics is the following: Since H 1,1 overestimates the opportunity cost, one has to choose which method mik will receive the reward from enabling the method mj0 , which is exactly what the heuristic H 1,0 does.",
                "However, heuristic H 1,0 leaves K − 1 methods that precede the method mj0 without any reward which leads to starvation.",
                "Starvation can be avoided if opportunity cost functions are split using heuristic H 1/2,1/2 , that provides reward to all enabling methods.",
                "However, the sum of split opportunity cost functions for the H 1/2,1/2 heuristic can be smaller than the non-zero split opportunity cost function for the H 1,0 heuristic, which is clearly undesirable.",
                "Such situation (Figure 4, heuristic H 1,0 ) occurs because the mean f+g 2 of two functions f, g is not smaller than f nor g only if f = g. This is why we have proposed the bH 1,1 heuristic, which by definition avoids the overestimation, underestimation and starvation problems. 7.",
                "EXPERIMENTAL EVALUATION Since the VFP algorithm that we introduced provides two orthogonal improvements over the OC-DEC-MDP algorithm, the experimental evaluation we performed consisted of two parts: In part 1, we tested empirically the quality of solutions that an locally optimal solver (either OC-DEC-MDP or VFP) finds, given it uses different opportunity cost function splitting heuristic, and in part 2, we compared the runtimes of the VFP and OC-DEC- MDP algorithms for a variety of mission plan configurations.",
                "Part 1: We first ran the VFP algorithm on a generic mission plan configuration from Figure 3 where only methods mj0 , mi1 , mi2 and m0 were present.",
                "Time windows of all methods were set to 400, duration pj0 of method mj0 was uniform, i.e., pj0 (t) = 1 400 and durations pi1 , pi2 of methods mi1 , mi2 were normal distributions, i.e., pi1 = N(μ = 250, σ = 20), and pi2 = N(μ = 200, σ = 100).",
                "We assumed that only method mj0 provided reward, i.e. rj0 = 10 was the reward for finishing the execution of method mj0 before time t = 400.",
                "We show our results in Figure (4) where the x-axis of each of the graphs represents time whereas the y-axis represents the opportunity cost.",
                "The first graph confirms, that when the opportunity cost function Vj0 was split into opportunity cost functions Vi1 and Vi2 using the H 1,1 heuristic, the function Vi1 +Vi2 was not always below the Vj0 function.",
                "In particular, Vi1 (280) + Vi2 (280) exceeded Vj0 (280) by 69%.",
                "When heuristics H 1,0 , H 1/2,1/2 and bH 1,1 were used (graphs 2,3 and 4), the function Vi1 + Vi2 was always below Vj0 .",
                "We then shifted our attention to the civilian rescue domain introduced in Figure 1 for which we sampled all action execution durations from the normal distribution N = (μ = 5, σ = 2)).",
                "To obtain the baseline for the heuristic performance, we implemented a globally optimal solver, that found a true expected total reward for this domain (Figure (6a)).",
                "We then compared this reward with a expected total reward found by a locally optimal solver guided by each of the discussed heuristics.",
                "Figure (6a), which plots on the y-axis the expected total reward of a policy complements our previous results: H 1,1 heuristic overestimated the expected total reward by 280% whereas the other heuristics were able to guide the locally optimal solver close to a true expected total reward.",
                "Part 2: We then chose H 1,1 to split the opportunity cost functions and conducted a series of experiments aimed at testing the scalability of VFP for various mission plan configurations, using the performance of the OC-DEC-MDP algorithm as a benchmark.",
                "We began the VFP scalability tests with a configuration from Figure (5a) associated with the civilian rescue domain, for which method execution durations were extended to normal distributions N(μ = Figure 5: Mission plan configurations: (a) civilian rescue domain, (b) chain of n methods, (c) tree of n methods with branching factor = 3 and (d) square mesh of n methods.",
                "Figure 6: VFP performance in the civilian rescue domain. 30, σ = 5), and the deadline was extended to Δ = 200.",
                "We decided to test the runtime of the VFP algorithm running with three different levels of accuracy, i.e., different approximation parameters P and V were chosen, such that the cumulative error of the solution found by VFP stayed within 1%, 5% and 10% of the solution found by the OC- DEC-MDP algorithm.",
                "We then run both algorithms for a total of 100 policy improvement iterations.",
                "Figure (6b) shows the performance of the VFP algorithm in the civilian rescue domain (y-axis shows the runtime in milliseconds).",
                "As we see, for this small domain, VFP runs 15% faster than OCDEC-MDP when computing the policy with an error of less than 1%.",
                "For comparison, the globally optimal solved did not terminate within the first three hours of its runtime which shows the strength of the opportunistic solvers, like OC-DEC-MDP.",
                "We next decided to test how VFP performs in a more difficult domain, i.e., with methods forming a long chain (Figure (5b)).",
                "We tested chains of 10, 20 and 30 methods, increasing at the same time method time windows to 350, 700 and 1050 to ensure that later methods can be reached.",
                "We show the results in Figure (7a), where we vary on the x-axis the number of methods and plot on the y-axis the algorithm runtime (notice the logarithmic scale).",
                "As we observe, scaling up the domain reveals the high performance of VFP: Within 1% error, it runs up to 6 times faster than OC-DECMDP.",
                "We then tested how VFP scales up, given that the methods are arranged into a tree (Figure (5c)).",
                "In particular, we considered trees with branching factor of 3, and depth of 2, 3 and 4, increasing at the same time the time horizon from 200 to 300, and then to 400.",
                "We show the results in Figure (7b).",
                "Although the speedups are smaller than in case of a chain, the VFP algorithm still runs up to 4 times faster than OC-DEC-MDP when computing the policy with an error of less than 1%.",
                "We finally tested how VFP handles the domains with methods arranged into a n × n mesh, i.e., C≺ = { mi,j, mk,j+1 } for i = 1, ..., n; k = 1, ..., n; j = 1, ..., n − 1.",
                "In particular, we consider 836 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 4: Visualization of heuristics for opportunity costs splitting.",
                "Figure 7: Scalability experiments for OC-DEC-MDP and VFP for different network configurations. meshes of 3×3, 4×4, and 5×5 methods.",
                "For such configurations we have to greatly increase the time horizon since the probabilities of enabling the final methods by a particular time decrease exponentially.",
                "We therefore vary the time horizons from 3000 to 4000, and then to 5000.",
                "We show the results in Figure (7c) where, especially for larger meshes, the VFP algorithm runs up to one order of magnitude faster than OC-DEC-MDP while finding a policy that is within less than 1% from the policy found by OC- DECMDP. 8.",
                "CONCLUSIONS Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very difficult to solve, especially for the real-world domains.",
                "In this paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.",
                "Our heuristic solution method, called Value Function Propagation (VFP), provided two orthogonal improvements of OC-DEC-MDP: (i) It speeded up OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the overestimation of the opportunity cost of OC-DEC-MDP.",
                "In terms of related work, we have extensively discussed the OCDEC-MDP algorithm [4].",
                "Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].",
                "Unfortunately, they fail to scale up to large-scale domains at present time.",
                "Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.",
                "Finally, value function techniques have been studied in context of single agent MDPs [7] [9].",
                "However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.",
                "Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research Laboratory under Contract No.",
                "FA875005C0030.",
                "The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 9.",
                "REFERENCES [1] R. Becker, V. Lesser, and S. Zilberstein.",
                "Decentralized MDPs with Event-Driven Interactions.",
                "In AAMAS, pages 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser, and C. V. Goldman.",
                "Transition-Independent Decentralized Markov Decision Processes.",
                "In AAMAS, pages 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein, and N. Immerman.",
                "The complexity of decentralized control of Markov decision processes.",
                "In UAI, pages 32-37, 2000. [4] A. Beynier and A. Mouaddib.",
                "A polynomial algorithm for decentralized Markov decision processes with temporal constraints.",
                "In AAMAS, pages 963-969, 2005. [5] A. Beynier and A. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized Markov decision processes.",
                "In AAAI, pages 1089-1094, 2006. [6] C. Boutilier.",
                "Sequential optimality and coordination in multiagent systems.",
                "In IJCAI, pages 478-485, 1999. [7] J. Boyan and M. Littman.",
                "Exact solutions to time-dependent MDPs.",
                "In NIPS, pages 1026-1032, 2000. [8] C. Goldman and S. Zilberstein.",
                "Optimizing information exchange in cooperative multi-agent systems, 2003. [9] L. Li and M. Littman.",
                "Lazy approximation for solving continuous finite-horizon MDPs.",
                "In AAAI, pages 1175-1180, 2005. [10] Y. Liu and S. Koenig.",
                "Risk-sensitive planning with one-switch utility functions: Value iteration.",
                "In AAAI, pages 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman, and M. Boddy.",
                "Coordinated plan management using multiagent MDPs.",
                "In AAAI Spring Symposium, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, and S. Marsella.",
                "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings.",
                "In IJCAI, pages 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe, and M. Yokoo.",
                "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs.",
                "In IJCAI, pages 1758-1760, 2005.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 837"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        }
    }
}