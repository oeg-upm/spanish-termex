{
    "id": "H-3",
    "original_text": "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP. 6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one. Studies in literature have found many contextual factors that strongly influence the interpretation of a query. Recent studies have tried to consider the users interests by creating a user profile. However, a single profile for a user may not be sufficient for a variety of queries of the user. In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query. The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations. In this paper, both types of context are integrated in an IR model based on language modeling. Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness. Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1. INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need. Many relevant terms can be absent from queries and terms included may be ambiguous. These issues have been addressed in a large number of previous studies. Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc. In these studies, however, it has been generally assumed that query is the only element available about the users information need. In reality, query is always formulated in a search context. As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments. These factors include, among many others, the users domain of interest, knowledge, preferences, etc. All these elements specify the contexts around the query. So we call them context around query in this paper. It has been demonstrated that users query should be placed in its context for a correct interpretation. Recent studies have investigated the integration of some contexts around the query [9][30][23]. Typically, a user profile is constructed to reflect the users domains of interest and background. A user profile is used to favor the documents that are more closely related to the profile. However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query. For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored. A possible solution to this problem is to use query-related profiles or models instead of user-centric ones. In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query. This method allows us to select more appropriate query-specific context around the query. Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science. Using this relation, one would be able to expand the query program with the term computer. However, domain knowledge is available only for a few domains (e.g. Medicine). The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27]. However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query. For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program. Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program. Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV. So the important question is how we can serve these context words in queries to select the appropriate relations to apply. These context words form a context within query. In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer). Although improvements are observed in some cases, they are limited. We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations. The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context. This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains. The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors. Our approach has been tested on several TREC collections. The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary. We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain. This paper is organized as follows. In section 2, we review some related work and introduce the principle of our approach. Section 3 presents our general model. Then sections 4 and 5 describe respectively the domain model and the knowledge model. Section 6 explains the method for parameter training. Experiments are presented in section 7 and conclusions in section 8. 2. CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14]. Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21]. In this section, we review some of the studies in IR concerning these aspects. Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query. It can be used in different ways. Most often, a user profile is created to encompass all the domains of interest of a user [23]. In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user. The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user. On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history. In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains. The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile. This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified. A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest. The domains related to a query are then identified according to the query. This will enable us to use a more appropriate query-specific profile, instead of a user-centric one. This approach is used in [18] in which ODP directories are used. However, only a small scale experiment has been carried out. A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them. However, the experiments showed variable results. It remains unclear whether domain models can be effectively used in IR. In this study, we also model topic domains. We will carry out experiments on both automatic and manual identification of query domains. Domain models will also be integrated with other factors. In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce. Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31]. In both cases, the relations are defined between two single terms such as t1→t2. If a query contains term t1, then t2 is always considered as a candidate for expansion. As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not. For example, program→computer should not be applied to TV program even if the latter contains program. However, little information is available in the relation to help us determine if an application context is appropriate. To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31]. Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word. Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain. For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term. It is possible to integrate stronger control on the utilization of knowledge. For example, [17] defined strong logical relations to encode knowledge of different domains. If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied. However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice. In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts. As a result, computer will be used to expand queries Java program or program algorithm, but not TV program. This principle is similar to that of [33] for word sense disambiguation. However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts. From this point of view, our approach is more similar to word sense discrimination [27]. In this paper, we use the same approach and we will integrate it into a more global model with other context factors. As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query. Within query context exists in many queries. In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity). Some context words are often used together with it. In these cases, contexts within query are created and can be exploited. Query profile and other factors Many attempts have been made in IR to create query-specific profiles. We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family. A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query. In order to create a good query model, such a query-specific feedback model should be integrated. There are many other contextual factors ([26]) that we do not deal with in this paper. However, it seems clear that many factors are complementary. As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context. Both types of contexts have been proven useful [32]. Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment. These terms are often presumed when a user issues a query such as waste cleanup in the domain. It is useful to add them into the query. We see a clear complementarity among these factors. It is then useful to combine them together in a single IR model. In this study, we will integrate all the above factors within a unified framework based on language modeling. Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them. This is described in the following section. 3. GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary. Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model. In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing. In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query. To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need. In particular, all the related and presumed words should be included in the query model. A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34]. In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms. They are then combined through interpolation. In this paper, we generalize this approach and integrate more models for the query. Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE. F Qθ has been used in several previous studies [16][35]. In this paper, F Qθ is extracted using the 20 blind feedback documents. We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5. Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights. Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model. Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30]. The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting). We describe this in the following sections. 4. CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain. These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP. In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains. By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents. The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results. An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned. These domains have been mapped to ODP categories. It is found that both approaches mentioned above are equally effective and result in comparable performance. Therefore, in this study, we only use the second approach. This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query. This will be explained in detail in our experiments. Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted. If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge. Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model). Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]). The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35]. It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language. This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations). Table 1. Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query. This can be done manually by the user or automatically by the system using query classification. We will compare both approaches. Query classification has been investigated in several studies [18][28]. In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e.: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22]. The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise. Therefore, we only retain the top 100 strongest terms. The same strategy is used for Knowledge model. Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large. This is particularly true for large domains such as Science and technology defined in TREC queries. Using such a large domain model as the background can introduce much noise terms. Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query. These documents are the top-ranked documents retrieved with the original query within the domain. This approach is indeed a combination of domain and feedback models. In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5. EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically. In general, a term relation can be represented as A→B. Both A and B have been restricted to single terms in previous studies. A single term in A means that the relation is applicable to all the queries containing that term. As we explained earlier, this is the source of many wrong applications. The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query. For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query. The term added in the condition specifies a stricter context to apply the relation. We call this type of relation context-dependent relation. In principle, the addition is not restricted to one term. However, we will make this restriction due to the following reasons: • User queries are usually very short. Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations. The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13]. Here, we use a simple co-occurrence analysis. Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences. In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related. We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query. This is a direct extension of the translation model proposed in [3] to our context-dependent relations. The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6. MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3). As the parameter λ only affects document model, we will set it to the same value in all our experiments. The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2. The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction. We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged. Each direction is searched in turn, until no improvement in MAP is observed. In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7. EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3. The choice of this test collection is due to the availability of manually specified domain for each query. This allows us to compare with an approach using automatic domain identification. Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests. Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1. We can see that the distribution varies strongly between domains and between the two query sets. We have also tested on TREC 7 and 8 data. For this series of tests, each collection is used in turn as training data while the other is used for testing. Some statistics of the data are described in Tab. 2. All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used. Some queries (4, 5 and 3 in the three query sets) only contain one word. For these queries, knowledge model is not applicable. On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically? How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries. How do they compare? On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones. Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback. In all the experiments, document models are created using Jelinek-Mercer smoothing. This choice is made according to the observation in [36] that the method performs very well for long queries. In our case, as queries are expanded, they perform similarly to long queries. In our preliminary tests, we also found this method performed better than the other methods (e.g. Dirichlet), especially for the main baseline method with Feedback model. Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback). We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries. This latter selects expansion terms with strongest global relation to the query. This relation is measured by the sum of relations to each of the query terms. This method is equivalent to [24]. It is also similar to the translation model [3]. We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov. M edical&Bio.M ilitaryPolitics Sci.&Tech. U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1. Distribution of domains Table 2. TREC collection statistics Collection Document Size (GB) Voc. # of Doc. Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4. T-test is also performed for statistical significance. As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used. All the improvements over cooccurrence model are statistically significant (this is not shown in the table). The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion. This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms. The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower. However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3. This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways. Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included. C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history. Strategies for using domain models: U1 - The domain model is determined by the user manually. U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2. In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models. The same method is used on queries 1-50 to tune the parameters. Table 3. Baseline models Unigram Model Coll. Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4. Knowledge models Co-occurrence Knowledge model Coll. Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.) Table 5. Domain models with relevant documents (C1) Domain Sub-Domain Coll. Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6. Domain models with top-100 documents (C2) Domain Sub-Domain Coll. Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain). In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2). First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases. The improvements on Disks 1-3 and TREC7 are statistically significant. However, the improvement scales are smaller than using Feedback and Relation models. Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models. In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc. Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5). In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models. This may seem surprising. An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain. Relevant documents for all in-domain queries vary greatly. Therefore, in some large domains, characteristic terms have variable effects on queries. On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents. Thus both strategies produce very similar effects. This result opens the door for a simpler method that does not require relevance judgments, for example using search history. Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5). However, once Feedback model is used, the advantage disappears. On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain. It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred. On the other hand, sub-domain models capture similar characteristics to Feedback model. So when the latter is used, sub-domain models become superfluous. However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences. This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries. Here, we examine the possibility to automatically identify query domains. Table 7 shows the results with this strategy using both strategies for domain model construction. We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models). This shows that automatic domain identification is a way to select domain model as effective as manual identification. This also demonstrates the feasibility to use domain models for queries when no domain information is provided. Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications. This is much lower than the above 80% rates reported in [18]. A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g. International relations, International politics, Politics). However, in this situation, wrong domains assigned to queries are not always irrelevant and useless. For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query. Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8. This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model. We have tested both strategies to create domain models, but the differences between them are very small. So we only report the results with the relevant documents. Our first observation is that the complete models produce the best results. All the improvements over the baseline model (with feedback) are statistically significant. This result confirms that the integration of contextual factors is effective. Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models. Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6. We see that the most important factor is Feedback model. This is also the single factor which produced the highest improvements over the original query model. This observation seems to indicate that this model has the highest capability to capture the information need behind the query. However, even with lower weights, the other models do have strong impacts on the final effectiveness. This demonstrates the benefit of integrating more contextual factors in IR. Table 7. Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll. Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8. Complete models (C1) All Doc. Domain Coll. Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need. Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile. In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased. Similarly to some previous studies, we propose to model topic domains instead of the user. Previous investigations on context focused on factors around the query. We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion. We have integrated the above contextual factors, together with feedback model, in a single language model. Our experimental results strongly confirm the benefit of using contexts in IR. This work also shows that the language modeling framework is appropriate for integrating many contextual factors. This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains. It would also be interesting to test the method on Web search using user search history. We will investigate these problems in our future research. 9. REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval. SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models. Inf. Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Concept based query expansion. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval. SIGIR, pp.334-342, 2001.",
    "original_translation": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001.",
    "original_sentences": [
        "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
        "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
        "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
        "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
        "Recent studies have tried to consider the users interests by creating a user profile.",
        "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
        "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
        "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
        "In this paper, both types of context are integrated in an IR model based on language modeling.",
        "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
        "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
        "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
        "Many relevant terms can be absent from queries and terms included may be ambiguous.",
        "These issues have been addressed in a large number of previous studies.",
        "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
        "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
        "In reality, query is always formulated in a search context.",
        "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
        "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
        "All these elements specify the contexts around the query.",
        "So we call them context around query in this paper.",
        "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
        "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
        "Typically, a user profile is constructed to reflect the users domains of interest and background.",
        "A user profile is used to favor the documents that are more closely related to the profile.",
        "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
        "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
        "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
        "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
        "This method allows us to select more appropriate query-specific context around the query.",
        "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
        "Using this relation, one would be able to expand the query program with the term computer.",
        "However, domain knowledge is available only for a few domains (e.g.",
        "Medicine).",
        "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
        "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
        "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
        "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
        "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
        "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
        "These context words form a context within query.",
        "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
        "Although improvements are observed in some cases, they are limited.",
        "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
        "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
        "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
        "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
        "Our approach has been tested on several TREC collections.",
        "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
        "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
        "This paper is organized as follows.",
        "In section 2, we review some related work and introduce the principle of our approach.",
        "Section 3 presents our general model.",
        "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
        "Section 6 explains the method for parameter training.",
        "Experiments are presented in section 7 and conclusions in section 8. 2.",
        "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
        "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
        "In this section, we review some of the studies in IR concerning these aspects.",
        "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
        "It can be used in different ways.",
        "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
        "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
        "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
        "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
        "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
        "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
        "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
        "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
        "The domains related to a query are then identified according to the query.",
        "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
        "This approach is used in [18] in which ODP directories are used.",
        "However, only a small scale experiment has been carried out.",
        "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
        "However, the experiments showed variable results.",
        "It remains unclear whether domain models can be effectively used in IR.",
        "In this study, we also model topic domains.",
        "We will carry out experiments on both automatic and manual identification of query domains.",
        "Domain models will also be integrated with other factors.",
        "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
        "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
        "In both cases, the relations are defined between two single terms such as t1→t2.",
        "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
        "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
        "For example, program→computer should not be applied to TV program even if the latter contains program.",
        "However, little information is available in the relation to help us determine if an application context is appropriate.",
        "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
        "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
        "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
        "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
        "It is possible to integrate stronger control on the utilization of knowledge.",
        "For example, [17] defined strong logical relations to encode knowledge of different domains.",
        "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
        "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
        "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
        "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
        "This principle is similar to that of [33] for word sense disambiguation.",
        "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
        "From this point of view, our approach is more similar to word sense discrimination [27].",
        "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
        "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
        "Within query context exists in many queries.",
        "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
        "Some context words are often used together with it.",
        "In these cases, contexts within query are created and can be exploited.",
        "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
        "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
        "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
        "In order to create a good query model, such a query-specific feedback model should be integrated.",
        "There are many other contextual factors ([26]) that we do not deal with in this paper.",
        "However, it seems clear that many factors are complementary.",
        "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
        "Both types of contexts have been proven useful [32].",
        "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
        "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
        "It is useful to add them into the query.",
        "We see a clear complementarity among these factors.",
        "It is then useful to combine them together in a single IR model.",
        "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
        "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
        "This is described in the following section. 3.",
        "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
        "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
        "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
        "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
        "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
        "In particular, all the related and presumed words should be included in the query model.",
        "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
        "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
        "They are then combined through interpolation.",
        "In this paper, we generalize this approach and integrate more models for the query.",
        "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
        "F Qθ has been used in several previous studies [16][35].",
        "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
        "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
        "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
        "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
        "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
        "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
        "We describe this in the following sections. 4.",
        "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
        "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
        "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
        "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
        "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
        "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
        "These domains have been mapped to ODP categories.",
        "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
        "Therefore, in this study, we only use the second approach.",
        "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
        "This will be explained in detail in our experiments.",
        "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
        "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
        "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
        "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
        "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
        "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
        "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
        "Table 1.",
        "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
        "This can be done manually by the user or automatically by the system using query classification.",
        "We will compare both approaches.",
        "Query classification has been investigated in several studies [18][28].",
        "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
        "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
        "Therefore, we only retain the top 100 strongest terms.",
        "The same strategy is used for Knowledge model.",
        "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
        "This is particularly true for large domains such as Science and technology defined in TREC queries.",
        "Using such a large domain model as the background can introduce much noise terms.",
        "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
        "These documents are the top-ranked documents retrieved with the original query within the domain.",
        "This approach is indeed a combination of domain and feedback models.",
        "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
        "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
        "In general, a term relation can be represented as A→B.",
        "Both A and B have been restricted to single terms in previous studies.",
        "A single term in A means that the relation is applicable to all the queries containing that term.",
        "As we explained earlier, this is the source of many wrong applications.",
        "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
        "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
        "The term added in the condition specifies a stricter context to apply the relation.",
        "We call this type of relation context-dependent relation.",
        "In principle, the addition is not restricted to one term.",
        "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
        "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
        "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
        "Here, we use a simple co-occurrence analysis.",
        "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
        "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
        "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
        "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
        "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
        "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
        "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
        "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
        "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
        "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
        "Each direction is searched in turn, until no improvement in MAP is observed.",
        "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
        "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
        "The choice of this test collection is due to the availability of manually specified domain for each query.",
        "This allows us to compare with an approach using automatic domain identification.",
        "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
        "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
        "We can see that the distribution varies strongly between domains and between the two query sets.",
        "We have also tested on TREC 7 and 8 data.",
        "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
        "Some statistics of the data are described in Tab. 2.",
        "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
        "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
        "For these queries, knowledge model is not applicable.",
        "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
        "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
        "How do they compare?",
        "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
        "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
        "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
        "This choice is made according to the observation in [36] that the method performs very well for long queries.",
        "In our case, as queries are expanded, they perform similarly to long queries.",
        "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
        "Dirichlet), especially for the main baseline method with Feedback model.",
        "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
        "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
        "This latter selects expansion terms with strongest global relation to the query.",
        "This relation is measured by the sum of relations to each of the query terms.",
        "This method is equivalent to [24].",
        "It is also similar to the translation model [3].",
        "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
        "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
        "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
        "Distribution of domains Table 2.",
        "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
        "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
        "T-test is also performed for statistical significance.",
        "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
        "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
        "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
        "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
        "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
        "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
        "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
        "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
        "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
        "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
        "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
        "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
        "The same method is used on queries 1-50 to tune the parameters.",
        "Table 3.",
        "Baseline models Unigram Model Coll.",
        "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
        "Knowledge models Co-occurrence Knowledge model Coll.",
        "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
        "Table 5.",
        "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
        "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
        "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
        "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
        "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
        "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
        "The improvements on Disks 1-3 and TREC7 are statistically significant.",
        "However, the improvement scales are smaller than using Feedback and Relation models.",
        "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
        "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
        "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
        "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
        "This may seem surprising.",
        "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
        "Relevant documents for all in-domain queries vary greatly.",
        "Therefore, in some large domains, characteristic terms have variable effects on queries.",
        "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
        "Thus both strategies produce very similar effects.",
        "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
        "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
        "However, once Feedback model is used, the advantage disappears.",
        "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
        "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
        "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
        "So when the latter is used, sub-domain models become superfluous.",
        "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
        "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
        "Here, we examine the possibility to automatically identify query domains.",
        "Table 7 shows the results with this strategy using both strategies for domain model construction.",
        "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
        "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
        "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
        "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
        "This is much lower than the above 80% rates reported in [18].",
        "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
        "International relations, International politics, Politics).",
        "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
        "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
        "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
        "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
        "We have tested both strategies to create domain models, but the differences between them are very small.",
        "So we only report the results with the relevant documents.",
        "Our first observation is that the complete models produce the best results.",
        "All the improvements over the baseline model (with feedback) are statistically significant.",
        "This result confirms that the integration of contextual factors is effective.",
        "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
        "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
        "We see that the most important factor is Feedback model.",
        "This is also the single factor which produced the highest improvements over the original query model.",
        "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
        "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
        "This demonstrates the benefit of integrating more contextual factors in IR.",
        "Table 7.",
        "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
        "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
        "Complete models (C1) All Doc.",
        "Domain Coll.",
        "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
        "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
        "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
        "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
        "Similarly to some previous studies, we propose to model topic domains instead of the user.",
        "Previous investigations on context focused on factors around the query.",
        "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
        "We have integrated the above contextual factors, together with feedback model, in a single language model.",
        "Our experimental results strongly confirm the benefit of using contexts in IR.",
        "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
        "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
        "It would also be interesting to test the method on Web search using user search history.",
        "We will investigate these problems in our future research. 9.",
        "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
        "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
        "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
        "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
        "Inf.",
        "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
        "Concept based query expansion.",
        "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
        "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
        "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
        "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
        "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
        "SIGIR, pp.334-342, 2001."
    ],
    "translated_text_sentences": [
        "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP.",
        "6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
        "La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único.",
        "Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta.",
        "Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario.",
        "Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario.",
        "En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta.",
        "El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes.",
        "En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje.",
        "Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación.",
        "Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1.",
        "Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información.",
        "Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos.",
        "Estos problemas han sido abordados en un gran número de estudios previos.",
        "Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc.",
        "En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios.",
        "En realidad, la consulta siempre se formula en un contexto de búsqueda.",
        "Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia.",
        "Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc.",
        "Todos estos elementos especifican los contextos alrededor de la consulta.",
        "Así que los llamamos contexto alrededor de la consulta en este documento.",
        "Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta.",
        "Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23].",
        "Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios.",
        "Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil.",
        "Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica.",
        "Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente.",
        "Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario.",
        "En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada.",
        "Este método nos permite seleccionar un contexto más apropiado y específico para la consulta.",
        "Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación.",
        "Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora.",
        "Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo,",
        "Medicina.",
        "La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27].",
        "Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta.",
        "Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión.",
        "Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión.",
        "Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV.",
        "Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar.",
        "Estas palabras de contexto forman un contexto dentro de la consulta.",
        "En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora).",
        "Aunque se observan mejoras en algunos casos, son limitadas.",
        "Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones.",
        "El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado.",
        "Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios.",
        "El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales.",
        "Nuestro enfoque ha sido probado en varias colecciones de TREC.",
        "Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios.",
        "También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio.",
        "Este documento está organizado de la siguiente manera.",
        "En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque.",
        "La sección 3 presenta nuestro modelo general.",
        "Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento.",
        "La sección 6 explica el método para el entrenamiento de parámetros.",
        "Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8.",
        "Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14].",
        "Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21].",
        "En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos.",
        "El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta.",
        "Se puede utilizar de diferentes maneras.",
        "La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23].",
        "En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario.",
        "Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario.",
        "Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario.",
        "En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos.",
        "La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil.",
        "Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente.",
        "Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado.",
        "Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta.",
        "Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario.",
        "Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP.",
        "Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala.",
        "Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos.",
        "Sin embargo, los experimentos mostraron resultados variables.",
        "No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI.",
        "En este estudio, también modelamos dominios de temas.",
        "Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta.",
        "Los modelos de dominio también se integrarán con otros factores.",
        "En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos.",
        "Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas.",
        "En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2.",
        "Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión.",
        "Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían.",
        "Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación.",
        "Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado.",
        "Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31].",
        "Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta.",
        "Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen.",
        "Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión.",
        "Es posible integrar un control más fuerte sobre la utilización del conocimiento.",
        "Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios.",
        "Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica.",
        "Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica.",
        "En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos.",
        "Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión.",
        "Este principio es similar al de [33] para la desambiguación del sentido de las palabras.",
        "Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos.",
        "Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27].",
        "En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales.",
        "Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta.",
        "Dentro del contexto de la consulta existe en muchas consultas.",
        "De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad).",
        "Algunas palabras de contexto suelen usarse junto con ella.",
        "En estos casos, se crean contextos dentro de la consulta y pueden ser explotados.",
        "Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta.",
        "Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia.",
        "Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta.",
        "Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta.",
        "Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo.",
        "Sin embargo, parece claro que muchos factores son complementarios.",
        "Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global.",
        "Ambos tipos de contextos han demostrado ser útiles [32].",
        "El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente.",
        "Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio.",
        "Es útil agregarlos a la consulta.",
        "Observamos una clara complementariedad entre estos factores.",
        "Es útil entonces combinarlos juntos en un único modelo de IR.",
        "En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje.",
        "Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos.",
        "Esto se describe en la siguiente sección. 3.",
        "MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario.",
        "El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección.",
        "En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado.",
        "En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta.",
        "Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información.",
        "En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta.",
        "Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34].",
        "En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos.",
        "Luego se combinan a través de la interpolación.",
        "En este artículo, generalizamos este enfoque e integramos más modelos para la consulta.",
        "Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE.",
        "F Qθ ha sido utilizado en varios estudios previos [16][35].",
        "En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega.",
        "Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5.",
        "Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla.",
        "Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente.",
        "Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30].",
        "El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros).",
        "Describimos esto en las siguientes secciones. 4.",
        "CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio.",
        "Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP.",
        "En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios.",
        "Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio.",
        "Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados.",
        "Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio.",
        "Estos dominios han sido asignados a categorías de ODP.",
        "Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable.",
        "Por lo tanto, en este estudio, solo utilizamos el segundo enfoque.",
        "Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta.",
        "Esto se explicará detalladamente en nuestros experimentos.",
        "Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje.",
        "Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán.",
        "Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección).",
        "Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]).",
        "El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35].",
        "Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma.",
        "Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones).",
        "Tabla 1.",
        "Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta.",
        "Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas.",
        "Vamos a comparar ambos enfoques.",
        "La clasificación de consultas ha sido investigada en varios estudios [18][28].",
        "En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22].",
        "El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido.",
        "Por lo tanto, solo conservamos los 100 términos más fuertes.",
        "La misma estrategia se utiliza para el modelo de conocimiento.",
        "Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande.",
        "Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC.",
        "Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido.",
        "Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta.",
        "Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio.",
        "Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación.",
        "En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5.",
        "EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática.",
        "En general, una relación de términos puede ser representada como A→B.",
        "Tanto A como B han sido restringidos a términos individuales en estudios anteriores.",
        "Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término.",
        "Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas.",
        "La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta.",
        "Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta.",
        "El término añadido en la condición especifica un contexto más estricto para aplicar la relación.",
        "Llamamos a este tipo de relación relación dependiente del contexto.",
        "En principio, la adición no está restringida a un término.",
        "Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas.",
        "La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos.",
        "La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13].",
        "Aquí, utilizamos un análisis de co-ocurrencia simple.",
        "Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias.",
        "Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados.",
        "Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta.",
        "Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto.",
        "La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6.",
        "PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3).",
        "Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos.",
        "El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2.",
        "Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda.",
        "Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios.",
        "Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP.",
        "Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7.",
        "EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3.",
        "La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta.",
        "Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio.",
        "A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas.",
        "Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1.",
        "Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas.",
        "También hemos realizado pruebas con datos de TREC 7 y 8.",
        "Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas.",
        "Algunas estadísticas de los datos se describen en la Tabla 2.",
        "Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar.",
        "Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra.",
        "Para estas consultas, el modelo de conocimiento no es aplicable.",
        "En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado?",
        "¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas.",
        "¿Cómo se comparan?",
        "En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto.",
        "Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación.",
        "En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer.",
        "Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas.",
        "En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas.",
        "En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo,",
        "Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación.",
        "La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación).",
        "También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas.",
        "Este último selecciona términos de expansión con la relación global más fuerte con la consulta.",
        "Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta.",
        "Este método es equivalente a [24].",
        "También es similar al modelo de traducción [3].",
        "Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno.",
        "Medicina y Biología. Política Militar. Ciencia y Tecnología.",
        "Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1.",
        "Distribución de dominios Tabla 2.",
        "Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos",
        "Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4.",
        "La prueba t también se realiza para determinar la significancia estadística.",
        "Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación.",
        "Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla).",
        "Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas.",
        "Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados.",
        "El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores.",
        "Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos.",
        "Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras.",
        "Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio.",
        "C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda.",
        "Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente.",
        "U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2.",
        "En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio.",
        "El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros.",
        "Tabla 3.",
        "Modelos de referencia Modelo Unigrama Coll.",
        "Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4.",
        "Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col.",
        "Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.)",
        "Tabla 5.",
        "Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección.",
        "Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6.",
        "Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col.",
        "Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio).",
        "En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2).",
        "Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general.",
        "Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas.",
        "Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación.",
        "Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio.",
        "Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc.",
        "Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5).",
        "En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio.",
        "Esto puede parecer sorprendente.",
        "Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio.",
        "Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente.",
        "Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas.",
        "Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes.",
        "Por lo tanto, ambas estrategias producen efectos muy similares.",
        "Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda.",
        "Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5).",
        "Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece.",
        "Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio.",
        "Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños.",
        "Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación.",
        "Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos.",
        "Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias.",
        "Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas.",
        "Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta.",
        "La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio.",
        "Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio).",
        "Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual.",
        "Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio.",
        "Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales.",
        "Esto es mucho menor que las tasas superiores al 80% reportadas en [18].",
        "Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo,",
        "Relaciones internacionales, política internacional, política.",
        "Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles.",
        "Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta.",
        "Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8.",
        "Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento.",
        "Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas.",
        "Por lo tanto, solo informamos los resultados con los documentos relevantes.",
        "Nuestra primera observación es que los modelos completos producen los mejores resultados.",
        "Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas.",
        "Este resultado confirma que la integración de factores contextuales es efectiva.",
        "En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales.",
        "Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6.",
        "Vemos que el factor más importante es el modelo de retroalimentación.",
        "Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original.",
        "Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta.",
        "Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final.",
        "Esto demuestra el beneficio de integrar más factores contextuales en RI.",
        "Tabla 7.",
        "Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll.",
        "Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8.",
        "Modelos completos (C1) Todos los documentos.",
        "Colección de dominio.",
        "Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
        "CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario.",
        "Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario.",
        "En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente.",
        "De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario.",
        "Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta.",
        "Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta.",
        "Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje.",
        "Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR.",
        "Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales.",
        "Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta.",
        "También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario.",
        "Investigaremos estos problemas en nuestra futura investigación.",
        "REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía.",
        "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información.",
        "SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación.",
        "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico.",
        "I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish?",
        "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
        "Expansión de consulta basada en conceptos.",
        "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf.",
        "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web.",
        "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas.",
        "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados.",
        "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc.",
        "SIGIR, pp.334-342, 2001."
    ],
    "error_count": 3,
    "keys": {
        "user profile": {
            "translated_key": "perfil de usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a <br>user profile</br>.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a <br>user profile</br> is constructed to reflect the users domains of interest and background.",
                "A <br>user profile</br> is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a <br>user profile</br> is created to encompass all the domains of interest of a user [23].",
                "In [5], a <br>user profile</br> contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single <br>user profile</br> (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the <br>user profile</br> can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single <br>user profile</br>, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a <br>user profile</br>.",
                "In this paper, we argue that a single <br>user profile</br> (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "Recent studies have tried to consider the users interests by creating a <br>user profile</br>.",
                "Typically, a <br>user profile</br> is constructed to reflect the users domains of interest and background.",
                "A <br>user profile</br> is used to favor the documents that are more closely related to the profile.",
                "Most often, a <br>user profile</br> is created to encompass all the domains of interest of a user [23].",
                "In [5], a <br>user profile</br> contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user."
            ],
            "translated_annotated_samples": [
                "Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un <br>perfil de usuario</br>.",
                "Normalmente, un <br>perfil de usuario</br> se construye para reflejar los dominios de interés y antecedentes de los usuarios.",
                "Un <br>perfil de usuario</br> se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil.",
                "La mayoría de las veces, se crea un <br>perfil de usuario</br> para abarcar todos los dominios de interés de un usuario [23].",
                "En [5], un <br>perfil de usuario</br> contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un <br>perfil de usuario</br>. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un <br>perfil de usuario</br> se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un <br>perfil de usuario</br> se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un <br>perfil de usuario</br> para abarcar todos los dominios de interés de un usuario [23]. En [5], un <br>perfil de usuario</br> contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query-specific context": {
            "translated_key": "contexto específico de consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate <br>query-specific context</br> around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "This method allows us to select more appropriate <br>query-specific context</br> around the query."
            ],
            "translated_annotated_samples": [
                "Este método nos permite seleccionar un contexto más apropiado y específico para la consulta."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "user-centric one": {
            "translated_key": "uno centrado en el usuario",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a <br>user-centric one</br>.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "This will enable us to use a more appropriate query-specific profile, instead of a <br>user-centric one</br>."
            ],
            "translated_annotated_samples": [
                "Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                []
            ]
        },
        "domain of interest": {
            "translated_key": "dominio de interés",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the <br>domain of interest</br>, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users <br>domain of interest</br>, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users <br>domain of interest</br>, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users <br>domain of interest</br> and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "<br>domain of interest</br> and context around query A <br>domain of interest</br> specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate <br>domain of interest</br>.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "The former specifies the environment of a query such as the <br>domain of interest</br>, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "These factors include, among many others, the users <br>domain of interest</br>, knowledge, preferences, etc.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users <br>domain of interest</br>, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users <br>domain of interest</br> and knowledge are considered to be among the most important ones [20][21].",
                "<br>domain of interest</br> and context around query A <br>domain of interest</br> specifies a particular background for the interpretation of a query."
            ],
            "translated_annotated_samples": [
                "El primero especifica el entorno de una consulta, como el <br>dominio de interés</br>, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes.",
                "Estos factores incluyen, entre muchos otros, el <br>dominio de interés</br> de los usuarios, conocimientos, preferencias, etc.",
                "Hay muchos factores contextuales en IR: el <br>dominio de interés</br> de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14].",
                "Entre ellos, el <br>dominio de interés</br> y conocimiento de los usuarios se consideran como uno de los más importantes [20][21].",
                "El <br>dominio de interés</br> y el contexto alrededor de la consulta. Un <br>dominio de interés</br> especifica un antecedente particular para la interpretación de una consulta."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el <br>dominio de interés</br>, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el <br>dominio de interés</br> de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el <br>dominio de interés</br> de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el <br>dominio de interés</br> y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El <br>dominio de interés</br> y el contexto alrededor de la consulta. Un <br>dominio de interés</br> especifica un antecedente particular para la interpretación de una consulta. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "interest domain": {
            "translated_key": "dominio de interés",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "context factor": {
            "translated_key": "factores de contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the <br>context factor</br>s brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other <br>context factor</br>s.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "Our experiments on several TREC collections show that each of the <br>context factor</br>s brings significant improvements in retrieval effectiveness.",
                "In this paper, we use the same approach and we will integrate it into a more global model with other <br>context factor</br>s."
            ],
            "translated_annotated_samples": [
                "Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los <br>factores de contexto</br> aporta mejoras significativas en la efectividad de recuperación.",
                "En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros <br>factores contextuales</br>."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los <br>factores de contexto</br> aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros <br>factores contextuales</br>. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    "factores de contexto",
                    "factores contextuales"
                ]
            ]
        },
        "word sense disambiguation": {
            "translated_key": "desambiguación del sentido de las palabras",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using <br>word sense disambiguation</br> [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for <br>word sense disambiguation</br>.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised <br>word sense disambiguation</br> rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using <br>word sense disambiguation</br> [25], etc.",
                "This principle is similar to that of [33] for <br>word sense disambiguation</br>.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised <br>word sense disambiguation</br> rivaling supervised methods."
            ],
            "translated_annotated_samples": [
                "Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la <br>desambiguación del sentido de las palabras</br> [25], etc.",
                "Este principio es similar al de [33] para la <br>desambiguación del sentido de las palabras</br>.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la <br>desambiguación del sentido de las palabras</br> [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la <br>desambiguación del sentido de las palabras</br>. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "information need": {
            "translated_key": "necesidad de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an <br>information need</br>, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the <br>information need</br>.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users <br>information need</br>.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the <br>information need</br>.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the <br>information need</br> behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user <br>information need</br>.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an <br>information need</br>, but it is not the only one.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the <br>information need</br>.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users <br>information need</br>.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the <br>information need</br>.",
                "This observation seems to indicate that this model has the highest capability to capture the <br>information need</br> behind the query."
            ],
            "translated_annotated_samples": [
                "La consulta del usuario es un elemento que especifica una <br>necesidad de información</br>, pero no es el único.",
                "Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la <br>necesidad de información</br>.",
                "En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la <br>necesidad de información</br> de los usuarios.",
                "Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la <br>necesidad de información</br>.",
                "Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la <br>información necesaria</br> detrás de la consulta."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una <br>necesidad de información</br>, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la <br>necesidad de información</br>. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la <br>necesidad de información</br> de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la <br>necesidad de información</br>. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la <br>información necesaria</br> detrás de la consulta. ",
            "candidates": [],
            "error": [
                [
                    "necesidad de información",
                    "necesidad de información",
                    "necesidad de información",
                    "necesidad de información",
                    "información necesaria"
                ]
            ]
        },
        "search context": {
            "translated_key": "contexto de búsqueda",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a <br>search context</br>.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "In reality, query is always formulated in a <br>search context</br>."
            ],
            "translated_annotated_samples": [
                "En realidad, la consulta siempre se formula en un <br>contexto de búsqueda</br>."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un <br>contexto de búsqueda</br>. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "domain knowledge": {
            "translated_key": "conocimiento de dominio",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is <br>domain knowledge</br>, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, <br>domain knowledge</br> is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of <br>domain knowledge</br> has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "Another strong contextual factor identified in literature is <br>domain knowledge</br>, or domain-specific term relations, such as program→computer in computer science.",
                "However, <br>domain knowledge</br> is available only for a few domains (e.g.",
                "The shortage of <br>domain knowledge</br> has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27]."
            ],
            "translated_annotated_samples": [
                "Otro factor contextual fuerte identificado en la literatura es el <br>conocimiento del dominio</br>, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación.",
                "Sin embargo, el <br>conocimiento de dominio</br> está disponible solo para algunos dominios (por ejemplo,",
                "La escasez de <br>conocimiento de dominio</br> ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el <br>conocimiento del dominio</br>, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el <br>conocimiento de dominio</br> está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de <br>conocimiento de dominio</br> ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    "conocimiento del dominio",
                    "conocimiento de dominio",
                    "conocimiento de dominio"
                ]
            ]
        },
        "utilization of general knowledge": {
            "translated_key": "conocimiento general",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the <br>utilization of general knowledge</br> for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "The shortage of domain knowledge has led to the <br>utilization of general knowledge</br> for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27]."
            ],
            "translated_annotated_samples": [
                "La escasez de conocimiento de dominio ha llevado a la utilización de <br>conocimiento general</br> para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de <br>conocimiento general</br> para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "general knowledge utilization": {
            "translated_key": "Utilización del conocimiento general",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "problem of knowledge ambiguity": {
            "translated_key": "ambigüedad del conocimiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous <br>problem of knowledge ambiguity</br> [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "However, the use of general knowledge gives rise to an enormous <br>problem of knowledge ambiguity</br> [31]: we are often unable to determine if a relation applies to a query."
            ],
            "translated_annotated_samples": [
                "Sin embargo, el uso del conocimiento general da lugar a un enorme problema de <br>ambigüedad del conocimiento</br> [31]: a menudo no podemos determinar si una relación se aplica a una consulta."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de <br>ambigüedad del conocimiento</br> [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "knowledge ambiguity problem": {
            "translated_key": "problema de ambigüedad del conocimiento",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "context-independent": {
            "translated_key": "independientes del contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, <br>context-independent</br> (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a <br>context-independent</br> relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with <br>context-independent</br> ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional <br>context-independent</br> term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, <br>context-independent</br> (such as program→computer).",
                "For example, instead of creating a <br>context-independent</br> relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with <br>context-independent</br> ones.",
                "We also compare the context-dependent knowledge model with the traditional <br>context-independent</br> term relations (defined between two single terms), which are used to expand queries."
            ],
            "translated_annotated_samples": [
                "En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, <br>independientes del contexto</br> (como programa→computadora).",
                "Por ejemplo, en lugar de crear una relación <br>independiente del contexto</br> Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta.",
                "En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto.",
                "También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales <br>independientes del contexto</br> (definidas entre dos términos individuales), que se utilizan para ampliar las consultas."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, <br>independientes del contexto</br> (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación <br>independiente del contexto</br> Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales <br>independientes del contexto</br> (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    "independientes del contexto",
                    "independiente del contexto",
                    "independientes del contexto"
                ]
            ]
        },
        "context information": {
            "translated_key": "información de contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary <br>context information</br> in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of <br>context information</br> in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating <br>context information</br> into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "We argue that the problem stems from the lack of necessary <br>context information</br> in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of <br>context information</br> in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "This confirms the hypothesis we made, that by incorporating <br>context information</br> into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms."
            ],
            "translated_annotated_samples": [
                "Sostenemos que el problema se origina en la falta de <br>información de contexto</br> necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones.",
                "En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de <br>información de contexto</br> en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos.",
                "Esto confirma la hipótesis que planteamos, que al incorporar <br>información de contexto</br> en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de <br>información de contexto</br> necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de <br>información de contexto</br> en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar <br>información de contexto</br> en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "domain model": {
            "translated_key": "modelo de dominio",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific <br>domain model</br>: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the <br>domain model</br> and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "<br>domain model</br> specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a <br>domain model</br> and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting <br>domain model</br> will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the <br>domain model</br> Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the <br>domain model</br> of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the <br>domain model</br> is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the <br>domain model</br> are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the <br>domain model</br> too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large <br>domain model</br> as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-<br>domain model</br> more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the <br>domain model</br>? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The <br>domain model</br> is determined by the user manually.",
                "U2 - The <br>domain model</br> is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a <br>domain model</br> (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for <br>domain model</br> construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select <br>domain model</br> as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, <br>domain model</br> and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "This paper aims to make contributions on the following aspects: • Query-specific <br>domain model</br>: We construct more specific domain models instead of a single user model grouping all the domains.",
                "Then sections 4 and 5 describe respectively the <br>domain model</br> and the knowledge model.",
                "<br>domain model</br> specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a <br>domain model</br> and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "If maximum likelihood estimation is used directly on these documents, the resulting <br>domain model</br> will contain both domain-specific terms and general terms, and the former do not emerge."
            ],
            "translated_annotated_samples": [
                "Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios.",
                "Luego, las secciones 4 y 5 describen respectivamente el <br>modelo de dominio</br> y el modelo de conocimiento.",
                "El <br>modelo de dominio</br> especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente.",
                "Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un <br>modelo de dominio</br> y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE.",
                "Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el <br>modelo de dominio</br> resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el <br>modelo de dominio</br> y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El <br>modelo de dominio</br> especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un <br>modelo de dominio</br> y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el <br>modelo de dominio</br> resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "radical solution": {
            "translated_key": "solución más radical",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more <br>radical solution</br> lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more <br>radical solution</br> lies in the addition of contexts in relations."
            ],
            "translated_annotated_samples": [
                "Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una <br>solución más radical</br> radica en la adición de contextos en las relaciones."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una <br>solución más radical</br> radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "google personalized search": {
            "translated_key": "Búsqueda Personalizada de Google",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as <br>google personalized search</br> [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "On the other hand, [9][15][26][30], as well as <br>google personalized search</br> [12] use the documents read by the user, stored on users computer or extracted from users search history."
            ],
            "translated_annotated_samples": [
                "Por otro lado, [9][15][26][30], así como la <br>Búsqueda Personalizada de Google</br> [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la <br>Búsqueda Personalizada de Google</br> [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "query context": {
            "translated_key": "contexto de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within <br>query context</br> exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "Within <br>query context</br> exists in many queries."
            ],
            "translated_annotated_samples": [
                "Dentro del <br>contexto de la consulta</br> existe en muchas consultas."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del <br>contexto de la consulta</br> existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "term relation": {
            "translated_key": "relación de términos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) language model created for a document D, θQ a language model for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a <br>term relation</br> can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single language model.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "In general, a <br>term relation</br> can be represented as A→B."
            ],
            "translated_annotated_samples": [
                "En general, una <br>relación de términos</br> puede ser representada como A→B."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un modelo de lenguaje (unigrama) creado para un documento D, θQ un modelo de lenguaje para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un modelo de lenguaje. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un modelo de lenguaje general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una <br>relación de términos</br> puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo modelo de lenguaje. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "language model": {
            "translated_key": "modelo de lenguaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Using Query Contexts in Information Retrieval Jing Bai 1 , Jian-Yun Nie 1 , Hugues Bouchard 2 , Guihong Cao 1 1 Department IRO, University of Montreal CP.",
                "6128, succursale Centre-ville, Montreal, Quebec, H3C 3J7, Canada {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo!",
                "Inc. Montreal, Quebec, Canada bouchard@yahoo-inc.com ABSTRACT User query is an element that specifies an information need, but it is not the only one.",
                "Studies in literature have found many contextual factors that strongly influence the interpretation of a query.",
                "Recent studies have tried to consider the users interests by creating a user profile.",
                "However, a single profile for a user may not be sufficient for a variety of queries of the user.",
                "In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query.",
                "The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations.",
                "In this paper, both types of context are integrated in an IR model based on language modeling.",
                "Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.",
                "Categories and Subject Descriptors H.3.3 [Information storage and retrieval]: Information Search and Retrieval - Retrieval Models General Terms Algorithms, Performance, Experimentation, Theory. 1.",
                "INTRODUCTION Queries, especially short queries, do not provide a complete specification of the information need.",
                "Many relevant terms can be absent from queries and terms included may be ambiguous.",
                "These issues have been addressed in a large number of previous studies.",
                "Typical solutions include expanding either document or query representation [19][35] by exploiting different resources [24][31], using word sense disambiguation [25], etc.",
                "In these studies, however, it has been generally assumed that query is the only element available about the users information need.",
                "In reality, query is always formulated in a search context.",
                "As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgments.",
                "These factors include, among many others, the users domain of interest, knowledge, preferences, etc.",
                "All these elements specify the contexts around the query.",
                "So we call them context around query in this paper.",
                "It has been demonstrated that users query should be placed in its context for a correct interpretation.",
                "Recent studies have investigated the integration of some contexts around the query [9][30][23].",
                "Typically, a user profile is constructed to reflect the users domains of interest and background.",
                "A user profile is used to favor the documents that are more closely related to the profile.",
                "However, a single profile for a user can group a variety of different domains, which are not always relevant to a particular query.",
                "For example, if a user working in computer science issues a query Java hotel, the documents on Java language will be incorrectly favored.",
                "A possible solution to this problem is to use query-related profiles or models instead of user-centric ones.",
                "In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query.",
                "This method allows us to select more appropriate query-specific context around the query.",
                "Another strong contextual factor identified in literature is domain knowledge, or domain-specific term relations, such as program→computer in computer science.",
                "Using this relation, one would be able to expand the query program with the term computer.",
                "However, domain knowledge is available only for a few domains (e.g.",
                "Medicine).",
                "The shortage of domain knowledge has led to the utilization of general knowledge for query expansion [31], which is more available from resources such as thesauri, or it can be automatically extracted from documents [24][27].",
                "However, the use of general knowledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query.",
                "For example, usually little information is available to determine whether program→computer is applicable to queries Java program and TV program.",
                "Therefore, the relation has been applied to all queries containing program in previous studies, leading to a wrong expansion for TV program.",
                "Looking at the two query examples, however, people can easily determine whether the relation is applicable, by considering the context words Java and TV.",
                "So the important question is how we can serve these context words in queries to select the appropriate relations to apply.",
                "These context words form a context within query.",
                "In some previous studies [24][31], context words in a query have been used to select expansion terms suggested by term relations, which are, however, context-independent (such as program→computer).",
                "Although improvements are observed in some cases, they are limited.",
                "We argue that the problem stems from the lack of necessary context information in relations themselves, and a more radical solution lies in the addition of contexts in relations.",
                "The method we propose is to add context words into the condition of a relation, such as {Java, program} → computer, to limit its applicability to the appropriate context.",
                "This paper aims to make contributions on the following aspects: • Query-specific domain model: We construct more specific domain models instead of a single user model grouping all the domains.",
                "The domain related to a specific query is selected (either manually or automatically) for each query. • Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query. • Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate multiple contextual factors.",
                "Our approach has been tested on several TREC collections.",
                "The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness, and their effects are complementary.",
                "We will also show that it is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain.",
                "This paper is organized as follows.",
                "In section 2, we review some related work and introduce the principle of our approach.",
                "Section 3 presents our general model.",
                "Then sections 4 and 5 describe respectively the domain model and the knowledge model.",
                "Section 6 explains the method for parameter training.",
                "Experiments are presented in section 7 and conclusions in section 8. 2.",
                "CONTEXTS AND UTILIZATION IN IR There are many contextual factors in IR: the users domain of interest, knowledge about the subject, preference, document recency, and so on [2][14].",
                "Among them, the users domain of interest and knowledge are considered to be among the most important ones [20][21].",
                "In this section, we review some of the studies in IR concerning these aspects.",
                "Domain of interest and context around query A domain of interest specifies a particular background for the interpretation of a query.",
                "It can be used in different ways.",
                "Most often, a user profile is created to encompass all the domains of interest of a user [23].",
                "In [5], a user profile contains a set of topic categories of ODP (Open Directory Project, http://dmoz.org) identified by the user.",
                "The documents (Web pages) classified in these categories are used to create a term vector, which represents the whole domains of interest of the user.",
                "On the other hand, [9][15][26][30], as well as Google Personalized Search [12] use the documents read by the user, stored on users computer or extracted from users search history.",
                "In all these studies, we observe that a single user profile (usually a statistical model or vector) is created for a user without distinguishing the different topic domains.",
                "The systematic application of the user profile can incorrectly bias the results for queries unrelated to the profile.",
                "This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has previously searched in or identified.",
                "A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest.",
                "The domains related to a query are then identified according to the query.",
                "This will enable us to use a more appropriate query-specific profile, instead of a user-centric one.",
                "This approach is used in [18] in which ODP directories are used.",
                "However, only a small scale experiment has been carried out.",
                "A similar approach is used in [8], where domain models are created using ODP categories and user queries are manually mapped to them.",
                "However, the experiments showed variable results.",
                "It remains unclear whether domain models can be effectively used in IR.",
                "In this study, we also model topic domains.",
                "We will carry out experiments on both automatic and manual identification of query domains.",
                "Domain models will also be integrated with other factors.",
                "In the following discussion, we will call the topic domain of a query a context around query to contrast with another context within query that we will introduce.",
                "Knowledge and context within query Due to the unavailability of domain-specific knowledge, general knowledge resources such as Wordnet and term relations extracted automatically have been used for query expansion [27][31].",
                "In both cases, the relations are defined between two single terms such as t1→t2.",
                "If a query contains term t1, then t2 is always considered as a candidate for expansion.",
                "As we mentioned earlier, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not.",
                "For example, program→computer should not be applied to TV program even if the latter contains program.",
                "However, little information is available in the relation to help us determine if an application context is appropriate.",
                "To remedy this problem, approaches have been proposed to make a selection of expansion terms after the application of relations [24][31].",
                "Typically, one defines some sort of global relation between the expansion term and the whole query, which is usually a sum of its relations to every query word.",
                "Although some inappropriate expansion terms can be removed because they are only weakly connected to some query terms, many others remain.",
                "For example, if the relation program→computer is strong enough, computer will have a strong global relation to the whole query TV program and it still remains as an expansion term.",
                "It is possible to integrate stronger control on the utilization of knowledge.",
                "For example, [17] defined strong logical relations to encode knowledge of different domains.",
                "If the application of a relation leads to a conflict with the query (or with other pieces of evidence), then it is not applied.",
                "However, this approach requires encoding all the logical consequences including contradictions in knowledge, which is difficult to implement in practice.",
                "In our earlier study [1], a simpler and more general approach is proposed to solve the problem at its source, i.e. the lack of context information in term relations: by introducing stricter conditions in a relation, for example {Java, program}→computer and {algorithm, program}→computer, the applicability of the relations will be naturally restricted to correct contexts.",
                "As a result, computer will be used to expand queries Java program or program algorithm, but not TV program.",
                "This principle is similar to that of [33] for word sense disambiguation.",
                "However, we do not explicitly assign a meaning to a word; rather we try to make differences between word usages in different contexts.",
                "From this point of view, our approach is more similar to word sense discrimination [27].",
                "In this paper, we use the same approach and we will integrate it into a more global model with other context factors.",
                "As the context words added into relations allow us to exploit the word context within the query, we call such factors context within query.",
                "Within query context exists in many queries.",
                "In fact, users often do not use a single ambiguous word such as Java as query (if they are aware of its ambiguity).",
                "Some context words are often used together with it.",
                "In these cases, contexts within query are created and can be exploited.",
                "Query profile and other factors Many attempts have been made in IR to create query-specific profiles.",
                "We can consider implicit feedback or blind feedback [7][16][29][32][35] in this family.",
                "A short-term feedback model is created for the given query from feedback documents, which has been proven to be effective to capture some aspects of the users intent behind the query.",
                "In order to create a good query model, such a query-specific feedback model should be integrated.",
                "There are many other contextual factors ([26]) that we do not deal with in this paper.",
                "However, it seems clear that many factors are complementary.",
                "As found in [32], a feedback model creates a local context related to the query, while the general knowledge or the whole corpus defines a global context.",
                "Both types of contexts have been proven useful [32].",
                "Domain model specifies yet another type of useful information: it reflects a set of specific background terms for a domain, for example pollution, rain, greenhouse, etc. for the domain of Environment.",
                "These terms are often presumed when a user issues a query such as waste cleanup in the domain.",
                "It is useful to add them into the query.",
                "We see a clear complementarity among these factors.",
                "It is then useful to combine them together in a single IR model.",
                "In this study, we will integrate all the above factors within a unified framework based on language modeling.",
                "Each component contextual factor will determines a different ranking score, and the final document ranking combines all of them.",
                "This is described in the following section. 3.",
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) <br>language model</br> created for a document D, θQ a <br>language model</br> for the query Q, and V the vocabulary.",
                "Smoothing on document model is recognized to be crucial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) where λ is an interpolation parameter and θC the collection model.",
                "In the basic language modeling approaches, the query model is estimated by Maximum Likelihood Estimation (MLE) without any smoothing.",
                "In such a setting, the basic retrieval operation is still limited to keyword matching, according to a few words in the query.",
                "To improve retrieval effectiveness, it is important to create a more complete query model that represents better the information need.",
                "In particular, all the related and presumed words should be included in the query model.",
                "A more complete query model by several methods have been proposed using feedback documents [16][35] or using term relations [1][10][34].",
                "In these cases, we construct two models for the query: the initial query model containing only the original terms, and a new model containing the added terms.",
                "They are then combined through interpolation.",
                "In this paper, we generalize this approach and integrate more models for the query.",
                "Let us use 0 Qθ to denote the original query model, F Qθ for the feedback model created from feedback documents, Dom Qθ for a domain model and K Qθ for a knowledge model created by applying term relations. 0 Qθ can be created by MLE.",
                "F Qθ has been used in several previous studies [16][35].",
                "In this paper, F Qθ is extracted using the 20 blind feedback documents.",
                "We will describe the details to construct Dom Qθ and K Qθ in Section 4 and 5.",
                "Given these models, we create the following final query model by interpolation: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) where X={0, Dom, K, F} is the set of all component models and iα (with 1=∑∈Xi iα ) are their mixture weights.",
                "Then the document score in Equation (1) is extended as follows: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) where )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = is the score according to each component model.",
                "Here we can see that our strategy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30].",
                "The remaining problem is to construct domain models and knowledge model and to combine all the models (parameter setting).",
                "We describe this in the following sections. 4.",
                "CONSTRUCTING AND USING DOMAIN MODELS As in previous studies, we exploit a set of documents already classified in each domain.",
                "These documents can be identified in two different ways: 1) One can take advantages of an existing domain hierarchy and the documents manually classified in them, such as ODP.",
                "In that case, a new query should be classified into the same domains either manually or automatically. 2) A user can define his own domains.",
                "By assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.",
                "The answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.",
                "An earlier study [4] has compared the above two strategies using TREC queries 51-150, for which a domain has been manually assigned.",
                "These domains have been mapped to ODP categories.",
                "It is found that both approaches mentioned above are equally effective and result in comparable performance.",
                "Therefore, in this study, we only use the second approach.",
                "This choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.",
                "This will be explained in detail in our experiments.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a <br>language model</br> can be extracted.",
                "If maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general <br>language model</br> (collection model).",
                "Then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) where c(t; D) is the count of t in document D and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).",
                "The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θDom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].",
                "It is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.",
                "This can be observed in the following table, which shows some words in the domain model of Environment before and after EM iterations (50 iterations).",
                "Table 1.",
                "Term probabilities before/after EM Term Initial Final change Term Initial Final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% Given a set of domain models, the related ones have to be assigned to a new query.",
                "This can be done manually by the user or automatically by the system using query classification.",
                "We will compare both approaches.",
                "Query classification has been investigated in several studies [18][28].",
                "In this study, we use a simple classification method: the selected domain is the one with which the querys KL-divergence score is the lowest, i.e. : )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) This classification method is an extension to Naïve Bayes as shown in [22].",
                "The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.",
                "Therefore, we only retain the top 100 strongest terms.",
                "The same strategy is used for Knowledge model.",
                "Although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.",
                "This is particularly true for large domains such as Science and technology defined in TREC queries.",
                "Using such a large domain model as the background can introduce much noise terms.",
                "Therefore, we further construct a sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.",
                "These documents are the top-ranked documents retrieved with the original query within the domain.",
                "This approach is indeed a combination of domain and feedback models.",
                "In our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when Feedback model is also used. 5.",
                "EXTRACTING CONTEXT-DEPENDENT TERM RELATIONS FROM DOCUMENTS In this paper, we extract term relations from the document collection automatically.",
                "In general, a term relation can be represented as A→B.",
                "Both A and B have been restricted to single terms in previous studies.",
                "A single term in A means that the relation is applicable to all the queries containing that term.",
                "As we explained earlier, this is the source of many wrong applications.",
                "The solution we propose is to add more context terms into A, so that it is applicable only when all the terms in A appear in a query.",
                "For example, instead of creating a context-independent relation Java→program, we will create {Java, computer}→program, which means that program is selected when both Java and computer appear in a query.",
                "The term added in the condition specifies a stricter context to apply the relation.",
                "We call this type of relation context-dependent relation.",
                "In principle, the addition is not restricted to one term.",
                "However, we will make this restriction due to the following reasons: • User queries are usually very short.",
                "Adding more terms into the condition will create many rarely applicable relations; • In most cases, an ambiguous word such as Java can be effectively disambiguated by one useful context word such as computer or hotel; • The addition of more terms will also lead to a higher space and time complexity for extracting and storing term relations.",
                "The extraction of relations of type {tj,tk} → ti can be performed using mining algorithms for association rules [13].",
                "Here, we use a simple co-occurrence analysis.",
                "Windows of fixed size (10 words in our case) are used to obtain co-occurrence counts of three terms, and the probability )|( kji tttP is determined as follows: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) where ),,( kji tttc is the count of co-occurrences.",
                "In order to reduce space requirement, we further apply the following filtering criteria: • The two terms in the condition should appear at least certain time together in the collection (10 in our case) and they should be related.",
                "We use the following pointwise mutual information as a measure of relatedness (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowledge model is defined as follows: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) where (tj tk)∈Q means any combination of two terms in the query.",
                "This is a direct extension of the translation model proposed in [3] to our context-dependent relations.",
                "The score according to the Knowledge model is then defined as follows: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Again, only the top 100 expansion terms are used. 6.",
                "MODEL PARAMETERS There are several parameters in our model: λ in Equation (2) and αi (i∈{0, Dom, K, F}) in Equation (3).",
                "As the parameter λ only affects document model, we will set it to the same value in all our experiments.",
                "The value λ=0.5 is determined to maximize the effectiveness of the baseline models (see Section 7.2) on the training data: TREC queries 1-50 and documents on Disk 2.",
                "The mixture weights αi of component models are trained on the same training data using the following method of line search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction.",
                "We start by searching in one direction - testing all the values in that direction, while keeping the values in other directions unchanged.",
                "Each direction is searched in turn, until no improvement in MAP is observed.",
                "In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selected. 7.",
                "EXPERIMENTS 7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.",
                "The choice of this test collection is due to the availability of manually specified domain for each query.",
                "This allows us to compare with an approach using automatic domain identification.",
                "Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.",
                "Queries 1-50 are used for training and 51-150 for testing. 13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig. 1.",
                "We can see that the distribution varies strongly between domains and between the two query sets.",
                "We have also tested on TREC 7 and 8 data.",
                "For this series of tests, each collection is used in turn as training data while the other is used for testing.",
                "Some statistics of the data are described in Tab. 2.",
                "All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.",
                "Some queries (4, 5 and 3 in the three query sets) only contain one word.",
                "For these queries, knowledge model is not applicable.",
                "On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model? • If the query domain is not specified, can it be determined automatically?",
                "How effective is this method? • We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.",
                "How do they compare?",
                "On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.",
                "Finally, we will see the impact of each component model when all the factors are combined. 7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.",
                "In all the experiments, document models are created using Jelinek-Mercer smoothing.",
                "This choice is made according to the observation in [36] that the method performs very well for long queries.",
                "In our case, as queries are expanded, they perform similarly to long queries.",
                "In our preliminary tests, we also found this method performed better than the other methods (e.g.",
                "Dirichlet), especially for the main baseline method with Feedback model.",
                "Table 3 shows the retrieval effectiveness on all the collections. 7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).",
                "We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.",
                "This latter selects expansion terms with strongest global relation to the query.",
                "This relation is measured by the sum of relations to each of the query terms.",
                "This method is equivalent to [24].",
                "It is also similar to the translation model [3].",
                "We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.",
                "M edical&Bio.M ilitaryPolitics Sci.&Tech.",
                "U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.",
                "Distribution of domains Table 2.",
                "TREC collection statistics Collection Document Size (GB) Voc. # of Doc.",
                "Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.",
                "T-test is also performed for statistical significance.",
                "As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.",
                "All the improvements over cooccurrence model are statistically significant (this is not shown in the table).",
                "The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.",
                "This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.",
                "The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model alone are slightly lower.",
                "However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.",
                "This demonstrates that the impacts produced by feedback and term relations are different and complementary. 7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.",
                "Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.",
                "C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.",
                "Strategies for using domain models: U1 - The domain model is determined by the user manually.",
                "U2 - The domain model is determined by the system. 7.4.1 Creating Domain models We test strategies C1 and C2.",
                "In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.",
                "The same method is used on queries 1-50 to tune the parameters.",
                "Table 3.",
                "Baseline models Unigram Model Coll.",
                "Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.",
                "Knowledge models Co-occurrence Knowledge model Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback. ++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively. ** and * are similar but compared to the baseline model with feedback.)",
                "Table 5.",
                "Domain models with relevant documents (C1) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.",
                "Domain models with top-100 documents (C2) Domain Sub-Domain Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).",
                "In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).",
                "First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.",
                "The improvements on Disks 1-3 and TREC7 are statistically significant.",
                "However, the improvement scales are smaller than using Feedback and Relation models.",
                "Looking at the distribution of the domains (Fig. 1), this observation is not surprising: for many domains, we only have few training queries, thus few indomain documents to create domain models.",
                "In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.",
                "Second, we observe that the two methods to create domain models perform equally well (Tab. 6 vs. Tab. 5).",
                "In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.",
                "This may seem surprising.",
                "An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.",
                "Relevant documents for all in-domain queries vary greatly.",
                "Therefore, in some large domains, characteristic terms have variable effects on queries.",
                "On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.",
                "Thus both strategies produce very similar effects.",
                "This result opens the door for a simpler method that does not require relevance judgments, for example using search history.",
                "Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).",
                "However, once Feedback model is used, the advantage disappears.",
                "On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.",
                "It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.",
                "On the other hand, sub-domain models capture similar characteristics to Feedback model.",
                "So when the latter is used, sub-domain models become superfluous.",
                "However, if domain models are constructed with top-ranked documents (Tab. 6), sub-domain models make much less differences.",
                "This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents. 7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.",
                "Here, we examine the possibility to automatically identify query domains.",
                "Table 7 shows the results with this strategy using both strategies for domain model construction.",
                "We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 & 6, Domain models).",
                "This shows that automatic domain identification is a way to select domain model as effective as manual identification.",
                "This also demonstrates the feasibility to use domain models for queries when no domain information is provided.",
                "Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.",
                "This is much lower than the above 80% rates reported in [18].",
                "A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.",
                "International relations, International politics, Politics).",
                "However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.",
                "For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.",
                "Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models. 7.5 Complete Models The results with the complete model are shown in Table 8.",
                "This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.",
                "We have tested both strategies to create domain models, but the differences between them are very small.",
                "So we only report the results with the relevant documents.",
                "Our first observation is that the complete models produce the best results.",
                "All the improvements over the baseline model (with feedback) are statistically significant.",
                "This result confirms that the integration of contextual factors is effective.",
                "Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.",
                "Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.",
                "We see that the most important factor is Feedback model.",
                "This is also the single factor which produced the highest improvements over the original query model.",
                "This observation seems to indicate that this model has the highest capability to capture the information need behind the query.",
                "However, even with lower weights, the other models do have strong impacts on the final effectiveness.",
                "This demonstrates the benefit of integrating more contextual factors in IR.",
                "Table 7.",
                "Automatic query domain identification (U2) Dom. with rel. doc. (C1) Dom. with top-100 doc. (C2) Coll.",
                "Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.",
                "Complete models (C1) All Doc.",
                "Domain Coll.",
                "Measure Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8.",
                "CONCLUSIONS Traditional IR approaches usually consider the query as the only element available for the user information need.",
                "Many previous studies have investigated the integration of some contextual factors in IR models, typically by incorporating a user profile.",
                "In this paper, we argue that a single user profile (or model) can contain a too large variety of different topics so that new queries can be incorrectly biased.",
                "Similarly to some previous studies, we propose to model topic domains instead of the user.",
                "Previous investigations on context focused on factors around the query.",
                "We showed in this paper that factors within the query are also important - they help select the appropriate term relations to apply in query expansion.",
                "We have integrated the above contextual factors, together with feedback model, in a single <br>language model</br>.",
                "Our experimental results strongly confirm the benefit of using contexts in IR.",
                "This work also shows that the language modeling framework is appropriate for integrating many contextual factors.",
                "This work can be further improved on several aspects, including other methods to extract term relations, to integrate more context words in conditions and to identify query domains.",
                "It would also be interesting to test the method on Web search using user search history.",
                "We will investigate these problems in our future research. 9.",
                "REFERENCES [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations for information retrieval, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interaction with texts: Information retrieval as information seeking behavior, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Information retrieval as statistical translation, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modèles de langue appliqués à la recherche dinformation contextuelle, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Using ODP metadata to personalize search, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Word association norms, mutual information, and lexicography.",
                "ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Relevance feedback and personalization: A language modeling perspective, In: The DELOS-NSF Workshop on Personalization and Recommender Systems Digital Libraries, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Context-based topic models for query modification, CIIR Technical Report, University of Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Stuff Ive seen: a system for personal information retrieval and re-use, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Semantic term matching in axiomatic approaches to information retrieval, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discriminative model for information retrieval.",
                "SIGIR05, pp. 290-297, 2005. [12] Goole Personalized Search, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for association rule mining - a general survey and comparison.",
                "SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Information retrieval in context: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Personalized ranking of search results with learned user interest hierarchies from bookmarks, WEBKDD05 Workshop at ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Relevance-based language models, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Belief revision for adaptive information retrieval, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Personalized web search by mapping user queries to categories, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Cluster-based retrieval using language models, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Toward a user-centered information service, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Toward a theory of user-based relevance: A call for a new paradigm of inquiry, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive Bayes Classifiers with Statistical Language Models.",
                "Inf.",
                "Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Personalized Search, Communications of ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P.",
                "Concept based query expansion.",
                "SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Retrieving with good sense, Inf.",
                "Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A reexamination of relevance: Towards a dynamic, situational definition, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Query enrichment for web-query classification.",
                "ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Context-sensitive information retrieval using implicit feedback, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizing search via automated analysis of interests and activities, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Query expansion using lexical-semantic relations.",
                "SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Query expansion using local and global document analysis, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Unsupervised word sense disambiguation rivaling supervised methods.",
                "ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contextsensitive semantic smoothing for the language modeling approach to genomic IR, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Model-based feedback in the language modeling approach to information retrieval, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "SIGIR, pp.334-342, 2001."
            ],
            "original_annotated_samples": [
                "GENERAL IR MODEL In the language modeling framework, a typical score function is defined in KL-divergence as follows: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) where θD is a (unigram) <br>language model</br> created for a document D, θQ a <br>language model</br> for the query Q, and V the vocabulary.",
                "Whatever the strategy, we will obtain a set of documents for each domain, from which a <br>language model</br> can be extracted.",
                "Therefore, we employ an EM process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general <br>language model</br> (collection model).",
                "We have integrated the above contextual factors, together with feedback model, in a single <br>language model</br>."
            ],
            "translated_annotated_samples": [
                "MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un <br>modelo de lenguaje</br> (unigrama) creado para un documento D, θQ un <br>modelo de lenguaje</br> para la consulta Q, y V el vocabulario.",
                "Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un <br>modelo de lenguaje</br>.",
                "Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un <br>modelo de lenguaje</br> general (modelo de colección).",
                "Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo <br>modelo de lenguaje</br>."
            ],
            "translated_text": "Utilizando Contextos de Consulta en la Recuperación de Información Jing Bai 1, Jian-Yun Nie 1, Hugues Bouchard 2, Guihong Cao 1 Departamento IRO, Universidad de Montreal CP. 6128, sucursal Centro-ville, Montreal, Quebec, H3C 3J7, Canadá {baijing, nie, caogui}@iro.umontreal.ca 2 Yahoo! La consulta del usuario es un elemento que especifica una necesidad de información, pero no es el único. Los estudios en literatura han encontrado muchos factores contextuales que influyen fuertemente en la interpretación de una consulta. Estudios recientes han intentado considerar los intereses de los usuarios mediante la creación de un perfil de usuario. Sin embargo, un solo perfil para un usuario puede no ser suficiente para una variedad de consultas del usuario. En este estudio, proponemos utilizar contextos específicos de la consulta en lugar de los centrados en el usuario, incluyendo el contexto alrededor de la consulta y el contexto dentro de la consulta. El primero especifica el entorno de una consulta, como el dominio de interés, mientras que el último se refiere a las palabras de contexto dentro de la consulta, lo cual es especialmente útil para la selección de relaciones de términos relevantes. En este artículo, ambos tipos de contexto se integran en un modelo de RI basado en modelado del lenguaje. Nuestros experimentos en varias colecciones de TREC muestran que cada uno de los factores de contexto aporta mejoras significativas en la efectividad de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Modelos de recuperación Términos Generales Algoritmos, Rendimiento, Experimentación, Teoría. 1. Las consultas, especialmente las consultas cortas, no proporcionan una especificación completa de la necesidad de información. Muchos términos relevantes pueden estar ausentes de las consultas y los términos incluidos pueden ser ambiguos. Estos problemas han sido abordados en un gran número de estudios previos. Las soluciones típicas incluyen expandir la representación del documento o la consulta [19][35] aprovechando diferentes recursos [24][31], utilizando la desambiguación del sentido de las palabras [25], etc. En estos estudios, sin embargo, se ha asumido generalmente que la consulta es el único elemento disponible sobre la necesidad de información de los usuarios. En realidad, la consulta siempre se formula en un contexto de búsqueda. Como se ha encontrado en muchos estudios previos [2][14][20][21][26], los factores contextuales tienen una fuerte influencia en las valoraciones de relevancia. Estos factores incluyen, entre muchos otros, el dominio de interés de los usuarios, conocimientos, preferencias, etc. Todos estos elementos especifican los contextos alrededor de la consulta. Así que los llamamos contexto alrededor de la consulta en este documento. Se ha demostrado que la consulta de los usuarios debe ser colocada en su contexto para una interpretación correcta. Estudios recientes han investigado la integración de algunos contextos alrededor de la consulta [9][30][23]. Normalmente, un perfil de usuario se construye para reflejar los dominios de interés y antecedentes de los usuarios. Un perfil de usuario se utiliza para favorecer los documentos que están más estrechamente relacionados con el perfil. Sin embargo, un solo perfil de usuario puede agrupar una variedad de dominios diferentes, que no siempre son relevantes para una consulta específica. Por ejemplo, si un usuario que trabaja en informática emite una consulta Java hotel, los documentos sobre el lenguaje Java serán favorecidos incorrectamente. Una posible solución a este problema es utilizar perfiles o modelos relacionados con la consulta en lugar de centrarse en el usuario. En este artículo, proponemos modelar dominios de temas, entre los cuales se seleccionará el o los relacionados para una consulta dada. Este método nos permite seleccionar un contexto más apropiado y específico para la consulta. Otro factor contextual fuerte identificado en la literatura es el conocimiento del dominio, o relaciones de términos específicos del dominio, como programa→computadora en ciencias de la computación. Usando esta relación, uno sería capaz de expandir el programa de consulta con el término computadora. Sin embargo, el conocimiento de dominio está disponible solo para algunos dominios (por ejemplo, Medicina. La escasez de conocimiento de dominio ha llevado a la utilización de conocimiento general para la expansión de consultas [31], el cual es más accesible a partir de recursos como tesauros, o puede ser extraído automáticamente de documentos [24][27]. Sin embargo, el uso del conocimiento general da lugar a un enorme problema de ambigüedad del conocimiento [31]: a menudo no podemos determinar si una relación se aplica a una consulta. Por ejemplo, generalmente hay poca información disponible para determinar si el programa → computadora es aplicable a consultas de programa Java y programa de televisión. Por lo tanto, la relación se ha aplicado a todas las consultas que contienen el programa en estudios anteriores, lo que ha llevado a una expansión incorrecta para el programa de televisión. Al observar los dos ejemplos de consulta, sin embargo, las personas pueden determinar fácilmente si la relación es aplicable, considerando las palabras de contexto Java y TV. Entonces, la pregunta importante es cómo podemos utilizar estas palabras de contexto en las consultas para seleccionar las relaciones apropiadas a aplicar. Estas palabras de contexto forman un contexto dentro de la consulta. En algunos estudios previos [24][31], las palabras de contexto en una consulta se han utilizado para seleccionar términos de expansión sugeridos por relaciones de términos, que son, sin embargo, independientes del contexto (como programa→computadora). Aunque se observan mejoras en algunos casos, son limitadas. Sostenemos que el problema se origina en la falta de información de contexto necesaria en las relaciones mismas, y una solución más radical radica en la adición de contextos en las relaciones. El método que proponemos es agregar palabras de contexto a la condición de una relación, como {Java, programa} → computadora, para limitar su aplicabilidad al contexto adecuado. Este documento tiene como objetivo hacer contribuciones en los siguientes aspectos: • Modelo de dominio específico de consulta: Construimos modelos de dominio más específicos en lugar de un único modelo de usuario que agrupe todos los dominios. El dominio relacionado con una consulta específica es seleccionado (ya sea manual o automáticamente) para cada consulta. • Contexto dentro de la consulta: Integrarnos palabras de contexto en relaciones de términos para que solo se puedan aplicar relaciones apropiadas a la consulta. • Múltiples factores contextuales: Finalmente, proponemos un marco basado en un enfoque de modelado de lenguaje para integrar múltiples factores contextuales. Nuestro enfoque ha sido probado en varias colecciones de TREC. Los experimentos muestran claramente que ambos tipos de contexto pueden resultar en mejoras significativas en la efectividad de recuperación, y sus efectos son complementarios. También demostraremos que es posible determinar el dominio de la consulta de forma automática, lo que resulta en una efectividad comparable a la especificación manual del dominio. Este documento está organizado de la siguiente manera. En la sección 2, revisamos algunos trabajos relacionados e introducimos el principio de nuestro enfoque. La sección 3 presenta nuestro modelo general. Luego, las secciones 4 y 5 describen respectivamente el modelo de dominio y el modelo de conocimiento. La sección 6 explica el método para el entrenamiento de parámetros. Los experimentos se presentan en la sección 7 y las conclusiones en la sección 8. Hay muchos factores contextuales en IR: el dominio de interés de los usuarios, el conocimiento sobre el tema, las preferencias, la actualidad de los documentos, y así sucesivamente [2][14]. Entre ellos, el dominio de interés y conocimiento de los usuarios se consideran como uno de los más importantes [20][21]. En esta sección, revisamos algunos de los estudios en IR relacionados con estos aspectos. El dominio de interés y el contexto alrededor de la consulta. Un dominio de interés especifica un antecedente particular para la interpretación de una consulta. Se puede utilizar de diferentes maneras. La mayoría de las veces, se crea un perfil de usuario para abarcar todos los dominios de interés de un usuario [23]. En [5], un perfil de usuario contiene un conjunto de categorías temáticas de ODP (Proyecto del Directorio Abierto, http://dmoz.org) identificadas por el usuario. Los documentos (páginas web) clasificados en estas categorías se utilizan para crear un vector de términos, que representa todos los dominios de interés del usuario. Por otro lado, [9][15][26][30], así como la Búsqueda Personalizada de Google [12], utilizan los documentos leídos por el usuario, almacenados en la computadora del usuario o extraídos del historial de búsqueda del usuario. En todos estos estudios, observamos que se crea un único perfil de usuario (generalmente un modelo estadístico o vector) para un usuario sin distinguir los diferentes dominios temáticos. La aplicación sistemática del perfil de usuario puede sesgar incorrectamente los resultados para consultas no relacionadas con el perfil. Esta situación puede ocurrir con frecuencia en la práctica, ya que un usuario puede buscar una variedad de temas fuera de los dominios que ha buscado o identificado previamente. Una posible solución a este problema es la creación de múltiples perfiles, uno para un dominio de interés separado. Los dominios relacionados con una consulta son identificados luego de acuerdo a la consulta. Esto nos permitirá utilizar un perfil específico de consulta más apropiado, en lugar de uno centrado en el usuario. Este enfoque se utiliza en [18] en el que se utilizan directorios de ODP. Sin embargo, solo se ha llevado a cabo un experimento a pequeña escala. Un enfoque similar se utiliza en [8], donde se crean modelos de dominio utilizando categorías de ODP y las consultas de los usuarios se asignan manualmente a ellos. Sin embargo, los experimentos mostraron resultados variables. No está claro si los modelos de dominio pueden ser utilizados de manera efectiva en RI. En este estudio, también modelamos dominios de temas. Realizaremos experimentos tanto en la identificación automática como manual de dominios de consulta. Los modelos de dominio también se integrarán con otros factores. En la siguiente discusión, llamaremos al dominio del tema de una consulta un contexto alrededor de la consulta para contrastarlo con otro contexto dentro de la consulta que introduciremos. Debido a la falta de conocimiento específico del dominio, se han utilizado recursos de conocimiento general como Wordnet y relaciones de términos extraídas automáticamente para la expansión de consultas. En ambos casos, las relaciones se definen entre dos términos individuales, como t1→t2. Si una consulta contiene el término t1, entonces t2 siempre se considera como un candidato para la expansión. Como mencionamos anteriormente, nos enfrentamos al problema de la ambigüedad de las relaciones: algunas relaciones se aplican a una consulta y otras no deberían. Por ejemplo, el término \"programa\" aplicado a \"computadora\" no debería ser utilizado para referirse a un programa de televisión, aunque este último contenga programación. Sin embargo, hay poca información disponible en relación con la ayuda para determinar si un contexto de aplicación es apropiado. Para remediar este problema, se han propuesto enfoques para hacer una selección de términos de expansión después de la aplicación de relaciones [24][31]. Normalmente, se define algún tipo de relación global entre el término de expansión y toda la consulta, que suele ser la suma de sus relaciones con cada palabra de la consulta. Aunque algunos términos de expansión inapropiados pueden eliminarse porque están débilmente conectados a algunos términos de consulta, muchos otros permanecen. Por ejemplo, si la relación programa→computadora es lo suficientemente fuerte, la computadora tendrá una relación global fuerte con todo el programa de televisión de la consulta y seguirá siendo un término de expansión. Es posible integrar un control más fuerte sobre la utilización del conocimiento. Por ejemplo, [17] definió relaciones lógicas fuertes para codificar el conocimiento de diferentes dominios. Si la aplicación de una relación conduce a un conflicto con la consulta (o con otras piezas de evidencia), entonces no se aplica. Sin embargo, este enfoque requiere codificar todas las consecuencias lógicas, incluidas las contradicciones en el conocimiento, lo cual es difícil de implementar en la práctica. En nuestro estudio anterior [1], se propone un enfoque más simple y general para resolver el problema en su origen, es decir, la falta de información de contexto en las relaciones de términos: al introducir condiciones más estrictas en una relación, por ejemplo {Java, programa}→computadora y {algoritmo, programa}→computadora, la aplicabilidad de las relaciones se restringirá naturalmente a contextos correctos. Como resultado, la computadora se utilizará para ampliar consultas de programas Java o algoritmos de programas, pero no de programas de televisión. Este principio es similar al de [33] para la desambiguación del sentido de las palabras. Sin embargo, no asignamos explícitamente un significado a una palabra; más bien intentamos hacer diferencias entre los usos de las palabras en diferentes contextos. Desde este punto de vista, nuestro enfoque es más similar a la discriminación de sentido de las palabras [27]. En este artículo, utilizamos el mismo enfoque y lo integraremos en un modelo más global con otros factores contextuales. Dado que las palabras de contexto añadidas a las relaciones nos permiten explotar el contexto de palabras dentro de la consulta, llamamos a tales factores contexto dentro de la consulta. Dentro del contexto de la consulta existe en muchas consultas. De hecho, los usuarios a menudo no utilizan una sola palabra ambigua como Java como consulta (si son conscientes de su ambigüedad). Algunas palabras de contexto suelen usarse junto con ella. En estos casos, se crean contextos dentro de la consulta y pueden ser explotados. Perfil de consulta y otros factores Se han realizado muchos intentos en IR para crear perfiles específicos de consulta. Podemos considerar retroalimentación implícita o retroalimentación ciega [7][16][29][32][35] en esta familia. Se crea un modelo de retroalimentación a corto plazo para la consulta dada a partir de documentos de retroalimentación, el cual ha demostrado ser efectivo para capturar algunos aspectos de la intención de los usuarios detrás de la consulta. Para crear un buen modelo de consulta, se debe integrar un modelo de retroalimentación específico de la consulta. Hay muchos otros factores contextuales ([26]) con los que no tratamos en este artículo. Sin embargo, parece claro que muchos factores son complementarios. Como se encontró en [32], un modelo de retroalimentación crea un contexto local relacionado con la consulta, mientras que el conocimiento general o todo el corpus define un contexto global. Ambos tipos de contextos han demostrado ser útiles [32]. El modelo de dominio especifica otro tipo de información útil: refleja un conjunto de términos de fondo específicos para un dominio, por ejemplo, contaminación, lluvia, efecto invernadero, etc. para el dominio del Medio Ambiente. Estos términos suelen ser presupuestos cuando un usuario emite una consulta como limpieza de residuos en el dominio. Es útil agregarlos a la consulta. Observamos una clara complementariedad entre estos factores. Es útil entonces combinarlos juntos en un único modelo de IR. En este estudio, integraremos todos los factores mencionados anteriormente dentro de un marco unificado basado en modelado del lenguaje. Cada factor contextual de cada componente determinará un puntaje de clasificación diferente, y la clasificación final del documento combina todos ellos. Esto se describe en la siguiente sección. 3. MODELO IR GENERAL En el marco del modelado de lenguaje, una función de puntuación típica se define en la divergencia de Kullback-Leibler de la siguiente manera: ( ) ( ) ( ) ( )DQ Vt DQ KLtPtPDQScore θθθθ |||log|, −∝= ∑∈ (1) donde θD es un <br>modelo de lenguaje</br> (unigrama) creado para un documento D, θQ un <br>modelo de lenguaje</br> para la consulta Q, y V el vocabulario. El suavizado en el modelo de documentos se reconoce como crucial [35], y uno de los métodos comunes de suavizado es el suavizado de interpolación Jelinek-Mercer: ( ) ( ) ( ) ( )CDD tPtPtP θλθλθ ||1| +−= (2) donde λ es un parámetro de interpolación y θC el modelo de colección. En los enfoques básicos de modelado de lenguaje, el modelo de consulta se estima mediante la Estimación de Máxima Verosimilitud (MLE) sin ningún suavizado. En un entorno así, la operación básica de recuperación sigue estando limitada a la coincidencia de palabras clave, según unas pocas palabras en la consulta. Para mejorar la eficacia de la recuperación, es importante crear un modelo de consulta más completo que represente mejor la necesidad de información. En particular, todas las palabras relacionadas y supuestas deben incluirse en el modelo de consulta. Se han propuesto modelos de consulta más completos mediante varios métodos que utilizan documentos de retroalimentación [16][35] o relaciones de términos [1][10][34]. En estos casos, construimos dos modelos para la consulta: el modelo de consulta inicial que contiene solo los términos originales, y un nuevo modelo que contiene los términos añadidos. Luego se combinan a través de la interpolación. En este artículo, generalizamos este enfoque e integramos más modelos para la consulta. Utilicemos 0 Qθ para denotar el modelo de consulta original, F Qθ para el modelo de retroalimentación creado a partir de documentos de retroalimentación, Dom Qθ para un modelo de dominio y K Qθ para un modelo de conocimiento creado aplicando relaciones de términos. 0 Qθ puede ser creado mediante MLE. F Qθ ha sido utilizado en varios estudios previos [16][35]. En este documento, F Qθ se extrae utilizando los 20 documentos de retroalimentación ciega. Describiremos los detalles para construir Dom Qθ y K Qθ en las Secciones 4 y 5. Dado estos modelos, creamos el siguiente modelo de consulta final por interpolación: ∑∈ = Xi i QiQ tPtP )|()|( θαθ (3) donde X={0, Dom, K, F} es el conjunto de todos los modelos de componentes e iα (con 1=∑∈Xi iα ) son sus pesos de mezcla. Entonces, el puntaje del documento en la Ecuación (1) se extiende de la siguiente manera: ( ) ∑∑∑ ∈∈ ∈ == Xi ii Vt Xi D i Qi DQScoretPtPDQScore ),()|(log)|(, αθθα (4) donde )|(log)|(),( D Vt i Qi tPtPDQScore θθ∑∈ = es el puntaje de acuerdo a cada modelo de componente. Aquí podemos ver que nuestra estrategia de mejorar el modelo de consulta con factores contextuales es equivalente a la reorganización de documentos, que se utiliza en [5][15][30]. El problema restante es construir modelos de dominio y modelos de conocimiento y combinar todos los modelos (configuración de parámetros). Describimos esto en las siguientes secciones. 4. CONSTRUCCIÓN Y USO DE MODELOS DE DOMINIO Como en estudios anteriores, explotamos un conjunto de documentos ya clasificados en cada dominio. Estos documentos se pueden identificar de dos maneras diferentes: 1) Se puede aprovechar una jerarquía de dominio existente y los documentos clasificados manualmente en ellos, como ODP. En ese caso, una nueva consulta debería ser clasificada en los mismos dominios ya sea de forma manual o automática. 2) Un usuario puede definir sus propios dominios. Al asignar un dominio a sus consultas, el sistema puede recopilar un conjunto de respuestas a las consultas automáticamente, las cuales luego se consideran documentos dentro del dominio. Las respuestas podrían ser aquellas que el usuario haya leído, ojeado o considerado relevantes para una consulta en el dominio, o simplemente podrían ser los resultados de recuperación mejor clasificados. Un estudio anterior [4] ha comparado las dos estrategias anteriores utilizando las consultas TREC 51-150, para las cuales se ha asignado manualmente un dominio. Estos dominios han sido asignados a categorías de ODP. Se ha encontrado que ambos enfoques mencionados anteriormente son igualmente efectivos y dan lugar a un rendimiento comparable. Por lo tanto, en este estudio, solo utilizamos el segundo enfoque. Esta elección también está motivada por la posibilidad de comparar entre la asignación manual y automática de dominios a una nueva consulta. Esto se explicará detalladamente en nuestros experimentos. Cualquiera que sea la estrategia, obtendremos un conjunto de documentos para cada dominio, a partir del cual se puede extraer un <br>modelo de lenguaje</br>. Si se utiliza la estimación de máxima verosimilitud directamente en estos documentos, el modelo de dominio resultante contendrá tanto términos específicos del dominio como términos generales, y los primeros no surgirán. Por lo tanto, empleamos un proceso de EM para extraer la parte específica del dominio de la siguiente manera: asumimos que los documentos en un dominio son generados por un modelo específico del dominio (a ser extraído) y un <br>modelo de lenguaje</br> general (modelo de colección). Entonces, la probabilidad de un documento en el dominio puede formularse de la siguiente manera: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= Dt Dtc CDomDom tPtPDP ; ||1| θηθηθ (5) donde c(t; D) es el conteo de t en el documento D y η es un parámetro de suavizado (que se fijará en 0.5 como en [35]). El algoritmo EM se utiliza para extraer el modelo de dominio Domθ que maximiza P(Dom| θDom) (donde Dom es el conjunto de documentos en el dominio), es decir: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ||1maxarg |maxarg θηθη θθ θ θ (6) Este es el mismo proceso que se utiliza para extraer el modelo de retroalimentación en [35]. Es capaz de extraer las palabras más específicas del dominio de los documentos mientras filtra las palabras comunes del idioma. Esto se puede observar en la siguiente tabla, que muestra algunas palabras en el modelo de dominio de Medio Ambiente antes y después de las iteraciones de EM (50 iteraciones). Tabla 1. Probabilidades de términos antes/después de EM Término Inicial Final cambio Término Inicial Final cambio aire 0.00358 0.00558 + 56% año 0.00357 0.00052 - 86% medio ambiente 0.00213 0.00340 + 60% sistema 0.00212 7.13*e-6 - 99% lluvia 0.00197 0.00336 + 71% programa 0.00189 0.00040 - 79% contaminación 0.00177 0.00301 + 70% millón 0.00131 5.80*e-6 - 99% tormenta 0.00176 0.00302 + 72% hacer 0.00108 5.79*e-5 - 95% inundación 0.00164 0.00281 + 71% compañía 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% presidente 0.00077 2.71*e-6 - 99% efecto invernadero 0.00034 0.00058 + 72% mes 0.00073 3.88*e-5 - 95% Dado un conjunto de modelos de dominio, los relacionados deben asignarse a una nueva consulta. Esto se puede hacer manualmente por el usuario o automáticamente por el sistema utilizando la clasificación de consultas. Vamos a comparar ambos enfoques. La clasificación de consultas ha sido investigada en varios estudios [18][28]. En este estudio, utilizamos un método de clasificación simple: el dominio seleccionado es aquel cuyo puntaje de divergencia KL de las consultas es el más bajo, es decir: )|(log)|(minarg 0 Dom Qt Q Dom Q tPtP Dom θθθ θ ∑∈ = (7) Este método de clasificación es una extensión de Naïve Bayes como se muestra en [22]. El puntaje dependiendo del modelo de dominio es entonces el siguiente: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Aunque la ecuación anterior requiere el uso de todos los términos en el vocabulario, en la práctica, solo los términos más fuertes en el modelo de dominio son útiles y los términos con bajas probabilidades suelen ser ruido. Por lo tanto, solo conservamos los 100 términos más fuertes. La misma estrategia se utiliza para el modelo de conocimiento. Aunque los modelos de dominio son más refinados que un solo perfil de usuario, los temas en un solo dominio aún pueden ser muy diferentes, lo que hace que el modelo de dominio sea demasiado grande. Esto es especialmente cierto para dominios amplios como Ciencia y tecnología definidos en las consultas de TREC. Usar un modelo de dominio tan grande como antecedente puede introducir muchos términos de ruido. Por lo tanto, construimos un modelo de subdominio más relacionado con la consulta dada, utilizando un subconjunto de documentos dentro del dominio que están relacionados con la consulta. Estos documentos son los documentos mejor clasificados recuperados con la consulta original dentro del dominio. Este enfoque es de hecho una combinación de modelos de dominio y retroalimentación. En nuestros experimentos, veremos que esta especificación adicional del subdominio es necesaria en algunos casos, pero no en todos, especialmente cuando también se utiliza el modelo de retroalimentación. 5. EXTRACCIÓN DE RELACIONES DE TÉRMINOS DEPENDIENTES DEL CONTEXTO DE DOCUMENTOS En este artículo, extraemos relaciones de términos de la colección de documentos de forma automática. En general, una relación de términos puede ser representada como A→B. Tanto A como B han sido restringidos a términos individuales en estudios anteriores. Un solo término en A significa que la relación es aplicable a todas las consultas que contienen ese término. Como explicamos anteriormente, esta es la fuente de muchas aplicaciones incorrectas. La solución que proponemos es agregar más términos de contexto en A, de modo que sea aplicable solo cuando todos los términos en A aparezcan en una consulta. Por ejemplo, en lugar de crear una relación independiente del contexto Java→programa, crearemos {Java, computadora}→programa, lo que significa que el programa se selecciona cuando tanto Java como computadora aparecen en una consulta. El término añadido en la condición especifica un contexto más estricto para aplicar la relación. Llamamos a este tipo de relación relación dependiente del contexto. En principio, la adición no está restringida a un término. Sin embargo, haremos esta restricción debido a las siguientes razones: • Las consultas de los usuarios suelen ser muy cortas. La adición de más términos a la condición creará muchas relaciones raramente aplicables; en la mayoría de los casos, una palabra ambigua como Java puede ser efectivamente desambiguada por una palabra de contexto útil como computadora u hotel; la adición de más términos también conducirá a una mayor complejidad de espacio y tiempo para extraer y almacenar relaciones de términos. La extracción de relaciones de tipo {tj, tk} → ti se puede realizar utilizando algoritmos de minería de reglas de asociación [13]. Aquí, utilizamos un análisis de co-ocurrencia simple. Las ventanas de tamaño fijo (10 palabras en nuestro caso) se utilizan para obtener recuentos de co-ocurrencia de tres términos, y la probabilidad )|( kji tttP se determina de la siguiente manera: ∑= lt kjlkjikji tttctttctttP ),,(),,()|( (9) donde ),,( kji tttc es el recuento de co-ocurrencias. Para reducir el requisito de espacio, aplicamos además los siguientes criterios de filtrado: • Los dos términos en la condición deben aparecer juntos al menos cierto número de veces en la colección (10 en nuestro caso) y deben estar relacionados. Utilizamos la siguiente información mutua puntual como medida de relación (MI > 0) [6]: )()( ),( log),( kj kj kj tPtP ttP ttMI = • La probabilidad de una relación debe ser mayor que un umbral (0.0001 en nuestro caso); Teniendo un conjunto de relaciones, el modelo de conocimiento correspondiente se define de la siguiente manera: )|()|()|( )|()|()|( 00 )( 0 )( QkQjkj Qtt i Qkjkj Qtt i K Q tPtPtttP ttPtttPtP kj kj θθ θθ ∑ ∑ ∈ ∈ = = (10) donde (tj tk)∈Q significa cualquier combinación de dos términos en la consulta. Esta es una extensión directa del modelo de traducción propuesto en [3] a nuestras relaciones dependientes del contexto. La puntuación según el modelo de Conocimiento se define entonces de la siguiente manera: ∑ ∑∈ ∈ = Vt DiQkQjkj Qtt iK i kj tPtPtPtttPDQScore )|(log)|()|()|(),( 00 )( θθθ (11) Nuevamente, solo se utilizan los 100 términos de expansión principales. 6. PARÁMETROS DEL MODELO Hay varios parámetros en nuestro modelo: λ en la Ecuación (2) y αi (i∈{0, Dom, K, F}) en la Ecuación (3). Dado que el parámetro λ solo afecta al modelo de documento, lo estableceremos con el mismo valor en todos nuestros experimentos. El valor λ=0.5 se determina para maximizar la efectividad de los modelos base (ver Sección 7.2) en los datos de entrenamiento: consultas TREC 1-50 y documentos en el Disco 2. Los pesos de la mezcla αi de los modelos de componentes se entrenan en los mismos datos de entrenamiento utilizando el siguiente método de búsqueda de línea [11] para maximizar la Precisión Promedio Media (MAP): cada parámetro se considera como una dirección de búsqueda. Comenzamos buscando en una dirección, probando todos los valores en esa dirección, mientras mantenemos los valores en otras direcciones sin cambios. Cada dirección se busca sucesivamente, hasta que no se observe ninguna mejora en la MAP. Para evitar quedar atrapados en un máximo local, comenzamos desde 10 puntos aleatorios y se selecciona la mejor configuración. 7. EXPERIMENTOS 7.1 Configuración Los datos principales de prueba son los de las pistas ad-hoc y de filtrado de TREC 1-3, que incluyen las consultas 1-150 y los documentos en los Discos 1-3. La elección de esta colección de pruebas se debe a la disponibilidad de un dominio especificado manualmente para cada consulta. Esto nos permite comparar con un enfoque que utiliza identificación automática de dominio. A continuación se muestra un ejemplo de tema: <num> Número: 103 <dom> Dominio: Ley y Gobierno <title> Tema: Reforma del Bienestar Solo utilizamos títulos de temas en todas nuestras pruebas. Las consultas 1-50 se utilizan para el entrenamiento y las consultas 51-150 para las pruebas. Se definen 13 dominios en estas consultas y su distribución entre los dos conjuntos de consultas se muestra en la Figura 1. Podemos ver que la distribución varía considerablemente entre dominios y entre los dos conjuntos de consultas. También hemos realizado pruebas con datos de TREC 7 y 8. Para esta serie de pruebas, cada colección se utiliza a su vez como datos de entrenamiento mientras que la otra se utiliza para pruebas. Algunas estadísticas de los datos se describen en la Tabla 2. Todos los documentos son preprocesados utilizando el stemmer de Porter en Lemur y se utiliza la lista de palabras vacías estándar. Algunas consultas (4, 5 y 3 en los tres conjuntos de consultas) solo contienen una palabra. Para estas consultas, el modelo de conocimiento no es aplicable. En los modelos de dominio, examinamos varias preguntas: • ¿Es útil incorporar el modelo de dominio cuando se especifica manualmente el dominio de consulta? • ¿Se puede determinar automáticamente el dominio de consulta si no está especificado? ¿Qué tan efectivo es este método? • Describimos dos formas de recopilar documentos para un dominio: ya sea utilizando documentos considerados relevantes para las consultas en el dominio o utilizando documentos recuperados para estas consultas. ¿Cómo se comparan? En el modelo de conocimiento, además de probar su efectividad, también queremos comparar las relaciones dependientes del contexto con las independientes del contexto. Finalmente, veremos el impacto de cada modelo de componente cuando se combinan todos los factores. 7.2 Métodos de referencia Se utilizan dos modelos de referencia: el modelo clásico de unigrama sin ninguna expansión, y el modelo con Retroalimentación. En todos los experimentos, se crean modelos de documentos utilizando suavizado de Jelinek-Mercer. Esta elección se realiza de acuerdo con la observación en [36] de que el método funciona muy bien para consultas largas. En nuestro caso, a medida que se expanden las consultas, tienen un rendimiento similar a las consultas largas. En nuestros tests preliminares, también encontramos que este método funcionó mejor que los otros métodos (por ejemplo, Dirichlet), especialmente para el método principal de línea base con modelo de retroalimentación. La Tabla 3 muestra la eficacia de recuperación en todas las colecciones. 7.3 Modelos de Conocimiento Este modelo se combina con ambos modelos base (con o sin retroalimentación). También comparamos el modelo de conocimiento dependiente del contexto con las relaciones de términos tradicionales independientes del contexto (definidas entre dos términos individuales), que se utilizan para ampliar las consultas. Este último selecciona términos de expansión con la relación global más fuerte con la consulta. Esta relación se mide por la suma de las relaciones con cada uno de los términos de la consulta. Este método es equivalente a [24]. También es similar al modelo de traducción [3]. Lo llamamos 0 5 10 15 20 25 30 35 Finanzas Ambientales Economía Internacional Finanzas Internacionales Política Internacional Relaciones Internacionales Derecho y Gobierno. Medicina y Biología. Política Militar. Ciencia y Tecnología. Economía de EE. UU. Política de EE. UU. Consulta 1-50 Consulta 51-150 Figura 1. Distribución de dominios Tabla 2. Estadísticas de la colección TREC Tamaño del documento (GB) Vocabulario # de documentos Disco de entrenamiento de consulta 2 0.86 350,085 231,219 Discos 1-50 Discos 1-3 Discos 1-3 3.10 785,932 1,078,166 51-150 Discos TREC7 4-5 1.85 630,383 528,155 351-400 Discos TREC8 4-5 1.85 630,383 528,155 401-450 Modelo de co-ocurrencia en la Tabla 4. La prueba t también se realiza para determinar la significancia estadística. Como podemos ver, las relaciones de simple co-ocurrencia pueden producir mejoras relativamente fuertes; pero las relaciones dependientes del contexto pueden producir mejoras mucho más fuertes en todos los casos, especialmente cuando no se utiliza retroalimentación. Todas las mejoras sobre el modelo de coocurrencia son estadísticamente significativas (esto no se muestra en la tabla). Las grandes diferencias entre los dos tipos de relación muestran claramente que las relaciones dependientes del contexto son más apropiadas para la expansión de consultas. Esto confirma la hipótesis que planteamos, que al incorporar información de contexto en las relaciones, podemos determinar mejor las relaciones apropiadas a aplicar y así evitar introducir términos de expansión inapropiados. El siguiente ejemplo puede confirmar aún más esta observación, donde mostramos los términos de expansión más fuertes sugeridos por ambos tipos de relación para la consulta #384 estación espacial luna: Relaciones de co-ocurrencia: año 0.016552 potencia 0.013226 tiempo 0.010925 1 0.009422 desarrollar 0.008932 oficina 0.008485 operar 0.008408 2 0.007875 tierra 0.007843 trabajo 0.007801 radio 0.007701 sistema 0.007627 construir 0.007451 000 0.007403 incluir 0.007377 estado 0.007076 programa 0.007062 nación 0.006937 abrir 0.006889 servicio 0.006809 aire 0.006734 espacio 0.006685 nuclear 0.006521 completo 0.006425 hacer 0.006410 compañía 0.006262 personas 0.006244 proyecto 0.006147 unidad 0.006114 general 0.006036 diario 0.006029 Relaciones dependientes del contexto: espacio 0.053913 mar 0.046589 tierra 0.041786 hombre 0.037770 programa 0.033077 proyecto 0.026901 base 0.025213 órbita 0.025190 construir 0.025042 misión 0.023974 llamada 0.022573 explorar 0.021601 lanzamiento 0.019574 desarrollar 0.019153 transbordador 0.016966 plan 0.016641 vuelo 0.016169 estación 0.016045 internacional 0.016002 energía 0.015556 operar 0.014536 potencia 0.014224 transporte 0.012944 construir 0.012160 nasa 0.011985 nación 0.011855 permanente 0.011521 japón 0.011433 apolo 0.010997 lunar 0.010898 En comparación con el modelo base con retroalimentación (Tab. 3), vemos que las mejoras realizadas por el modelo de conocimiento solo son ligeramente menores. Sin embargo, cuando ambos modelos se combinan, hay mejoras adicionales sobre el modelo de Retroalimentación, y estas mejoras son estadísticamente significativas en 2 de cada 3 casos. Esto demuestra que los impactos producidos por la retroalimentación y las relaciones de términos son diferentes y complementarios. Modelos de dominio En esta sección, probamos varias estrategias para crear y utilizar modelos de dominio, aprovechando la información del dominio del conjunto de consultas de diversas maneras. Estrategias para crear modelos de dominio: C1 - Con los documentos relevantes para las consultas dentro del dominio: esta estrategia simula el caso en el que tenemos un directorio existente en el que se incluyen documentos relevantes para el dominio. C2 - Con los 100 documentos principales recuperados con las consultas dentro del dominio: esta estrategia simula el caso en el que el usuario especifica un dominio para sus consultas sin juzgar la relevancia del documento, y el sistema recopila documentos relacionados de su historial de búsqueda. Estrategias para usar modelos de dominio: U1 - El modelo de dominio es determinado por el usuario manualmente. U2 - El modelo de dominio es determinado por el sistema. 7.4.1 Creación de modelos de dominio. Probamos las estrategias C1 y C2. En esta serie de pruebas, cada una de las consultas del 51 al 150 se utiliza sucesivamente como la consulta de prueba, mientras que las otras consultas y sus documentos relevantes (C1) o los documentos recuperados de mayor rango (C2) se utilizan para crear modelos de dominio. El mismo método se utiliza en las consultas del 1 al 50 para ajustar los parámetros. Tabla 3. Modelos de referencia Modelo Unigrama Coll. Medida Sin FB Con FB AvgP 0.1570 0.2344 (+49.30%) Recuperación /48 355 15 711 19 513 Discos 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recuperación /4 674 2 237 2 777 TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recuperación /4 728 2 764 3 237 TREC8 P@10 0.4340 0.4860 Tabla 4. Modelo de conocimiento Co-ocurrencia Modelo de conocimiento Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Discos1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (La columna SinFB se compara con el modelo base sin retroalimentación, mientras que ConFB se compara con el modelo base con retroalimentación. ++ y + significan cambios significativos en la prueba t con respecto al modelo base sin retroalimentación, a un nivel de p<0.01 y p<0.05, respectivamente. ** y * son similares pero se comparan con el modelo base con retroalimentación.) Tabla 5. Modelos de dominio con documentos relevantes (C1) Dominio Subdominio Colección. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Discos 1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Tabla 6. Modelos de dominio con los 100 documentos principales (C2) Dominio Subdominio Col. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Discos1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 También comparamos los modelos de dominio creados con todos los documentos del dominio (Dominio) y con solo los 10 documentos recuperados principales en el dominio con la consulta (Sub-Dominio). En estos tests, utilizamos la identificación manual del dominio de consulta para los Discos 1-3 (U1), pero la identificación automática para TREC7 y 8 (U2). Primero, es interesante notar que la incorporación de modelos de dominio puede mejorar la efectividad de recuperación en todos los casos en general. Las mejoras en los Discos 1-3 y TREC7 son estadísticamente significativas. Sin embargo, las escalas de mejora son más pequeñas que al usar los modelos de Retroalimentación y Relación. Al observar la distribución de los dominios (Fig. 1), esta observación no es sorprendente: para muchos dominios, solo tenemos unas pocas consultas de entrenamiento, por lo tanto, pocos documentos dentro del dominio para crear modelos de dominio. Además, los temas en el mismo dominio pueden variar considerablemente, en particular en dominios amplios como la ciencia y la tecnología, la política internacional, etc. Segundo, observamos que los dos métodos para crear modelos de dominio funcionan igual de bien (Tab. 6 vs. Tab. 5). En otras palabras, proporcionar juicios de relevancia para las consultas no aporta mucha ventaja para el propósito de crear modelos de dominio. Esto puede parecer sorprendente. Un análisis muestra de inmediato la razón: un modelo de dominio (de la forma en que lo creamos) solo captura la distribución de términos en el dominio. Los documentos relevantes para todas las consultas dentro del dominio varían considerablemente. Por lo tanto, en algunos dominios grandes, los términos característicos tienen efectos variables en las consultas. Por otro lado, dado que solo utilizamos la distribución de términos, aunque los documentos principales recuperados para las consultas dentro del dominio sean irrelevantes, aún pueden contener términos característicos del dominio de manera similar a los documentos relevantes. Por lo tanto, ambas estrategias producen efectos muy similares. Este resultado abre la puerta a un método más simple que no requiere juicios de relevancia, por ejemplo, utilizando el historial de búsqueda. Tercero, sin el modelo de retroalimentación, los modelos de subdominio construidos con documentos relevantes funcionan mucho mejor que los modelos de dominio completo (Tabla 5). Sin embargo, una vez que se utiliza el modelo de retroalimentación, la ventaja desaparece. Por un lado, esto confirma nuestra hipótesis anterior de que un dominio puede ser demasiado grande para poder sugerir términos relevantes para nuevas consultas en el dominio. Indirectamente valida nuestra primera hipótesis de que un modelo o perfil de usuario único puede ser demasiado grande, por lo que se prefieren modelos de dominio más pequeños. Por otro lado, los modelos de subdominio capturan características similares al modelo de retroalimentación. Por lo tanto, cuando se utiliza este último, los modelos de subdominio se vuelven superfluos. Sin embargo, si los modelos de dominio se construyen con documentos de alta clasificación (Tabla 6), los modelos de subdominio hacen muchas menos diferencias. Esto se puede explicar por el hecho de que los dominios construidos con documentos de alto rango tienden a ser más uniformes que los documentos relevantes con respecto a la distribución de términos, ya que los documentos recuperados en la parte superior suelen tener una correspondencia estadística más fuerte con las consultas que los documentos relevantes. 7.4.2 Determinación Automática del Dominio de la Consulta No es realista pedir siempre a los usuarios que especifiquen un dominio para sus consultas. Aquí examinamos la posibilidad de identificar automáticamente los dominios de consulta. La Tabla 7 muestra los resultados con esta estrategia utilizando ambas estrategias para la construcción del modelo de dominio. Podemos observar que la efectividad es solo ligeramente menor que la de aquellas producidas con la identificación manual del dominio de consulta (Tab. 5 y 6, Modelos de dominio). Esto demuestra que la identificación automática de dominios es una forma de seleccionar un modelo de dominio tan efectiva como la identificación manual. Esto también demuestra la viabilidad de utilizar modelos de dominio para consultas cuando no se proporciona información de dominio. Al observar la precisión de la identificación automática de dominios, sin embargo, es sorprendentemente baja: para las consultas 51-150, solo el 38% de los dominios determinados corresponden a las identificaciones manuales. Esto es mucho menor que las tasas superiores al 80% reportadas en [18]. Un análisis detallado revela que la razón principal es la cercanía de varios dominios en las consultas de TREC (por ejemplo, Relaciones internacionales, política internacional, política. Sin embargo, en esta situación, los dominios incorrectos asignados a las consultas no siempre son irrelevantes e inútiles. Por ejemplo, incluso cuando una consulta en Relaciones Internacionales se clasifica en Política Internacional, este último dominio aún puede sugerir términos útiles para la consulta. Por lo tanto, la relativamente baja precisión de clasificación no significa una baja utilidad de los modelos de dominio. 7.5 Modelos completos Los resultados con el modelo completo se muestran en la Tabla 8. Este modelo integra todos los componentes descritos en este documento: modelo de consulta original, modelo de retroalimentación, modelo de dominio y modelo de conocimiento. Hemos probado ambas estrategias para crear modelos de dominio, pero las diferencias entre ellas son muy pequeñas. Por lo tanto, solo informamos los resultados con los documentos relevantes. Nuestra primera observación es que los modelos completos producen los mejores resultados. Todas las mejoras sobre el modelo base (con retroalimentación) son estadísticamente significativas. Este resultado confirma que la integración de factores contextuales es efectiva. En comparación con los otros resultados, observamos mejoras consistentes, aunque pequeñas en algunos casos, en todos los modelos parciales. Al observar los pesos de la mezcla, que pueden reflejar la importancia de cada modelo, observamos que los mejores ajustes en todas las colecciones varían en los siguientes rangos: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 y 0.5≤αF ≤0.6. Vemos que el factor más importante es el modelo de retroalimentación. Este también es el único factor que produjo las mejoras más significativas sobre el modelo de consulta original. Esta observación parece indicar que este modelo tiene la mayor capacidad para capturar la información necesaria detrás de la consulta. Sin embargo, incluso con pesos más bajos, los otros modelos sí tienen un fuerte impacto en la efectividad final. Esto demuestra el beneficio de integrar más factores contextuales en RI. Tabla 7. Identificación automática del dominio de consulta (U2) Dom. con doc. rel. (C1) Dom. con doc. en el top-100 (C2) Coll. Medida Sin FB Con FB Sin FB Con FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recuperación 16 343 20 061 16 414 20 090 Discos 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Tabla 8. Modelos completos (C1) Todos los documentos. Colección de dominio. Medida Man. dom. id. (U1) Auto. dom. id. (U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Discos 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 8. CONCLUSIONES Los enfoques tradicionales de IR suelen considerar la consulta como el único elemento disponible para satisfacer la necesidad de información del usuario. Muchos estudios previos han investigado la integración de algunos factores contextuales en los modelos de RI, típicamente mediante la incorporación de un perfil de usuario. En este artículo, argumentamos que un único perfil de usuario (o modelo) puede contener una gran variedad de temas diferentes, de modo que las nuevas consultas pueden estar sesgadas incorrectamente. De manera similar a algunos estudios previos, proponemos modelar dominios de temas en lugar del usuario. Investigaciones previas sobre el contexto se centraron en factores alrededor de la consulta. Mostramos en este artículo que los factores dentro de la consulta también son importantes, ya que ayudan a seleccionar las relaciones de términos apropiadas para aplicar en la expansión de la consulta. Hemos integrado los factores contextuales mencionados anteriormente, junto con el modelo de retroalimentación, en un solo <br>modelo de lenguaje</br>. Nuestros resultados experimentales confirman firmemente el beneficio de utilizar contextos en IR. Este trabajo también muestra que el marco de modelado del lenguaje es apropiado para integrar muchos factores contextuales. Este trabajo puede ser mejorado en varios aspectos, incluyendo otros métodos para extraer relaciones entre términos, integrar más palabras de contexto en las condiciones e identificar dominios de consulta. También sería interesante probar el método en la búsqueda web utilizando el historial de búsqueda del usuario. Investigaremos estos problemas en nuestra futura investigación. REFERENCIAS [1] Bai, J., Nie, J.Y., Cao, G., Relaciones de términos dependientes del contexto para la recuperación de información, EMNLP06, pp. 551-559, 2006. [2] Belkin, N.J., Interacción con textos: la recuperación de información como comportamiento de búsqueda de información, Information Retrieval93: Von der modellierung zu anwendung, pp. 55-66, Konstanz: Krause & Womser-Hacker, 1993. [3] Berger, A., Lafferty, J., Recuperación de información como traducción estadística, SIGIR99, pp. 222-229, 1999. [4] Bouchard, H., Nie, J.Y., Modelos de lenguaje aplicados a la búsqueda de información contextual, Conf. en Recherche dInformation et Applications (CORIA), Lyon, 2006. [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlschütter, C., Uso de metadatos de ODP para personalizar la búsqueda, SIGIR, pp. 178-185, 2005. [6] Church, K. W., Hanks, P., Normas de asociación de palabras, información mutua y lexicografía. ACL, pp. 22-29, 1989. [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Retroalimentación de relevancia y personalización: una perspectiva de modelado de lenguaje, En: El taller DELOS-NSF sobre Personalización y Sistemas de Recomendación en Bibliotecas Digitales, pp. 49-54, 2006. [8] Croft, W. B., Wei, X., Modelos de temas basados en contexto para la modificación de consultas, Informe Técnico CIIR, Universidad de Massachusetts, 2005. [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R., Robbins, D. C., Cosas que he visto: un sistema para la recuperación y reutilización de información personal, SIGIR03, pp. 72-79, 2003. [10] Fang, H., Zhai, C., Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información, SIGIR06, pp.115-122, 2006. [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Modelo discriminativo lineal para la recuperación de información. SIGIR05, pp. 290-297, 2005. [12] Búsqueda personalizada de Google, http://www.google.com/psearch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algoritmos para la minería de reglas de asociación - una encuesta general y comparación. SIGKDD explorations, 2 (1), pp. 58-64, 2000. [14] Ingwersen, P., Jäverlin, K., Recuperación de información en contexto: IRiX, SIGIR Forum, 39: pp. 31-39, 2004. [15] Kim, H.-R., Chan, P.K., Ranking personalizado de resultados de búsqueda con jerarquías de interés de usuario aprendidas a partir de marcadores, Taller WEBKDD05 en ACM-KDD, pp. 32-43, 2005. [16] Lavrenko, V., Croft, W. B., Modelos de lenguaje basados en relevancia, SIGIR01, pp. 120-127, 2001. [17] Lau, R., Bruza, P., Song, D., Revisión de creencias para recuperación de información adaptativa, SIGIR04, pp. 130-137, 2004. [18] Liu, F., Yu,C., Meng, W., Búsqueda web personalizada mediante el mapeo de consultas de usuario a categorías, CIKM02, pp. 558-565. [19] Liu, X., Croft, W. B., Recuperación basada en clústeres utilizando modelos de lenguaje, SIGIR 04, pp. 186-193, 2004. [20] Morris, R.C., Hacia un servicio de información centrado en el usuario, JASIS, 45: pp. 20-30, 1994. [21] Park, T.K., Hacia una teoría de relevancia basada en el usuario: Un llamado a un nuevo paradigma de investigación, JASIS, 45: pp. 135-141, 1994. [22] Peng, F., Schuurmans, D., Wang, S. Mejora de clasificadores Naive Bayes con modelos de lenguaje estadístico. I'm sorry, but the sentence \"Inf.\" is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Retr. 7(3-4): pp. 317-345, 2004. [23] Pitkow, J., Schütze, H., Cass, T., Cooley, R., Turnbull, D., Edmonds, A., Adar, E., Breuel, T., Búsqueda personalizada, Comunicaciones de ACM, 45: pp. 50-55, 2002. [24] Qiu, Y., Frei, H.P. Expansión de consulta basada en conceptos. SIGIR93, pp.160-169, 1993. [25] Sanderson, M., Recuperación con buen sentido, Inf. Ret., 2(1): pp. 49-69, 2000. [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., Una reexaminación de la relevancia: Hacia una definición dinámica y situacional, Information Processing and Management, 26(6): pp. 755-774, 1990. [27] Schütze, H., Pedersen J.O., Un tesauro basado en coocurrencias y dos aplicaciones para la recuperación de información, Information Processing and Management, 33(3): pp. 307-318, 1997. [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yin, J., Yang, Q. Enriquecimiento de consultas para la clasificación de consultas web. ACMTOIS, 24(3): pp. 320-352, 2006. [29] Shen, X., Tan, B., Zhai, C., Recuperación de información sensible al contexto utilizando retroalimentación implícita, SIGIR05, pp. 43-50, 2005. [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalización de la búsqueda a través del análisis automatizado de intereses y actividades, SIGIR05, pp. 449-456, 2005. [31] Voorhees, E., Expansión de consultas utilizando relaciones léxico-semánticas. SIGIR94, pp. 61-69, 1994. [32] Xu, J., Croft, W.B., Expansión de consultas utilizando análisis local y global de documentos, SIGIR96, pp. 4-11, 1996. [33] Yarowsky, D. Desambiguación de sentido de palabras no supervisada que rivaliza con métodos supervisados. ACL, pp. 189-196. 1995. [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Suavizado semántico sensible al contexto para el enfoque de modelado de lenguaje en la recuperación de información genómica, SIGIR06, pp. 170-177, 2006. [35] Zhai, C., Lafferty, J., Retroalimentación basada en modelos en el enfoque de modelado de lenguaje para la recuperación de información, CIKM01, pp. 403-410, 2001. [36] Zhai, C., Lafferty, J., Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. SIGIR, pp.334-342, 2001. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}